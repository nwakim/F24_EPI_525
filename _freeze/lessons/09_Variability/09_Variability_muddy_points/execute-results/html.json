{
  "hash": "e14d79aae90c73d498551c6313816945",
  "result": {
    "markdown": "---\ntitle: \"Muddy Points\"\nsubtitle: \"Lesson 09: Variability of estimates\"\ndate-modified: \"today\"\nformat: \n  html:\n    link-external-newwindow: true\n    page-layout: full\n    toc: false\n    code-fold: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n## 1. Central Limit Theorem\n\nI'm sorry for those online (and those watching the recording) who can't hear me for part of the Central Limit Theorem!\n\n[There's a pretty good StatQuest video on the Central Limit Theorem!!](https://www.youtube.com/watch?v=YAlJCEDH2uY&ab_channel=StatQuestwithJoshStarmer)\n\n### Why is the CLT helpful??\n\n[Start the above StatQuest video here for an answer! ](https://youtu.be/YAlJCEDH2uY?si=JtEF4_hh6e1Bs8q9&t=316)\n\n## 2. Why is CLT for n > 30? And why does the standard error decrease as n increases?\n\nOkay! We can start by simulating samples with different n's. In this case, we need to use a different problem set up than heights. I am going to do **coin flips with a 0.3 probability of heads**. I am going to look at the proportion of heads I flip. \n\nFor a given $n$, say $n=10$, I have taken 1,000 samples with 10 coin flips in each sample. I take the mean proportion of heads from the 10 flips for each sample. So then each sample has a sample mean to represent its respective 10 flips. Thus, I have 1,000 sample means for $n=10$. \n\nI do this process for different values of n. So for $n=40$, I have done the same thing: I have taken 1,000 samples with 40 coin flips in each sample. I take the mean proportion of heads from the 40 flips for each sample. So then each sample has a sample mean to represent its respective 40 flips. Thus, I have 1,000 sample means for $n=40$. \n\n\nBelow I have plotted the 1,000 sample means for different values of n: 10, 20, 30, 40, 50, and 100. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"If you would like to see how I simulated this and plotted it\"}\nlibrary(tidyverse)\nlibrary(ggplot2)\nset.seed(48151623)\n\nmeans=NULL;sds=NULL; n=NULL\nfor (j in c(10, 20, 30, 40, 50, 100)){\n  for(i in 1:1000){\n  heads = rbinom(n = j, size = 1, prob = 0.4)\n  n = c(n, j)\n  means = c(means, mean(heads))\n  sds = c(sds, sd(heads))\n}\n}\n\nmeans_all = as.data.frame(cbind(n, 1:1000, means, sds)) %>%\n  mutate(mu = 0.5, SE = sqrt(0.5*0.5)/sqrt(n))\nn_labels = c(`10` = \"n = 10\", \n             `20` = \"n = 20\", \n             `30` = \"n = 30\", \n             `40` = \"n = 40\", \n             `50` = \"n = 50\", \n             `100` = \"n = 100\")\n\nsamp_dist_plots = means_all %>% \n  ggplot(aes(x=means)) + \n  geom_histogram(color=\"black\", fill=\"grey\", bins=60) +\n  labs(x=\"Mean height from samples (inches)\", y=\"Frequency\", \n       title=\"Sampling distributions from samples with different values of n\") +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.05), \n                     limits = c(0, 1)) +\n  facet_wrap( ~ n, ncol=2, labeller = as_labeller(n_labels)) +\n  theme_classic() +\n  theme(text = element_text(size = 12))\n\nsamp_dist_plots\n```\n\n::: {.cell-output-display}\n![](09_Variability_muddy_points_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=864}\n:::\n:::\n\n\nWe are left with the questions:\n\n1. Are all of these approximately normal with a mean of $\\mu$ and a standard error or $\\dfrac{\\sigma}{\\sqrt{n}}$? (Asking if the CLT applies to all of them)\n2. Why is the standard error smaller when n increases?\n\n### 1. Are all of these approximately normal with a mean of $\\mu$ and a standard error or $\\dfrac{\\sigma}{\\sqrt{n}}$? (Asking if the CLT applies to all of them)\n\nYou can see that for $n=10$ and $n=20$, the distributions do not look normal. They have a right skewed tail. So the CLT would not work here!\n\n### 2. Why is the standard error smaller when n increases?\n\nThis is because the sample means become more precise when n increases. \n\nThe proportion of heads in a sample of 100 flips, can take on values like 34/100 or 32/100. While the proportion for 10 flips can only take 2/10 or 3/10 or 4/10. The more flips, the more precise a proportion we can get. \n\n## 3. Difference between standard deviation and standard error\n\nStandard deviation is defined generically (and mathematically) for any data. We often refer to the population standard deviation simply as the standard deviation ($\\sigma$). \n\nThe standard deviation of a single sample is called $s$!! \n\nAnd the standard deviation of the sampling distribution (of sample means) is called the **standard error**!\n\n## 4. Why wouldn't the 10 blue sample's mean not similar to the red sample?\n\nIt would be similar if it pulled from the same population! But because it is a random sample, we will just get 10 people with their own heights. And the 10 people in the blue group might just have different heights than the 10 people in the red group! Thus, they would have slightly similar, but likely not exactly the same, sample means.\n\n## 5. Central limit theorem vs sampling distribution\n\nThe Central Limit Theorem is a theory can we can apply to our sampling distribution! The sampling distribution is how sample means would be distributed if we took many, many samples. The Central Limit Theorem allows us to say that the sampling distribution is approximately normal. By saying it's approximately normal, we can then use the properties of a normal distribution to do further analysis. \n\n\n\n\n",
    "supporting": [
      "09_Variability_muddy_points_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}