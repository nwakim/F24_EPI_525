---
title: "Lesson 11: Hypothesis testing for mean from one-sample"
subtitle: "TB sections 4.3, 5.1"
author: "Meike Niederhausen and Nicky Wakim"
title-slide-attributes:
    data-background-color: "#3070BF"
date: "11/6/2024"
format: 
  revealjs:
    theme: "../simple_NW.scss"
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: Lesson 11 Slides
    html-math-method: mathjax
    highlight-style: arrow
execute:
  echo: true
  freeze: auto
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(oibiostat)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) # NEW!!

# terminal: for icons
# quarto install extension quarto-ext/fontawesome

# set ggplot theme for slides 
theme_set(theme_bw(base_size = 22))
# theme_update(text = element_text(size=16))  # set global text size for ggplots

set.seed(456)
```

## Where are we?

![](../img_slides/course_map.png){fig-align="center"}

## Where are we? Continuous outcome zoomed in {visibility="hidden"}

<br>
<br>

![](../img_slides/flowchart_only_continuous.jpg){fig-align="center"}


## Goals for today: Part 1 

### (4.3, 5.1) Hypothesis testing for mean from one sample  

* Introduce hypothesis testing using the case of analyzing a mean from one sample (group)

::: columns
::: {.column width="50%"}
* [Steps of a hypothesis test:]{style="color:purple"}
    1. level of significance
    1. null ( $H_0$ ) and alternative ( $H_A$ ) hypotheses
    1. test statistic
    1. p-value
    1. conclusion
:::
::: {.column width="50%"}
* [Run a hypothesis test in R]{style="color:green"}
    * Load a dataset - need to specify location of dataset
    * R projects
    * Run a t-test in R
    * `tidy()` the test output using `broom` package
:::
:::

### (4.3.3) Confidence intervals (CIs) vs. hypothesis tests 


## Answering a research question

Research question is a generic form: Is there evidence to support that the population mean is different than $\mu$?

Two approaches to answer this question:

::: columns
::: {.column width="49%"}
::: green2
::: green2-ttl
Confidence interval
:::
::: green2-cont
- Create a __confidence interval (CI)__ for the population mean $\mu$ and determine whether 98.6°F is inside the CI or not.
- Answering the question: is 98.6°F a plausible value?
:::
:::
:::

::: {.column width="2%"}
:::
::: {.column width="49%"}
::: pink
::: pink-ttl
Hypothesis test
:::
::: pink-cont
- Run a __hypothesis test__ to see if there is evidence that the population mean $\mu$ is _significantly different_ from 98.6°F or not
- This does not give us a range of plausible values for the population mean $\mu$.
- Instead, we calculate a _test statistic_ and _p-value_ 
- See how likely we are to observe the sample mean $\overline{x}$ or a more extreme sample mean assuming that the population mean $\mu$ is 98.6°F
:::
:::
:::
:::

## Last time: point estimates

```{r}
#| echo: false
#| fig-width: 18
#| fig-height: 5
#| fig-align: center

set.seed(4258)
means=NULL;sds=NULL
for(i in 1:1000){
  height = rnorm(50, 65, 3)
  means = c(means, mean(height))
  sds = c(sds, sd(height))
}
means50 = as.data.frame(cbind(1:1000, means, sds))
mu <- 65
SE <- 3/sqrt(50)
sig <- round(SE, 2)

samp_dist50_plot2 = means50 %>% 
  ggplot(aes(x=means)) + 
  geom_histogram(color="black", fill="grey", bins=60, 
                 aes(y=..density..)) +
  labs(x="Mean height from samples (inches)", y="Density") +
  scale_x_continuous(breaks = seq(60, 70, by = 1), 
                     limits = c(63, 67)) +
  theme(text = element_text(size = 27)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                size = 3, 
                color = "#5BAFF8") +
  geom_vline(xintercept = 65.1, size = 3, color = "#C00000") +
  geom_vline(xintercept = 64.7, size = 3, color = "#0070C0") +
  geom_vline(xintercept = 64.9, size = 3, color = "#FFC000") +
  geom_vline(xintercept = 66.1, size = 3, color = "#70AD47")
  

samp_dist50_plot2

```

![](../img_slides/sample_point_est.png){fig-align="center"}

## Last time: Point estimates with their confidence intervals for $\mu$

:::::: columns
::: {.column width="22%"}
 

![](../img_slides/samp_point_est_vert.png){fig-align="center"}
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 9
#| fig-height: 10
#| fig-align: center

samp_dist_blue = means50 %>% 
  ggplot(aes(x=means)) + 
  geom_histogram(color="black", fill="grey", bins=60, 
                 aes(y=..density..)) +
  labs(x="", y="Density") +
  scale_x_continuous(breaks = seq(60, 70, by = 1), 
                     limits = c(63, 67)) +
  theme(text = element_text(size = 27), 
        axis.title = element_blank()) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                size = 3, 
                color = "#5BAFF8") +
  geom_vline(xintercept = 65.1, size = 3, color = "#C00000") +
  geom_vline(xintercept = 64.7, size = 3, color = "#0070C0") +
  geom_vline(xintercept = 64.9, size = 3, color = "#FFC000") +
  geom_vline(xintercept = 66.1, size = 3, color = "#70AD47") +
  annotate("rect", xmin = 64.7 - 1.95*.424, 
           xmax = 64.7 + 1.95*.424, 
           ymin = 0, ymax = 1.1,
           alpha = .3, fill = "#0070C0") +
  ylim(0, 1.1)

samp_dist_red = means50 %>% 
  ggplot(aes(x=means)) + 
  geom_histogram(color="black", fill="grey", bins=60, 
                 aes(y=..density..)) +
  labs(x="Mean height from samples (inches)", y="Density") +
  scale_x_continuous(breaks = seq(60, 70, by = 1), 
                     limits = c(63, 67)) +
  theme(text = element_text(size = 27), 
        axis.title = element_blank()) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                size = 3, 
                color = "#5BAFF8") +
  geom_vline(xintercept = 65.1, size = 3, color = "#C00000") +
  geom_vline(xintercept = 64.7, size = 3, color = "#0070C0") +
  geom_vline(xintercept = 64.9, size = 3, color = "#FFC000") +
  geom_vline(xintercept = 66.1, size = 3, color = "#70AD47") +
  annotate("rect", xmin = 65.1 - 1.95*.424, 
           xmax = 65.1 + 1.95*.424, 
           ymin = 0, ymax = 1.1,
           alpha = .3, fill = "#C00000") +
  ylim(0, 1.1)

samp_dist_yell = means50 %>% 
  ggplot(aes(x=means)) + 
  geom_histogram(color="black", fill="grey", bins=60, 
                 aes(y=..density..)) +
  labs(x="Mean height from samples (inches)", y="Density") +
  scale_x_continuous(breaks = seq(60, 70, by = 1), 
                     limits = c(63, 67)) +
  theme(text = element_text(size = 27), 
        axis.title = element_blank()) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                size = 3, 
                color = "#5BAFF8") +
  geom_vline(xintercept = 65.1, size = 3, color = "#C00000") +
  geom_vline(xintercept = 64.7, size = 3, color = "#0070C0") +
  geom_vline(xintercept = 64.9, size = 3, color = "#FFC000") +
  geom_vline(xintercept = 66.1, size = 3, color = "#70AD47") +
  annotate("rect", xmin = 64.9 - 1.95*.424, 
           xmax = 64.9 + 1.95*.424, 
           ymin = 0, ymax = 1.1,
           alpha = .3, fill = "#FFC000") +
  ylim(0, 1.1)

samp_dist_green = means50 %>% 
  ggplot(aes(x=means)) + 
  geom_histogram(color="black", fill="grey", bins=60, 
                 aes(y=..density..)) +
  labs(x="Mean height from samples (inches)", y="Density") +
  scale_x_continuous(breaks = seq(60, 70, by = 1), 
                     limits = c(63, 67)) +
  theme(text = element_text(size = 27), 
        axis.title = element_blank()) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                size = 3, 
                color = "#5BAFF8") +
  geom_vline(xintercept = 65.1, size = 3, color = "#C00000") +
  geom_vline(xintercept = 64.7, size = 3, color = "#0070C0") +
  geom_vline(xintercept = 64.9, size = 3, color = "#FFC000") +
  geom_vline(xintercept = 66.1, size = 3, color = "#70AD47") +
  annotate("rect", xmin = 66.1 - 1.95*.424, 
           xmax = 66.1 + 1.95*.424, 
           ymin = 0, ymax = 1.1,
           alpha = .3, fill = "#70AD47") +
  ylim(0, 1.1)

library(patchwork)

samp_dist_blue / samp_dist_yell / samp_dist_red / samp_dist_green

```
:::

::: {.column width="28%"}
Do these confidence intervals include $\mu$?
:::
::::::




## This time: Point estimates with probability assuming population mean $\mu$

:::::: columns
::: {.column width="22%"}
 

![](../img_slides/samp_point_est_vert.png){fig-align="center"}
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 9
#| fig-height: 10
#| fig-align: center

samp_dist_blue = means50 %>% 
  ggplot(aes(x=means)) + 
  geom_histogram(color="black", fill="grey", bins=60, 
                 aes(y=..density..)) +
  labs(x="", y="Density") +
  scale_x_continuous(breaks = seq(60, 70, by = 1), 
                     limits = c(63, 67)) +
  theme(text = element_text(size = 27), 
        axis.title = element_blank()) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                size = 3, 
                color = "#5BAFF8") +
  geom_vline(xintercept = 65.1, size = 3, color = "#C00000") +
  geom_vline(xintercept = 64.7, size = 3, color = "#0070C0") +
  geom_vline(xintercept = 64.9, size = 3, color = "#FFC000") +
  geom_vline(xintercept = 66.1, size = 3, color = "#70AD47") +
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                xlim = c(63, 64.7), 
                geom = "area", fill = "#0070C0", 
                alpha = 0.6)+
  ylim(0, 1.1)

samp_dist_red = means50 %>% 
  ggplot(aes(x=means)) + 
  geom_histogram(color="black", fill="grey", bins=60, 
                 aes(y=..density..)) +
  labs(x="Mean height from samples (inches)", y="Density") +
  scale_x_continuous(breaks = seq(60, 70, by = 1), 
                     limits = c(63, 67)) +
  theme(text = element_text(size = 27), 
        axis.title = element_blank()) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                size = 3, 
                color = "#5BAFF8") +
  geom_vline(xintercept = 65.1, size = 3, color = "#C00000") +
  geom_vline(xintercept = 64.7, size = 3, color = "#0070C0") +
  geom_vline(xintercept = 64.9, size = 3, color = "#FFC000") +
  geom_vline(xintercept = 66.1, size = 3, color = "#70AD47") +
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                xlim = c(65.1, 67), 
                geom = "area", fill = "#C00000", 
                alpha = 0.6)+
  ylim(0, 1.1)

samp_dist_yell = means50 %>% 
  ggplot(aes(x=means)) + 
  geom_histogram(color="black", fill="grey", bins=60, 
                 aes(y=..density..)) +
  labs(x="Mean height from samples (inches)", y="Density") +
  scale_x_continuous(breaks = seq(60, 70, by = 1), 
                     limits = c(63, 67)) +
  theme(text = element_text(size = 27), 
        axis.title = element_blank()) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                size = 3, 
                color = "#5BAFF8") +
  geom_vline(xintercept = 65.1, size = 3, color = "#C00000") +
  geom_vline(xintercept = 64.7, size = 3, color = "#0070C0") +
  geom_vline(xintercept = 64.9, size = 3, color = "#FFC000") +
  geom_vline(xintercept = 66.1, size = 3, color = "#70AD47") +
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                xlim = c(63, 64.9), 
                geom = "area", fill = "#FFC000", 
                alpha = 0.6)+
  ylim(0, 1.1)

samp_dist_green = means50 %>% 
  ggplot(aes(x=means)) + 
  geom_histogram(color="black", fill="grey", bins=60, 
                 aes(y=..density..)) +
  labs(x="Mean height from samples (inches)", y="Density") +
  scale_x_continuous(breaks = seq(60, 70, by = 1), 
                     limits = c(63, 67)) +
  theme(text = element_text(size = 27), 
        axis.title = element_blank()) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                size = 3, 
                color = "#5BAFF8") +
  geom_vline(xintercept = 65.1, size = 3, color = "#C00000") +
  geom_vline(xintercept = 64.7, size = 3, color = "#0070C0") +
  geom_vline(xintercept = 64.9, size = 3, color = "#FFC000") +
  geom_vline(xintercept = 66.1, size = 3, color = "#70AD47") +
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = sig), 
                xlim = c(66.1, 67), 
                geom = "area", fill = "#70AD47", 
                alpha = 0.6)+
  ylim(0, 1.1)

library(patchwork)

samp_dist_blue / samp_dist_yell / samp_dist_red / samp_dist_green

```
:::

::: {.column width="28%"}
Assuming the population mean is $\mu$, what is the probability that we observe $\overline{x}$ or a more extreme sample mean?
:::
::::::


## Is 98.6°F  really the mean "healthy" body temperature?

* __Where did the 98.6°F value come from?__
    * German physician Carl Reinhold August [Wunderlich](https://www.google.com/books/edition/_/a6UNq33GPfIC?hl=en&gbpv=1&pg=PP14) determined  98.6°F (or 37°C) based on temperatures from 25,000 patients in Leipzig in 1851.

* [1992 JAMA article](https://jamanetwork.com/journals/jama/article-abstract/400116) by Mackowiak, Wasserman, & Levine
    * They claim that 98.2°F (36.8°C) is a more accurate average body temp
    * Sample: n = 148 healthy individuals aged 18 - 40 years

* Other research indicating that the human body temperature is lower
    * In January 2020: _[Decreasing human body temperature in the United States since the Industrial Revolution](https://elifesciences.org/articles/49555)_
    * September 2023 update: _[Defining Usual Oral Temperature Ranges in Outpatients Using an Unsupervised Learning Algorithm](https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2809098)_ in JAMA Internal Medicine

* NYT article [The Average Human Body Temperature Is Not 98.6 Degrees](../resources/NYT_What_Is_a_Fever_Why_Your_Body_Temperature_May_Be_Cooler_Than_98.6_Degrees.pdf), Oct 12, 2023, by Dana G. Smith

::: lob
**Question:** based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?
:::

## Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?


Two approaches to answer this question:

::: columns
::: {.column width="49%"}
::: green2
::: green2-ttl
Confidence interval
:::
::: green2-cont
- Create a __confidence interval (CI)__ for the population mean $\mu$ and determine whether 98.6°F is inside the CI or not.
- Answering the question: is 98.6°F a plausible value?
:::
:::
:::

::: {.column width="2%"}
:::
::: {.column width="49%"}
::: pink2
::: pink2-ttl
Hypothesis test
:::
::: pink2-cont
- Run a __hypothesis test__ to see if there is evidence that the population mean $\mu$ is _significantly different_ from 98.6°F or not
- This does not give us a range of plausible values for the population mean $\mu$.
- Instead, we calculate a _test statistic_ and _p-value_ 
- See how likely we are to observe the sample mean $\overline{x}$ or a more extreme sample mean assuming that the population mean $\mu$ is 98.6°F
:::
:::
:::
:::



## [Approach 1: Create a 95% CI for the population mean body temperature]{style="color:#459B99"}

* Use data based on the results from the 1992 JAMA study
    * The original dataset used in the JAMA article is not available
    * However, Allen Shoemaker from Calvin College created a [dataset](http://jse.amstat.org/datasets/normtemp.dat.txt) with the same summary statistics as in the JAMA article, which we will use:

$$\overline{x} = 98.25,~s=0.733,~n=130$$

```{r}
#| include: false
n <- 130
xbar <- 98.25
sd <- 0.733
(tstar <- qt(.975, df=n-1))  # df = n-1
(se <- sd/sqrt(n))
(moe <- tstar * se) 
(LB <- xbar - moe)
(UB <- xbar + moe)
```
::: columns
::: {.column width="40%"}
CI for $\mu$:
\begin{align}
\overline{x} &\pm t^*\cdot\frac{s}{\sqrt{n}}\\
98.25 &\pm `r round(tstar,3)`\cdot\frac{0.733}{\sqrt{130}}\\
98.25 &\pm `r round(moe,3)`\\
(`r round(LB, 3)`&, `r round(UB, 3)`)
\end{align}

:::

::: {.column width="60%"}

 

Used $t^*$ = `qt(.975, df=129)` = `r round(qt(.975, df=129),3)`

 

**Conclusion:  **
We are 95% confident that the (population) mean body temperature is between `r round(LB, 3)`°F and `r round(UB, 3)`°F, which is discernably different than 98.6°F.

:::
:::



## [Approach 2: Hypothesis Test]{style="color:#BF396F"} 

From before: 

* Run a __hypothesis test__ to see if there is evidence that the population mean $\mu$ is _significantly different_ from 98.6°F or not.
    * This does not give us a range of plausible values for the population mean $\mu$.
    
    * Instead, we calculate a _test statistic_ and _p-value_ 
        * to see how likely we are to observe the sample mean $\overline{x}$
        * or a more extreme sample mean 
        * assuming that the population mean $\mu$ is 98.6°F.

__How do we calculate a _test statistic_ and _p-value_?__

- Use the sampling distribution and central limit theorem!!
- Two cases
  - Case 1: suppose we know the population sd $\sigma$
  - Case 2: we don't know the population sd $\sigma$ 



## Case 1: suppose we know the population sd $\sigma$

* [How likely we are to observe the sample mean $\overline{x}$ ,]{style="color:green"}
    * [or a more extreme sample mean, ]{style="color:green"}
    * [assuming that the population mean $\mu$ is 98.6°F?]{style="color:green"}
* Use $\overline{x} = 98.25$, $\sigma=0.733$, and $n=130$

```{r}
#| include: false
n <- 130
xbar <- 98.25
sd <- 0.733
(tstar <- qt(.975, df=n-1))  # df = n-1
(se <- sd/sqrt(n))
mu <- 98.6
mu + 0.06*(1:3)
mu - 0.06*(1:3)

(tstat <- (xbar - mu)/se)
pnorm(tstat)

# right tail cutoff
(mu-xbar) + mu
```
::: columns
::: {.column width="50%"}

```{r}
#| fig.width: 10
#| fig.height: 6
#| echo: false
# specify upper and lower bounds of shaded region below
mu <- 98.6
std <- se

ggplot(data.frame(x = c(mu-4*std, mu+4*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  # stat_function(fun = dnorm, 
  #               args = list(mean = mu, sd = std), 
  #         # specify the upper and lower bounds of the shaded region:
  #               xlim = c(mu-4*std, xbar),             
  #               geom = "area", fill = "darkblue") +
  # the breaks values below might need to be adjusted 
  # if there are too many values showing on the x-axis
  # scale_x_continuous(breaks=(mu-4*std):(mu+4*std)) +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 0.06*(1:5), mu + 0.06*(1:5))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "sample mean",
       title = "Sampling distribution of mean body temperatures") 
```

:::
::: {.column width="50%"}

:::
:::

<!-- * Calculate a _test statistic_ and _p-value_  -->


## Case 2: we don't know the population sd $\sigma$ 

::: columns
::: {.column width="40%"}

* This is usually the case in real life
* We estimate $\sigma$ with the sample standard deviation $s$
* From last time, we know that in this case we need to use the __t-distribution with d.f. = n-1__, instead of the normal distribution 
* [Question: How likely we are to observe the sample mean $\overline{x}$ or a more extreme sample mean, assuming that the population mean $\mu$ is 98.6°F?]{style="color:green"}
* Use $\overline{x} = 98.25$, $s=0.733$, and $n=130$

:::

::: {.column width="60%"}
```{r}
#| include: false
n <- 130
xbar <- 98.25
sd <- 0.733
(tstar <- qt(.975, df=n-1))  # df = n-1
(se <- sd/sqrt(n))
mu <- 98.6
mu + 0.06*(1:3)
mu - 0.06*(1:3)

(tstat <- (xbar - mu)/se)
pnorm(tstat)
pt(tstat, df=n-1)
2*pt(tstat, df=n-1)


```

:::
:::


## Steps in a Hypothesis Test

1. Set the __[level of significance]{style="color:darkorange"}__ $\alpha$

1. Specify the __[null]{style="color:darkorange"}__ ( $H_0$ ) and __[alternative]{style="color:darkorange"}__ ( $H_A$ ) __[hypotheses]{style="color:darkorange"}__
    1. In symbols
    1. In words
    1. Alternative: one- or two-sided?

1. Calculate the __[test statistic]{style="color:darkorange"}__. 

1. Calculate the __[p-value]{style="color:darkorange"}__ based on the observed test statistic and its sampling distribution

1. Write a __[conclusion]{style="color:darkorange"}__ to the hypothesis test
    1. Do we reject or fail to reject $H_0$?
    1. Write a conclusion in the context of the problem


## Step 2: Null & Alternative Hypotheses (1/2)

In statistics, a __hypothesis__ is a statement about the value of an __unknown population parameter__.


A __[hypothesis test]{style="color:darkorange"}__ consists of a test between two competing hypotheses: 

1. a __[null]{style="color:darkorange"}__ hypothesis $H_0$ (pronounced “H-naught”) vs. 
1. an __[alternative]{style="color:darkorange"}__ hypothesis $H_A$ (also denoted $H_1$)

Example of hypotheses in words: 

\begin{aligned}
H_0 &: \text{The population mean body temperature is 98.6°F}\\
\text{vs. } H_A &: \text{The population mean body temperature is not 98.6°F}
\end{aligned}

1. $H_0$ is a claim that there is “no effect” or “no difference of interest.”
1. $H_A$ is the claim a researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis $H_0$

## Step 2: Null & Alternative Hypotheses (2/2)

__[Notation for hypotheses:]{style="color:green"}__

\begin{aligned}
H_0 &: \mu = \mu_0\\
\text{vs. } H_A&: \mu \neq, <, \textrm{or}, > \mu_0
\end{aligned}

We call $\mu_0$ the *null value*

::: columns
::: {.column width="40%"}
$H_A: \mu \neq \mu_0$

::: {style="font-size: 90%;"}
* not choosing a priori whether we believe the population mean is greater or less than the null value $\mu_0$
:::
:::

::: {.column width="30%"}
$H_A: \mu < \mu_0$

::: {style="font-size: 90%;"}
* believe the population mean is **less** than the null value $\mu_0$
:::
:::

::: {.column width="30%"}
$H_A: \mu > \mu_0$

::: {style="font-size: 90%;"}
* believe the population mean is **greater** than the null value $\mu_0$
:::
:::
:::

* $H_A: \mu \neq \mu_0$ is the most common option, since it's the most conservative

Example: 

\begin{aligned}
H_0 &: \mu = 98.6\\
\text{vs. } H_A&: \mu \neq 98.6
\end{aligned}


## Step 3: [Test statistic]{style="color:darkorange"} (& its distribution)
::: columns
::: {.column width="50%"}
__[Case 1: know population sd $\sigma$]{style="color:purple"}__

$$
\text{test statistic} = z_{\overline{x}} = \frac{\overline{x} - \mu_0}{\frac{\sigma}{\sqrt{n}}}
$$

* Statistical theory tells us that [$z_{\overline{x}}$]{style="color:purple"} follows a [__Standard Normal distribution__ $N(0,1)$]{style="color:purple"}
:::

::: {.column width="50%"}

__[Case 2: don't know population sd $\sigma$]{style="color:green"}__

$$
\text{test statistic} = t_{\overline{x}} = \frac{\overline{x} - \mu_0}{\frac{s}{\sqrt{n}}}
$$

* Statistical theory tells us that [$t_{\overline{x}}$]{style="color:green"} follows a [__Student's t distribution__ with degrees of freedom (df) = $n-1$]{style="color:green"}

:::
:::

::: {style="font-size: 90%;"}
$\overline{x}$ = sample mean, $\mu_0$ = hypothesized population mean from $H_0$,  
$\sigma$ = _population_ standard deviation, $s$ = _sample_ standard deviation,  
$n$ = sample size
:::

__[Assumptions]{style="color:darkorange"}__: same as CLT

* __Independent observations__: the observations were collected independently.
* __Approximately normal sample or big n__: the distribution of the sample should be approximately normal, _or_ the sample size should be at least 30.


## Step 3: Test statistic calculation

Recall that $\overline{x} = 98.25$, $s=0.733$, and $n=130.$

The test statistic is:

$$t_{\overline{x}} = \frac{\overline{x} - \mu_0}{\frac{s}{\sqrt{n}}}
= \frac{98.25 - 98.6}{\frac{0.73}{\sqrt{130}}}
= -5.45$$

* Statistical theory tells us that $t_{\overline{x}}$ follows a __Student's t-distribution__ with $d.f. = n-1 = 129$.

::: columns
::: {.column width="75%"}
```{r}
#| fig.height: 4
#| fig.width: 10
#| echo: false
ggplot(data = data.frame(x = c(-6, 6)), aes(x)) + 
  stat_function(fun = dt, args = list(df = 129)) + 
  ylab("") + 
  xlab("t-dist with df = 129") +
  scale_y_continuous(breaks = NULL) + 
  geom_vline(xintercept = c(-5.45,5.45), 
             color = "red")
```

:::
::: {.column width="25%"}
__Assumptions met?__
:::
:::

## Step 4: p-value

The __[p-value]{style="color:darkorange"}__ is the [__probability__ of obtaining a test statistic _just as extreme or more extreme_ than the observed test statistic assuming the null hypothesis $H_0$ is true.]{style="color:darkblue"} 

::: columns
::: {.column width="50%"}
* The $p$-value is a quantification of "surprise"
    * Assuming $H_0$ is true, _how surprised are we with the observed results_?
    * _Ex_: assuming that the true mean body temperature is 98.6°F, how surprised are we to get a sample mean of 98.25°F  (or more extreme)?
    
* If the $p$-value is "small," it means there's a small probability that we would get the observed statistic (or more extreme) when $H_0$ is true.
:::

::: {.column width="50%"}
```{r}
#| fig.width: 10
#| fig.height: 6
#| echo: false
# specify upper and lower bounds of shaded region below
mu <- 98.6
std <- se

ggplot(data.frame(x = c(mu-4*std, mu+4*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  # stat_function(fun = dnorm, 
  #               args = list(mean = mu, sd = std), 
  #         # specify the upper and lower bounds of the shaded region:
  #               xlim = c(mu-4*std, xbar),             
  #               geom = "area", fill = "darkblue") +
  # the breaks values below might need to be adjusted 
  # if there are too many values showing on the x-axis
  # scale_x_continuous(breaks=(mu-4*std):(mu+4*std)) +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 0.06*(1:5), mu + 0.06*(1:5))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "sample mean",
       title = "Sampling distribution of mean body temperatures") 
```
:::
:::



## Step 4: p-value calculation 

Calculate the _p_-value using the __Student's t-distribution__ with $d.f. = n-1 = 129$:

$$p-value=P(T \leq -5.45) + P(T \geq 5.45) = 2.410889 \times 10^{-07}$$

```{r}
# use pt() instead of pnorm()
# need to specify df
2*pt(-5.4548, df = 130-1, lower.tail = TRUE)
```  

```{r}
#| fig.height: 4
#| fig.width: 10
#| echo: false
ggplot(data = data.frame(x = c(-6, 6)), aes(x)) + 
  stat_function(fun = dt, args = list(df = 129)) + 
  ylab("") + 
  xlab("t-dist with df = 129") +
  scale_y_continuous(breaks = NULL) + 
  geom_vline(xintercept = c(-5.45,5.45), 
             color = "red")
```

## Step 4: p-value estimation using $t$-table

* $t$-table only gives us *bounds* on the p-value
* Recall from using the $t$-table for CIs, that the table gives us the cutoff values for varying tail probabilities (1-tail & 2-tail)
* Find the row with the appropriate degrees of freedom
    * Use next smallest df in table if actual df not shown
    * I.e., for df = 129, use df = 100 in table
* Figure out where the test statistic's absolute value is in relation to the values in the columns, i.e. between which columns is the test statistic?
* The header rows for those columns gives the lower & upper bounds for the p-value
    * Choosing one-tail vs. two-tail test, depends on the alternative hypothesis $H_A$.
    * For a 2-sided test ( $H_A: \mu \neq \mu_0$ ), use two-tails 
    * For a 1-sided test ( $H_A: \mu < \textrm{or} > \mu_0$ ), use one-tail 



## Step 1: Significance Level $\alpha$

* __Before doing a hypothesis test__, we set a cut-off for how small the $p$-value should be in order to reject $H_0$.
* We call this the __[significance level]{style="color:darkorange"}__, denoted by the Greek symbol [alpha ( $\alpha$ )]{style="color:darkorange"}
* [Typical $\alpha$]{style="color:darkblue"} values are 
    * 0.05 - _most common by far!!_
    * 0.01 and 0.1
* Decision rule:    
    * When [$p$-value < $\alpha$]{style="color:green"}, we "__[reject]{style="color:green"}__ the null hypothesis [$H_0$]{style="color:green"}."
    * When [$p$-value $\geq \alpha$]{style="color:purple"}, we "__[fail to reject]{style="color:purple"}__ the null hypothesis [$H_0$]{style="color:purple"}."

:::{.callout-important}
* "Failing to reject" $H_0$ is __NOT__ the same as "accepting" $H_0$! 
* By failing to reject $H_0$ we are just saying that we don't have sufficient evidence to support the alternative $H_A$.
* _This does not imply that $H_0$ is true!!_
:::




## Step 5: Conclusion to hypothesis test

\begin{aligned}
H_0 &: \mu = 98.6\\
\text{vs. } H_A&: \mu \neq 98.6
\end{aligned}

* Recall the $p$-value = $2.410889 \times 10^{-07}$ 
* Use $\alpha$ = 0.05.
* Do we reject or fail to reject $H_0$?

__Conclusion statement__:

* Basic: ("stats class" conclusion)
    * There is sufficient evidence that the (population) mean body temperature is discernibly different from 98.6°F ( $p$-value < 0.001).

* Better: ("manuscript style" conclusion)
    * The average body temperature in the sample was 98.25°F (95% CI 98.12, 98.38°F), which is discernibly different from 98.6°F ( $p$-value < 0.001).


## Confidence Intervals vs. Hypothesis Testing

* See also V&H Section 4.3.3

# Running a t-test in R

* Working directory
* Load a dataset - need to specify location of dataset
* R projects
* Run a t-test in R
* `tidy()` the test output using `broom` package

## Load the dataset

* The data are in a csv file called `BodyTemperatures.csv`
* You need to tell R where the dataset is located!
* I recommend saving all datasets in a folder called data. 
    * The code I will be providing you will be set up this way.

* To make it easier to specify where the dataset is located, I recommend using the `here()` function from the `here` package: `here::here()`.

```{r}
# read_csv() is a function from the readr package that is a part of the tidyverse
library(here)   # first install this package

BodyTemps <- read_csv(here::here("data", "BodyTemperatures.csv"))
#                     location: look in "data" folder
#                               for the file "BodyTemperatures.csv"

glimpse(BodyTemps)
```

## `t.test`: base R's function for testing one mean 

* Use the body temperature example with $H_A: \mu \neq 98.6$
* We called the dataset `BodyTemps` when we loaded it

```{r}
glimpse(BodyTemps)

(temps_ttest <- t.test(x = BodyTemps$Temperature,
       # alternative = "two.sided",  # default
       mu = 98.6))
```

Note that the test output also gives the 95% CI using the t-distribution.


## `tidy()` the `t.test` output

* Use the `tidy()` function from the `broom` package for briefer output in table format that's stored as a `tibble`
* Combined with the `gt()` function from the `gt` package, we get a nice table 

```{r}
tidy(temps_ttest) %>% 
  gt() %>% 
  tab_options(table.font.size = 40) # use a different size in your HW
```

* Since the `tidy()` output is a tibble, we can easily `pull()` specific values from it:

::: columns
::: {.column width="50%"}

Using base R's `$`
```{r}
tidy(temps_ttest)$p.value  
```

Advantage: quick and easy
:::

::: {.column width="50%"}
Or the `tidyverse` way: using `pull()` from `dplyr` package 
```{r}
tidy(temps_ttest) %>% pull(p.value)
```

Advantage: can use together with piping (`%>%`) other functions
:::
:::

## Answering a research question

Research question is a generic form: Is there evidence to support that the population mean is different than $\mu$?



## What's next? 

CI's and hypothesis testing for different scenarios:


| Day | Section |  Population parameter   |       Symbol        |       Point estimate       |            Symbol             |
|:----:|:------:|:----------:|:--------:|:----------:|:-------:|
| 10  |   5.1   |        Pop mean         |        $\mu$        |        Sample mean         |           $\overline{x}$           |
| 10  |   5.2   | Pop mean of paired diff | $\mu_d$ or $\delta$ | Sample mean of paired diff |         $\overline{x}_{d}$         |
| 11  |   5.3   |    Diff in pop means    |    $\mu_1-\mu_2$    |    Diff in sample means    |    $\overline{x}_1 - \overline{x}_2$    |
| 12  |   8.1   |     Pop proportion      |         $p$         |        Sample prop         |         $\widehat{p}$         |
| 12  |   8.2   |   Diff in pop prop's    |      $p_1-p_2$      |   Diff in sample prop's    | $\widehat{p}_1-\widehat{p}_2$ |


