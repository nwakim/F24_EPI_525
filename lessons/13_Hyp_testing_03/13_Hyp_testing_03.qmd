---
title: "Lesson 13: Inference for difference in means from two independent samples"
subtitle: "TB sections 5.3"
author: "Meike Niederhausen and Nicky Wakim"
title-slide-attributes:
    data-background-color: "#3070BF"
date: "11/13/2024"
format: 
  revealjs:
    theme: "../simple_NW.scss"
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: Lesson 13 Slides
    html-math-method: mathjax
    highlight-style: arrow
execute:
  echo: true
  freeze: auto
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(oibiostat)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) # new-ish
library(here) # new-ish
library(pwr) # NEW!!

# terminal: for icons
# quarto install extension quarto-ext/fontawesome

# set ggplot theme for slides 
theme_set(theme_bw(base_size = 22))
# theme_update(text = element_text(size=16))  # set global text size for ggplots

set.seed(456)
```

## Where are we?

![](../img_slides/course_map.png){fig-align="center"}

## Where are we? Continuous outcome zoomed in

<br> <br>

![](../img_slides/flowchart_only_continuous.jpg){fig-align="center"}

## Goals for today

### 2-sample t-test (Section 5.3)

-   Statistical inference for difference in means from 2 independent samples
    1.  What are $H_0$ and $H_a$?

    2.  What is the SE for $\bar{x}_1 - \bar{x}_2$?

    3.  Hypothesis test

    4.  Confidence Interval

    5.  Run test in R - using long vs. wide data

    6.  Satterthwaite's df

    7.  Pooled SD

### Power and sample size (4.3.4, 5.4, plus notes)

-   Critical values & rejection region
-   Type I & II errors
-   Power
-   How to calculate sample size needed for a study?

# Learning Objectives

1.  Define paired data and explain how it differs from independent samples in the context of statistical analysis.
2.  Construct confidence intervals for the mean difference in paired data and interpret these intervals in the context of the research question.
3.  Perform the appropriate hypothesis test for paired data.

# Learning Objective

## Different types of inference based on different data types

| Lesson | Section | Population parameter | Symbol (pop) | Point estimate | Symbol (sample) | SE |
|:----------|:----------|:----------|:----------|:----------|:----------|:----------|
| 11 | 5.1 | Pop mean | $\mu$ | Sample mean | $\overline{x}$ | $\dfrac{s}{\sqrt{n}}$ |
| 12 | 5.2 | Pop mean of paired diff | $\mu_d$ or $\delta$ | Sample mean of paired diff | $\overline{x}_{d}$ | $\dfrac{s_d}{\sqrt{n}}$ |
| 13 | 5.3 | Diff in pop means | $\mu_1-\mu_2$ | Diff in sample means | $\overline{x}_1 - \overline{x}_2$ | **????** |
| 15 | 8.1 | Pop proportion | $p$ | Sample prop | $\widehat{p}$ |  |
| 15 | 8.2 | Diff in pop prop's | $p_1-p_2$ | Diff in sample prop's | $\widehat{p}_1-\widehat{p}_2$ |  |

## What are data from two independent sample?

-   Two independent samples: Individuals between and within samples are independent
    - Typically: measure the same outcome for each sample, but typically the two samples differ based on a single variable

 

-   Examples

    -   Any study where participants are randomized to a control and treatment group
    -   Study where create two groups based on whether they were exposed or not to some condition (can be observational)
    -   Book: "Does treatment using embryonic stem cells (ESCs) help improve heart function following a heart attack?"
    -   Book: "Is there evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who do not smoke?"

## For two independent samples: Population parameters vs. sample statistics

::::: columns
::: {.column width="50%"}
[**Population parameter**]{style="color:#C83532;"}

-   Population 1 mean: $\mu_1$
-   Population 2 mean: $\mu_2$

 

-   Mean difference: $\mu_1 - \mu_2$

 

-   Population 1 standard deviation: $\sigma_1$
-   Population 2 standard deviation: $\sigma_2$
:::

::: {.column width="50%"}
[**Sample statistic (point estimate)**]{style="color:#EF85B3;"}

-   Sample 1 mean: $\overline{x}_1$
-   Sample 2 mean: $\overline{x}_2$

 

-   Mean difference between samples: $\overline{x}_1 - \overline{x}_2$

 

-   Sample 1 standard deviation: $s_1$
-   Sample 2 standard deviation: $s_2$
:::
:::::

 


## Does caffeine increase finger taps/min (on average)?

-   [**We will illustrate how to perform a hypothesis test for paired data as we work through this example**]{style="color:#5BAFF8"}

**Study Design**:[^1]

[^1]: Based on following article with extra simulations by me: Hand, David J.; Daly, Fergus; McConway, K.; Lunn, D. and Ostrowski, E. (1993). [A handbook of small data sets](https://www.crcpress.com/A-Handbook-of-Small-Data-Sets/Hand-Daly-McConway-Lunn-Ostrowski/p/book/9780412399206). London, U.K.: Chapman and Hall.

-   70 college students students were trained to tap their fingers at a rapid rate
-   Each then drank 2 cups of coffee (double-blind)
    -   **Control** group: decaf
    -   **Caffeine** group: \~ 200 mg caffeine
-   After 2 hours, students were tested.
-   **Taps/minute** recorded

## Does caffeine increase finger taps/min (on average)?

-   Load the data from the csv file `CaffeineTaps.csv`
-   The code below is for when the data file is in a folder called `data` that is in your R project folder (your working directory)


```{r}
#| fig.width: 10
#| fig.height: 6
CaffTaps <- read_csv(here::here("data", "CaffeineTaps_n35.csv"))

glimpse(CaffTaps)
```

## EDA: Explore the finger taps data

::::: columns
::: {.column width="47%"}
```{r}
#| fig.width: 10
#| fig.height: 8
#| code-fold: true
#| code-summary: Code to make these histograms

ggplot(CaffTaps, aes(x=Taps)) +
  geom_histogram() +
  facet_wrap(vars(Group), ncol=1) +
  labs(y = "Number of people", x = "Taps/minute") +
  theme(text = element_text(size = 30))
```
:::

::: {.column width="53%"}
```{r}
#| code-fold: true
#| code-summary: Summary statistics stratified by group

# get_summary_stats() from 
  # rstatix package
sumstats <- CaffTaps %>% 
  group_by(Group) %>% 
  get_summary_stats(type = "mean_sd") 

sumstats %>% gt() %>% 
  tab_options(table.font.size = 40)
```

Then calculate the difference between the means:
```{r}
diff(sumstats$mean)
```
:::
:::::

# Learning Objectives

## Reference: Steps in a Hypothesis Test

1.  Check the [**assumptions**]{style="color:#3070BF"}

2.  Set the [**level of significance**]{style="color:#3070BF"} $\alpha$

3.  Specify the [**null**]{style="color:#3070BF"} ( $H_0$ ) and [**alternative**]{style="color:#3070BF"} ( $H_A$ ) [**hypotheses**]{style="color:#3070BF"}

    1.  In symbols
    2.  In words
    3.  Alternative: one- or two-sided?

4.  Calculate the [**test statistic**]{style="color:#3070BF"}.

5.  Calculate the [**p-value**]{style="color:#3070BF"} based on the observed test statistic and its sampling distribution

6.  Write a [**conclusion**]{style="color:#3070BF"} to the hypothesis test

    1.  Do we reject or fail to reject $H_0$?
    2.  Write a conclusion in the context of the problem

## Step 1: Check the assumptions

-   The assumptions to run a hypothesis test on a sample are:

    -   **Independent observations**: Each observation from both samples is independent from all other observations
    -   **Approximately normal sample or big n**: the distribution of *each sample* should be approximately normal, *or* the sample size of *each sample* should be at least 30

 

-   These are the criteria for the Central Limit Theorem in Lesson 09: Variability in estimates

 

-   In our example, we would check the assumptions with a statement:

    -   The observations are independent from each other. Each caffeine group (aka sample) has 35 individuals. Thus, we can use CLT to approximate the sampling distribution for each sample. 

## Step 2: Set the level of significance

-   **Before doing a hypothesis test**, we set a cut-off for how small the $p$-value should be in order to reject $H_0$.

-   Typically choose $\alpha = 0.05$

 

- See Lesson 11: Hypothesis Testing 1: Single-sample mean

## Step 3: Null & Alternative Hypotheses

-   **Question**: Is there evidence to support that drinking caffeine increases the number of finger taps/min?

::::: columns
::: {.column width="60%"}
Null and alternative hypotheses in **words**

*Include as much context as possible*

<br>

-   $H_0$: The population difference in mean finger taps/min between the caffeine and control groups is ...

-   $H_A$: The population difference in mean finger taps/min between the caffeine and control groups is ...
:::

::: {.column width="40%"}
Null and alternative hypotheses in **symbols**

\begin{align}
H_0:& \mu_{caff} - \mu_{ctrl} = \\
H_A:& \mu_{caff} - \mu_{ctrl} \\
\end{align}
:::
:::::

## Step 4: Test statistic (part 1)

Recall that in general the test statistic has the form:

$$\text{test stat} = \frac{\text{point estimate}-\text{null value}}{SE}$$ Thus, for a two sample independent means test, we have:

$$\text{test statistic} = \frac{\bar{x}_1 - \bar{x}_2 - 0}{SE_{\bar{x}_1 - \bar{x}_2}}$$

-   What is the formula for $SE_{\bar{x}_1 - \bar{x}_2}$?
-   What is the probability distribution of the test statistic?
-   What assumptions need to be satisfied?

## What distribution does $\bar{X}_1 - \bar{X}_2$ have?

::::: columns
::: {.column width="50%"}
Let $\bar{X}_1$ and $\bar{X}_2$ be the means of random samples from two independent groups, with parameters shown in table:
:::

::: {.column width="50%"}
|             | Group 1    | Group 2    |
|-------------|------------|------------|
| sample size | $n_1$      | $n_2$      |
| pop mean    | $\mu_1$    | $\mu_2$    |
| pop sd      | $\sigma_1$ | $\sigma_2$ |
:::
:::::

::: {style="font-size: 90%;"}
Some theoretical statistics:

-   If $\bar{X}_1$ and $\bar{X}_2$ are independent normal r.v.'s, then $\bar{X}_1 - \bar{X}_2$ is also normal
-   What is the mean of $\bar{X}_1 - \bar{X}_2$?

$$E[\bar{X}_1 - \bar{X}_2] = E[\bar{X}_1] - E[\bar{X}_2] = \mu_1-\mu_2$$

-   What is the standard deviation of $\bar{X}_1 - \bar{X}_2$?

\begin{align}
Var(\bar{X}_1 - \bar{X}_2) &= Var(\bar{X}_1) + Var(\bar{X}_2) = \frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2} \\
SD(\bar{X}_1 - \bar{X}_2) &= \sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}
\end{align}
:::

## Step 4: Test statistic (part 2)

:::::: {style="font-size: 90%;"}
::::: columns
::: {.column width="50%"}
$$
t_{\bar{x}_1 - \bar{x}_2} = \frac{\bar{x}_1 - \bar{x}_2 - 0}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
$$
:::

::: {.column width="50%"}
-   $\bar{x}_1, \bar{x}_2$ are the sample means
-   $\mu_0=0$ is the mean value specified in $H_0$
-   $s_1, s_2$ are the sample SD's
-   $n_1, n_2$ are the sample sizes
:::
:::::

-   Statistical theory tells us that $t_{\bar{x}_1 - \bar{x}_2}$ follows a **student's t-distribution** with
    -   $df \approx$ smaller of $n_1-1$ and $n_2-1$
    -   this is a conservative estimate (smaller than actual $df$ )

**Assumptions**:

-   **Independent observations & samples**
    -   The observations were collected independently.
    -   In particular, the observations from the two groups were not paired in any meaningful way.
-   **Approximately normal samples or big n's**
    -   The distributions of the samples should be approximately normal
    -   *or* *both* their sample sizes should be at least 30.
::::::

## Step 4: Test statistic (part 3)

::::: columns
::: {.column width="40%"}
```{r}
#| echo: false
CaffTaps %>% group_by(Group) %>% get_summary_stats(type = "mean_sd") %>% gt() %>% tab_options(table.font.size = 40)
```

$$
\text{test statistic} = t_{\bar{x}_1 - \bar{x}_2} = \frac{\bar{x}_1 - \bar{x}_2 - 0}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
$$
:::

::: {.column width="60%"}
```{r}
#| include: false
# 1: caffeine
# 2: no caffeine
n1 <- 10
n2 <- 10
(xbar1 <- sumstats$mean[1])
(xbar2 <- sumstats$mean[2])
(diff_x <- xbar1 - xbar2)
(sd1 <- sumstats$sd[1])
(sd2 <- sumstats$sd[2])
mu <- 0

(se <- sqrt(sd1^2/n1 + sd2^2/n2))
(tstat <- (diff_x - mu)/se)

alpha <- 0.05
(p_area <- 1-alpha/2)

1-pt(tstat, df=min(n1 -1, n2-1))
pt(tstat, df=min(n1 -1, n2-1), lower.tail = FALSE)

2*(1-pt(tstat, df=min(n1 -1, n2-1)))
```
:::
:::::

<br>

<br>

<hr>

Based on the value of the test statistic, do you think we are going to reject or fail to reject $H_0$?

## Step 5: p-value

The [**p-value**]{style="color:darkorange"} is the **probability** of obtaining a test statistic *just as extreme or more extreme* than the observed test statistic assuming the null hypothesis $H_0$ is true.

::::: columns
::: {.column width="50%"}
```{r}
#| fig.width: 10
#| fig.height: 4
#| echo: false
# specify upper and lower bounds of shaded region below
mu <- 0
std <- se

# The following figure is only an approximation of the 
# sampling distribution since I used a normal instead
# of t-distribution to make it.

ggplot(data.frame(x = c(mu-5*std, mu+5*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 1*(1:5), mu + 1*(1:5))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "difference in means",
       title = "Sampling distribution of difference in means") +
  geom_vline(xintercept = c(diff_x), 
             color = "red")
```

```{r}
#| fig.height: 3
#| fig.width: 10
#| echo: false
ggplot(data = data.frame(x = c(-5, 5)), aes(x)) + 
  stat_function(fun = dt, args = list(df = min(n1 -1, n2-1))) + 
  ylab("") + 
  xlab("t-dist with df = 9") +
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks=c(mu, mu - (1:5), mu + (1:5))) +
  geom_vline(xintercept = c(tstat), 
             color = "red")
```
:::

::: {.column width="50%"}
Calculate the *p*-value:

```{r}
#| include: false
# 1: caffeine
# 2: no caffeine
# n1 <- 10
# n2 <- 10
# (xbar1 <- sumstats$mean[1])
# (xbar2 <- sumstats$mean[2])
# (diff_x <- xbar1 - xbar2)
# (sd1 <- sumstats$sd[1])
# (sd2 <- sumstats$sd[2])
# mu <- 0
# 
# (se <- sqrt(sd1^2/n1 + sd2^2/n2))
# (tstat <- (diff_x - mu)/se)
# 
# alpha <- 0.05
# (p_area <- 1-alpha/2)

1-pt(tstat, df=min(n1 -1, n2-1))
pt(tstat, df=min(n1 -1, n2-1), lower.tail = FALSE)

2*(1-pt(tstat, df=min(n1 -1, n2-1)))
```
:::
:::::

## Step 6: Conclusion to hypothesis test

\begin{align}
H_0:& \mu_{caff} - \mu_{ctrl} = 0\\
H_A:& \mu_{caff} - \mu_{ctrl} > 0\\
\end{align}

-   Recall the $p$-value = 0.00397
-   Use $\alpha$ = 0.05.
-   Do we reject or fail to reject $H_0$?

**Conclusion statement**:

-   Stats class conclusion
    -   There is sufficient evidence that the (population) difference in mean finger taps/min with vs. without caffeine is greater than 0 ( $p$-value = 0.004).
-   More realistic manuscript conclusion:
    -   The mean finger taps/min were 244.8 (SD = 2.4) and 248.3 (SD = 2.2) for the control and caffeine groups, and the increase of 3.5 taps/min was statistically discrenible ( $p$-value = 0.004).

# Learning Objectives

## 95% CI for the mean difference in cholesterol levels

```{r}
#| echo: false
CaffTaps %>% group_by(Group) %>% get_summary_stats(type = "mean_sd") %>% gt() %>% tab_options(table.font.size = 40)
```

```{r}
#| include: false
# 1: caffeine
# 2: no caffeine
# n1 <- 10
# n2 <- 10
# (xbar1 <- sumstats$mean[1])
# (xbar2 <- sumstats$mean[2])
# (diff_x <- xbar1 - xbar2)
# (sd1 <- sumstats$sd[1])
# (sd2 <- sumstats$sd[2])
# mu <- 0
# 
# (se <- sqrt(sd1^2/n1 + sd2^2/n2))
# (tstat <- (diff_x - mu)/se)
# 
# 1-pt(tstat, df=min(n1 -1, n2-1))
# pt(tstat, df=min(n1 -1, n2-1), lower.tail = FALSE)
# 
# 2*(1-pt(tstat, df=min(n1 -1, n2-1)))
alpha <- 0.05
(p_area <- 1-alpha/2)

(tstar <- qt(p_area, df=min(n1 -1, n2-1))) 
# (se <- sqrt(sd1^2/n1 + sd2^2/n2))
(moe <- tstar * se) 
(LB <- diff_x - moe)
(UB <- diff_x + moe)
```

::::: columns
::: {.column width="50%"}
CI for $\mu_{caff} - \mu_{ctrl}$:

$$\bar{x}_{caff} - \bar{x}_{ctrl} \pm t^* \cdot \sqrt{\frac{s_{caff}^2}{n_{caff}}+\frac{s_{ctrl}^2}{n_{ctrl}}}$$
:::

::: {.column width="50%"}
:::
:::::

<br> <br> <br>

::: {style="font-size: 90%;"}
**Interpretation**:\
We are 95% confident that the (population) difference in mean finger taps/min between the caffeine and control groups is between `r round(LB, 3)` mg/dL and `r round(UB, 3)` mg/dL.

-   *Based on the CI, is there evidence that drinking caffeine made a difference in finger taps/min? Why or why not?*
:::

## R: 2-sample t-test (with long data)

-   The `CaffTaps` data are in a *long* format, meaning that
    -   all of the outcome values are in one column and
    -   another column indicates which group the values are from
-   This is a common format for data from multiple samples, especially if the sample sizes are different.

<!-- Using the caffeine induced finger tapping example with $H_A: \mu_{caff} - \mu_{ctrl} > 0$: -->

```{r}
(Taps_2ttest <- t.test(formula = Taps ~ Group, 
                       alternative = "greater", 
                       data = CaffTaps))
```

<!-- * The test output gives the 1-sided 95% CI since we ran a 1-sided test -->

## `tidy` the `t.test` output

```{r}
# use tidy command from broom package for briefer output that's a tibble
tidy(Taps_2ttest) %>% gt() %>% tab_options(table.font.size = 40)
```

-   Pull the p-value:

```{r}
tidy(Taps_2ttest)$p.value  # we can pull specific values from the tidy output
```

## R: 2-sample t-test (with wide data)

<!-- Using the caffeine induced finger tapping example with $H_A: \mu_{caff} - \mu_{ctrl} > 0$: -->

```{r}
# make CaffTaps data wide: pivot_wider needs an ID column so that it 
# knows how to "match" values from the Caffeine and NoCaffeine groups
CaffTaps_wide <- CaffTaps %>% 
  mutate(id = c(rep(1:10, 2), rep(11:35, 2))) %>% #  "fake" IDs for pivot_wider step
  pivot_wider(names_from = "Group",
              values_from = "Taps")

glimpse(CaffTaps_wide)

t.test(x = CaffTaps_wide$Caffeine, y = CaffTaps_wide$NoCaffeine, alternative = "greater") %>% 
  tidy() %>% gt() %>% tab_options(table.font.size = 40)
```

## Why are the df's in the R output different?

From many slides ago:

-   Statistical theory tells us that $t_{\bar{x}_1 - \bar{x}_2}$ follows a **student's t-distribution** with
    -   $df \approx$ smaller of $n_1-1$ and $n_2-1$
    -   this is a **conservative** estimate (smaller than actual $df$ )

The actual degrees of freedom are calculated using Satterthwaite's method:

$$\nu = \frac{[ (s_1^2/n_1) + (s_2^2/n_2) ]^2}
{(s_1^2/n_1)^2/(n_1 - 1) + (s_2^2/n_2)^2/(n_2-1) }
= \frac{ [ SE_1^2 + SE_2^2 ]^2}{ SE_1^4/df_1 + SE_2^4/df_2 }$$

<hr>

Verify the *p*-value in the R output using $\nu$ = 17.89012:

```{r}
pt(3.3942, df = 17.89012, lower.tail = FALSE)
```

## Pooled standard deviation estimate {.smaller}

-   Sometimes we have reasons to believe that the population SD's from the two groups are equal, such as when randomizing participants to two groups

::::: columns
::: {.column width="60%"}
-   In this case we can use a **pooled SD**:

$$s_{pooled}^2 = \frac{s_1^2 (n_1-1) + s_2^2 (n_2-1)}{n_1 + n_2 - 2}$$
:::

::: {.column width="40%"}
-   $n_1$, $n_2$ are the sample sizes, and
-   $s_1$, $s_2$ are the sample standard deviations
-   of the two groups
:::
:::::

-   We use the pooled SD instead of $s_1^2$ and $s_2^2$ when calculating the standard error

$$SE = \sqrt{\frac{s_{pooled}^2}{n_1} + \frac{s_{pooled}^2}{n_2}}= s_{pooled}\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$$

::::: columns
::: {.column width="50%"}
**Test statistic** with pooled SD:

$$t_{\bar{x}_1 - \bar{x}_2} = \frac{\bar{x}_1 - \bar{x}_2 -0}{s_{pooled}\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$$
:::

::: {.column width="50%"}
**CI** with pooled SD:

$$(\bar{x}_1 - \bar{x}_2) \pm t^{\star} \cdot s_{pooled} \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}$$
:::
:::::

-   The $t$ distribution degrees of freedom are now:

$$df = (n_1 - 1) + (n_2 - 1) = n_1 + n_2 - 2.$$

```{r}
#| echo: false
#| eval: false
(sp <- (2.214^2 + 2.394^2)/2)
sqrt(sp)
```

## R: 2-sample t-test with pooled SD

```{r}
# t-test with pooled SD
t.test(formula = Taps ~ Group, 
       alternative = "greater", 
       var.equal = TRUE,  # pooled SD 
       data = CaffTaps) %>% 
  tidy() %>% 
  gt() %>% tab_options(table.font.size = 40)

# t-test without pooled SD
t.test(formula = Taps ~ Group, 
       alternative = "greater", 
       var.equal = FALSE,  # default, NOT pooled SD 
       data = CaffTaps) %>% 
  tidy() %>% 
  gt() %>% tab_options(table.font.size = 40)
```

Similar output in this case - why??

## What's next?

::: {style="font-size: 90%;"}
CI's and hypothesis tests for different scenarios:

$$\text{point estimate} \pm z^*(or~t^*)\cdot SE,~~\text{test stat} = \frac{\text{point estimate}-\text{null value}}{SE}$$

| Day | Book | Population <br> parameter | Symbol | Point estimate | Symbol | SE |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| 10 | 5.1 | Pop mean | $\mu$ | Sample mean | $\bar{x}$ | $\frac{s}{\sqrt{n}}$ |
| 10 | 5.2 | Pop mean of paired diff | $\mu_d$ or $\delta$ | Sample mean of paired diff | $\bar{x}_{d}$ | $\frac{s_d}{\sqrt{n}}$ |
| 11 | 5.3 | [Diff in pop <br> means]{style="color:green"} | [$\mu_1-\mu_2$]{style="color:green"} | [Diff in sample <br> means]{style="color:green"} | [$\bar{x}_1 - \bar{x}_2$]{style="color:green"} | [$\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$ or pooled]{style="color:red"} |
| 12 | 8.1 | [Pop proportion]{style="color:darkblue"} | [$p$]{style="color:darkblue"} | [Sample prop]{style="color:darkblue"} | [$\widehat{p}$]{style="color:darkblue"} | [**???**]{style="color:red"} |
| 12 | 8.2 | [Diff in pop <br> proportions]{style="color:darkblue"} | [$p_1-p_2$]{style="color:darkblue"} | [Diff in sample <br> proportions]{style="color:darkblue"} | [$\widehat{p}_1-\widehat{p}_2$]{style="color:darkblue"} | [**???**]{style="color:red"} |
:::
