---
title: "Lesson 5: Random variables and Binomial distribution"
subtitle: "TB sections 3.1-3.2"
author: "Meike Niederhausen and Nicky Wakim"
title-slide-attributes:
    data-background-color: "#3070BF"
date: "10/14/2024"
format: 
  revealjs:
    theme: "../simple_NW.scss"
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: Lesson 5 Slides
    html-math-method: mathjax
    highlight-style: arrow
execute:
  echo: true
  freeze: auto
editor_options: 
  chunk_output_type: console
---

# Learning Objectives

1.  Define random variables and how they map to probability distributions
2.  Calculate the expected value and variance of random variables (and combinations of random variables)
3.  BInomial distribution

## Where are we?

![](../img_slides/course_map.png){fig-align="center"}

## Random variables

::::: definition
::: def-ttl
Random variable (RV)
:::

::: def-cont
A **random variable (r.v.)** assigns numerical values (probability) to the outcome of a random phenomenon
:::
:::::

Notation: A random variable is usually denoted with a capital letter such as $X$, $Y$, or $Z$.

## Probability distributions

::::::::::: columns
::::::::: {.column width="65%"}
::::: definition
::: def-ttl
Probability distribution
:::

::: def-cont
A **probability distribution** consists of all disjoint outcomes and their associated probabilities.
:::
:::::

-   We've already seen one [in our heads and tails example](https://seeing-theory.brown.edu/basic-probability/index.html)

::::: green
::: green-ttl
Rules for a probability distribution
:::

::: green-cont
A probability distribution is a list of all possible outcomes and their associated probabilities that satisfies three rules:

1.  The outcomes listed must be disjoint
2.  Each probability must be between 0 and 1
3.  The probabilities must total to 1
:::
:::::
:::::::::

::: {.column width="35%"}
![](/lessons/img_slides/prob_dist_HT.png){fig-align="right" width="498"}
:::
:::::::::::

## In the coin toss example...

-   We can start to define the probability distribution

-   Let's define the coin flip with the random variable $X$

    -   Where $X=1$ if we get a heads and $X=0$ if we get a tails

-   We can create a table for the random variable and probabilities of each outcome:

    | Coin flip ($x$)        | $x=1$ | $x=0$ |
    |------------------------|-------|-------|
    | Probability ($P(X=x)$) | 0.5   | 0.5   |

-   Note: I use $X$ to refer to the random variable and $x$ to refer to the realized value it takes

    -   Then we can write $P(X=x)$ to discuss the probability for each realized value ($x$) of the random variable ($X$)

-   Also note that the sum of the probabilities equal 1 $$\sum_{x=0}^1 P(X=x) = 1$$

## Let's extend this to rolling a die

::::: example
::: ex-ttl
Example 1: Rolling a die
:::

::: ex-cont
Suppose you roll a fair die. Let the random variable (r.v.) $X$ be the outcome of the roll, i.e. the value of the face showing on the die.

1.  What is the probability distribution of the r.v. $X$?
:::
:::::

Â 

![](/lessons/img_slides/die_prob_dist.png)

## Discrete vs. continuous random variables

- Probability distributions are usually either **discrete** or **continuous**, depending on whether the random variable is discrete or continuous.

::: columns
::: {.column width="47%"}
::::: red
::: red-ttl
Discrete random variable
:::

::: red-cont
A **discrete** r.v. $X$ takes on a finite number of values or countably infinite number of possible values.
:::
:::::

Think: 

- Number of heads in a set of coin tosses
- Number of people who have had chicken pox in a random sample
:::

::: {.column width="2%"}
:::

::: {.column width="49%"}
::: pink2
::: pink2-ttl
Continuous random variable
:::

::: pink2-cont
A **continuous** r.v. $X$ can take on any real value in an interval of values or unions of intervals.
:::
:::

Think: 

- Height in a population
- Blood pressure in a population
:::

:::

## Expectation

-   We call the mean of a random variable its **expected value**

-   The expected value is calculated as a **weighted average**

::::: definition
::: def-ttl
Expected value of a *discrete* random variable
:::

::: def-cont
If $X$ takes on outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, ..., $P(X=x_k)$, the expected value of $X$ is the sum of each outcome multiplied by its corresponding probability: 
$$\begin{aligned}\mu = E[X] = & x_1 P(X=x_1) + x_2 P(X=x_2) + \ldots + x_k P(X=x_k) \\ = & \sum_{i=1}^k x_iP(X=x_i) \end{aligned}$$
:::
:::::

## Back to rolling a die

::: {.column width="44%"}
::::: example
::: ex-ttl
Example 1: Rolling a die
:::

::: ex-cont
Let's go back to our fair fie with RV $X$ as the value of the face showing on the die.

2.  What is the expected outcome of the RV $X$?

3.  Now suppose the 6-sided die is not fair. How would we calculate the expected outcome?

| $x$   | $\mathbb{P}(X=x)$ |
|-----|-------------------|
| 1   | 0.10              |
| 2   | 0.20              |
| 3   | 0.05              |
| 4   | 0.05              |
| 5   | 0.25              |
| 6   | 0.35              |
:::
:::::
:::


## Variability of random variables

Just like with data, the variability of a r.v. is described with its variance or standard deviation

::::: definition
::: def-ttl
Variance of a discrete random variable
:::

::: def-cont
If $X$ takes on outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, ..., $P(X=x_k)$ and expected value \ $\mu=E(X)$, then the variance of $X$, denoted by $\text{Var}(X)$ or $\sigma^2$, is

$$\begin{align*}
\text{Var}(X) &= (x_1-\mu)^2 P(X=x_1) + \cdots+ (x_k-\mu)^2 P(X=x_k) \\
	&= \sum_{i=1}^{k} (x_i - \mu)^2 P(X=x_i)
\end{align*}$$
:::
:::::

::::: definition
::: def-ttl
Standard deviation of a discrete random variable
:::

::: def-cont
The standard deviation of $X$, labeled $SD(X)$ or $\sigma$, is $$\sigma = SD(X) = \sqrt{\text{Var}(X)} $$
:::
:::::

## Back to rolling a die


::: {.column width="44%"}
::::: example
::: ex-ttl
Example 1: Rolling a die
:::

::: ex-cont
Suppose you roll a fair 6-sided die. Let the random variable (r.v.)* $X$ be the outcome of the roll, i.e. the value of the face showing on the die. 

4. Find the variance and standard deviation of $X$.

| $x$   | $\mathbb{P}(X=x)$ |
|-----|-------------------|
| 1   | 1/6              |
| 2   | 1/6              |
| 3   | 1/6              |
| 4   | 1/6              |
| 5   | 1/6              |
| 6   | 1/6              |
:::
:::::
:::

## Linear combinations of random variables

::::: definition
::: def-ttl
Linear combinations of random variables
:::

::: def-cont
If $X$ and $Y$ are random variables and $a$ and $b$ are constants, then $$aX + bY$$ is a linear combination of the random variables.
:::
:::::

::: orange
::: orange-ttl
Theorem: Expected value of a linear combination of random variables
:::
::: orange-cont
If $X$ and $Y$ are random variables and $a$ and $b$ are constants, then $$E(aX + bY) = aE(X) + bE(Y)$$ and $$E(aX + b) = aE(X) + b$$
:::
:::

## Keep rolling dice!

::: example
::: ex-ttl
Example: Expected money for rolling 3 dice
:::
::: ex-cont
Let the random variables $X_1, X_2, X_3$ be the values shown on 3 fair 6-sided dice rolls. Suppose you are given in dollars the amount of the first roll, plus twice the value of the second roll, plus 4 times the value of the third roll. How much money do you expect to get?
:::
:::

## Variance of a linear combination

::: orange
::: orange-ttl
Theorem: Variance of a linear combination of random variables
:::
::: orange-cont
If $X$ and $Y$ are **independent** random variables and $a$ and $b$ are constants, then $$\text{Var}(aX +bY) = a^2\text{Var}(X) + b^2\text{Var}(Y)$$
:::
:::

## Keep keep rolling dice!

::: example
::: ex-ttl
Example: Expected money for rolling 3 dice
:::
::: ex-cont
Let the random variables $X_1, X_2, X_3$ be the values shown on 3 fair 6-sided dice rolls. Suppose you are given in dollars the amount of the first roll, plus twice the value of the second roll, plus 4 times the value of the third roll. What are the variance and standard deviation of the amount you get from the 3 rolls?
:::
:::

# Learning objectives

## Binomial distribution

-   Many situations involve modeling independent random events that have 2 possible outcomes (binary), such as

    -   Repeatedly flipping a coin

-   Repeated events are referred to as **trials**

-   The 2 possible outcomes are referred to as **successes** and **failures**.

-   We denote the probability of a success as $p$.

-   We denote the probability of a failure as $q = 1-p$.

## Bernoulli distribution

::::: definition
::: def-ttl
Bernoulli random variable
:::

::: def-cont
**Bernoulli** random variable. If $X$ is a random variable that takes value 1 with probability of success $p$ and 0 with probability $1-p$, then $X$ is a Bernoulli random variable.
:::
:::::

-   We call the probability of success $p$ the **parameter** of the Bernoulli distribution.

-   Each value of $p$ identifies a specific Bernoulli distribution out of the **family** of Bernoulli r.v.'s where $p$ is any value between 0 and 1 (inclusive).

-   If a r.v. $X$ is modeled by a Bernoulli distribution, then we write in shorthand

::: theorem
**Theorem 19**. *Mean and SD of a Bernoulli r.v. If* $X$ is a Bernoulli r.v. with probability of success $p$, then
:::

## Binomial distribution

\$\$

Recall Example 3.17:

-   About 25% of people that test positive for Covid-19 are vaccinated for Covid-19.

-   Define the r.v. $X_i$ to be 1 if someone that tests positive is vaccinated and 0 if they are not vaccinated.

-   Suppose 3 people have tested positive for Covid-19 (independently of each other).

-   Let $T$ denote the number of people that are vaccinated amongst the 3 that tested positive.

## Binomial distribution

The random variable $T$ above is an example of a Binomial random variable.

In general, a random variable $X$ is **Binomial** if the following hold:

1.  The trials are independent.

2.  The number of trials, $n$, is fixed.

3.  Each trial outcome can be classified as a *success* or *failure*.

4.  The probability of a success, $p$, is the same for each trial.

5.  The r.v. $X$ is the total number of successes in the $n$ trials.

## Binomial distribution

::::: definition
::: def-ttl
Binomial distribution
:::

::: def-cont
Distribution of a **Binomial** random variable. Let $X$ be the total number of successes in $n$ independent trials, each with probability $p$ of a success. Then probability of observing exactly $k$ successes in $n$ independent trials is
:::
:::::

-   The parameters of a binomial distribution are $p$ and $n$.

-   If a r.v. $X$ is modeled by a binomial distribution, then we write in shorthand

::: theorem
**Theorem 21**. *Mean and SD of a Binomial r.v. If* $X$ is a binomial r.v. with probability of success $p$, then
:::

## Binomial distribution example

::: {#VaccBinom10 .example}
**Example 22**. ***Vaccinated people testing positive for Covid-19 (revisited)** About 25% of people that test positive for Covid-19 are vaccinated for Covid-19. Suppose 10 people have tested positive for Covid-19 (independently of each other). Let* $X$ denote the number of people that are vaccinated amongst the 10 that tested positive.

1.  *What is the expected value of* $X$?

2.  *What is the SD of* $X$?

3.  *What is the probability that exactly 4 of the 10 people that tested positive are vaccinated?*

4.  *What is the probability that at most 3 of the 10 people that tested positive are vaccinated?*

5.  *What is the probability that at least 5 of the 10 people that tested positive are vaccinated?*
:::

## Bernoulli in R

## Resources!

-   [Seeing theory: random variables](https://seeing-theory.brown.edu/probability-distributions/index.html#section1)
