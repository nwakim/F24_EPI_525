---
title: "Lesson 15: Inference for a single proportion or difference of two (independent) proportions"
subtitle: "TB sections 8.1-8.2"
author: "Meike Niederhausen and Nicky Wakim"
title-slide-attributes:
    data-background-color: "#3070BF"
date: "11/25/2024"
format: 
  revealjs:
    theme: "../simple_NW.scss"
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: Lesson 15 Slides
    html-math-method: mathjax
    highlight-style: arrow
execute:
  echo: true
  freeze: auto
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(oibiostat)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) 
library(here) 
library(pwr) # NEW!!

# terminal: for icons
# quarto install extension quarto-ext/fontawesome

# set ggplot theme for slides 
theme_set(theme_bw(base_size = 22))
# theme_update(text = element_text(size=16))  # set global text size for ggplots

```

# Learning Objectives

1.  Remind ourselves of the Normal approximation of the binomial distribution and define the sampling distribution of a sample proportion

2.  Run a hypothesis test for a single proportion and interpret the results.

3.  Construct and interpret confidence intervals for a single proportion.

4.  Understand how CLT applies to a difference in binomial random variables

5.  Run a hypothesis test for a difference in proportions and interpret the results.

6.  Construct and interpret confidence intervals for a difference in proportions.


```{css, echo=FALSE}
.reveal code {
  max-height: 100% !important;
}
```

## Where are we?

![](../img_slides/course_map.png){fig-align="center"}

# Learning Objectives

::: lob
1.  Remind ourselves of the Normal approximation of the binomial distribution and define the sampling distribution of a sample proportion
:::

2.  Run a hypothesis test for a single proportion and interpret the results.

3.  Construct and interpret confidence intervals for a single proportion.

4.  Understand how CLT applies to a difference in binomial random variables

5.  Run a hypothesis test for a difference in proportions and interpret the results.

6.  Construct and interpret confidence intervals for a difference in proportions.

## Moving to categorical outcomes

-   Previously, we have discussed methods of inference for numerical data

    -   Our outcomes were numerical values
    -   We were doing inference of **means**
    -   We found confidence intervals for means
    -   We ran hypothesis tests for means
    
 

-   Above methods used can be extended to **categorical data**, such as binomial **proportions** or data in two-way tables

 

-   **Categorical data arise frequently in medical research**
  
    - Disease outcomes and patient characteristics are often recorded in natural categories 
    - **Examples:** types of treatment received, whether or not disease advanced to a later stage, or whether or not a patient responded initially to a treatment

## From Lesson 5: Binomial random variable

-   **One specific type of discrete random variable** is a binomial random variable

::::: definition
::: def-ttl
Binomial random variable
:::

::: def-cont
-   $X$ is a binomial random variable if it represents the number of successes in $n$ independent replications (or trials) of an experiment where

    -   Each replicate has two possible outcomes: either **success** or **failure**
    -   The probability of success is $p$
    -   The probability of failure is $q=1-p$
:::
:::::

-   A binomial random variable takes on values $0, 1, 2, \dots, n$.

-   If a r.v. $X$ is modeled by a Binomial distribution, then we write in shorthand $X \sim \text{Binom}(n,p)$

-   Quick example: The number of heads in 3 tosses of a fair coin is a binomial random variable with parameters $n = 3$ and $p = 0.5$.

## From Lesson 5: Binomial distribution

::::: definition
::: def-ttl
Distribution of a **Binomial** random variable
:::

::: def-cont
Let $X$ be the total number of successes in $n$ independent trials, each with probability $p$ of a success. Then probability of observing exactly $k$ successes in $n$ independent trials is

$$P(X = x) = \binom{n}{x} p^x (1-p)^{n-x},  x= 0, 1, 2, \dots, n $$
:::
:::::

-   The parameters of a binomial distribution are $p$ and $n$.

-   If a r.v. $X$ is modeled by a binomial distribution, then we write in shorthand $X \sim \text{Binom}(n,p)$

::::: orange
::: orange-ttl
Mean and variance of a Binomial r.v
:::

::: orange-cont
If $X$ is a binomial r.v. with probability of success $p$, then $E(X) = np$ and $\text{Var}(X)=np(1-p)$
:::
:::::

## From Lesson 6: Normal Approximation of the Binomial Distribution

-   Also known as: **Sampling distribution of** $\widehat{p}$

-   If $X\sim \text{Binomial}(n,p)$ and $np>10$ and $nq = n(1-p) > 10$

    -   Ensures sample size ($n$) is moderately large and the $p$ is not too close to 0 or 1
    -   Other resources use other criteria (like $npq>5$ or $np>5$)

 

-   THEN approximately $$X\sim \text{Normal}\big(\mu_X = np, \sigma_X = \sqrt{np(1-p)} \big)$$

-   **Continuity Correction**: Applied to account for the fact that the binomial distribution is discrete, while the normal distribution is continuous

    -   Adjust the binomial value (# of successes) by ±0.5 before calculating the normal probability.
    -   For $P(X \leq k)$ (Binomial), you would instead calculate $P(X \leq k + 0.5)$ (Normal approx)
    -   For $P(X \geq k)$ (Binomial), you would instead calculate $P(X \leq k - 0.5)$ (Normal approx)
    
## Poll Everywhere Question 1



## Sampling distribution of $\hat{p}$

-   $\hat{p}=\frac{X}{n}$ where $X$ is the number of "successes" and $n$ is the sample size.
-   $X \sim Bin(n,p)$, where $p$ is the population proportion.
-   For $n$ "big enough", the normal distribution can be used to approximate a binomial distribution:

$$X \sim N\Big(\mu = np, \sigma = \sqrt{np(1-p)} \Big)$$

-   Since $\hat{p}=\frac{X}{n}$ is a linear transformation of $X$, we have for large n:

$$\hat{p} \sim N\Big(\mu_{\hat{p}} = p, \sigma_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}} \Big)$$

-   What is "big enough"? At least 10 successes and 10 failures are expected in the sample: $np \geq 10$ and $n(1-p) \geq 10$

## For proportions: Population parameters vs. sample statistics

::::: columns
::: {.column width="50%"}
[**Population parameter**]{style="color:#C83532;"}

-   Proportion: $p$, $\pi$ ("pi")
:::

::: {.column width="50%"}
[**Sample statistic (point estimate)**]{style="color:#EF85B3;"}

-   Sample proportion: $\hat{p}$ ("p-hat")
:::
:::::

 

## Approaches to answer a research question

-   **Research question is a generic form for a single proportion:** Is there evidence to support that the population proportion is different than $p_0$?

:::::::::::::: columns
:::::: {.column width="50%"}
::::: green2
::: green2-ttl
Calculate **CI for the proportion** $p$:
:::

::: green2-cont
```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 2.2
#| fig-align: center
#| warning: false
#| message: false

sample_prop_plot = ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = 1), 
                size = 3, 
                color = "#EF85B3") +
  geom_vline(xintercept = 2, size = 3, color = "#C00000") +
  theme_classic() +
  theme(text = element_text(size = 35), 
        axis.ticks.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.title = element_blank()) +
  scale_x_continuous(breaks=seq(0,2,2), labels=c(expression(p[0]), expression(hat(p))))

sample_prop_plot +
    annotate("rect", xmin = 2 - 1.96*0.5, 
           xmax = 2 + 1.96*0.5, 
           ymin = 0, ymax =0.4,
           alpha = .3, fill = "#C00000")
```

$$\hat{p} \pm z^* \cdot SE_{\hat{p}} = \hat{p} \pm z^* \cdot\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$

-   with $z^*$ = z-score that aligns with specific confidence interval
:::
:::::
::::::

::::::::: {.column width="50%"}
:::::::: pink
::: pink-ttl
Run a **hypothesis test**:
:::

:::::: pink-cont
```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 2.2
#| fig-align: center
#| warning: false
#| message: false


sample_prop_plot +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = 1), 
                xlim = c(2, 4), 
                geom = "area", fill = "#C00000", 
                alpha = 0.6) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = 1), 
                xlim = c(-2, -4), 
                geom = "area", fill = "#C00000", 
                alpha = 0.6)
```

::::: columns
::: column
Hypotheses

\begin{align}
H_0:& p = p_0 \\
H_A:& p \neq p_0 \\
(or&~ <, >)
\end{align}
:::

::: column
Test statistic

$$
z_{\hat{p}} = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0\cdot(1-p_0)}{n}}}
$$
:::
:::::
::::::
::::::::
:::::::::
::::::::::::::

## R code: 1- and 2-sample proportions tests

```{r}
#| eval: false

prop.test(x, 
          n, 
          p = NULL, 
          alternative = c("two.sided", "less", "greater"), 
          conf.level = 0.95, 
          correct = TRUE)
```

- `x`: Counts of successes (can have one x or a vector of multiple x's)
- `n`: Number of trails (can have one n or a vector of multiple n's)
- `p`: Null value that we think the population proportion is
- `alternative`: If alternative hypothesis is $\neq$, $<$, or $>$
    - Default is "two.sided" ($\neq$)
- `conf.level` = Confidence level ($1-\alpha$)
    - Default is `0.05`
- `correct`: Continuity correction, whether we should use it or not
    - Default is `TRUE` (Nicky says keep it this way!)

# Learning Objectives

1.  Remind ourselves of the Normal approximation of the binomial distribution and define the sampling distribution of a sample proportion

::: lob
2.  Run a hypothesis test for a single proportion and interpret the results.
:::

3.  Construct and interpret confidence intervals for a single proportion.

4.  Understand how CLT applies to a difference in binomial random variables

5.  Run a hypothesis test for a difference in proportions and interpret the results.

6.  Construct and interpret confidence intervals for a difference in proportions.

## Example: immune response to advanced melanoma

-   Looking for therapies that trigger an immune response to advanced melanoma

-   In a study where 52 patients were treated concurrently with two new therapies, nivolumab and ipilimumab
    - 21 had an immune response.[^1]

-   **Outcome:** whether or not each person has an immune response

[^1]: Wolchok, et. al. \textit{NEJM} (2013) 369(2): 122-33.

Questions that can be addressed with inference...

-   What is the estimated population probability of immune response following concurrent therapy with nivolumab and ipilimumab? (calculate $\hat{p}$)

-   What is the 95% confidence interval for the estimated population probability of immune response following concurrent therapy with nivolumab and ipilimumab? (95% CI of $p$)

-   In previous studies, the proportion of patients responding to one of these agents was 30% or less. Do these results suggest that the probability of response to concurrent therapy is better than 0.30? (Hypothesis test of null of 0.3)

## Reference: Steps in a Hypothesis Test

1.  Check the [**assumptions**]{style="color:#3070BF"}

2.  Set the [**level of significance**]{style="color:#3070BF"} $\alpha$

3.  Specify the [**null**]{style="color:#3070BF"} ( $H_0$ ) and [**alternative**]{style="color:#3070BF"} ( $H_A$ ) [**hypotheses**]{style="color:#3070BF"}

    1.  In symbols
    2.  In words
    3.  Alternative: one- or two-sided?

4.  Calculate the [**test statistic**]{style="color:#3070BF"}.

5.  Calculate the [**p-value**]{style="color:#3070BF"} based on the observed test statistic and its sampling distribution

6.  Write a [**conclusion**]{style="color:#3070BF"} to the hypothesis test

    1.  Do we reject or fail to reject $H_0$?
    2.  Write a conclusion in the context of the problem

## Step 1: Check the assumptions (easier to do after Step 3)

The sampling distribution of $\hat{p}$ is approximately normal when

1.  The sample observations are independent, and

2.  At least 10 successes and 10 failures are expected in the sample: $np_0 \geq 10$ and $n(1-p_0) \geq 10$.\footnote{This condition is commonly referred to as the success-failure condition.}

 

-   Since $p$ is unknown, it is necessary to substitute $p_0$ (the null value) for $p$ when using the standard error to conduct hypothesis tests
    -   Because we are assuming the standard error of the null hypothesis!

```{r}
#| include: false
p0 <- 0.30
ph <- 21/52
n <- 52

n*p0
n*(1-p0)
```

 

-   For the example, we have $p_0 = 0.30$
    -   We check: $n p_0 = `r n` \cdot `r p0` = `r round(n*p0, 3)` > 10$
    -   We check: $n (1- p_0) = `r n` (1 -`r p0`)= `r round(n*(1-p0), 3)` > 10$

## Step 2: Set the level of significance

-   **Before doing a hypothesis test**, we set a cut-off for how small the $p$-value should be in order to reject $H_0$.

-   Typically choose $\alpha = 0.05$

 

-   See Lesson 11: Hypothesis Testing 1: Single-sample mean

## Step 3: Null & Alternative Hypotheses (1/2)

::::::::::: columns
:::::: column
::::: blue
::: blue-ttl
Notation for hypotheses (for paired data)
:::

::: blue-cont
\begin{aligned}
H_0 &: p = p_0\\
\text{vs. } H_A&: p \neq, <, \textrm{or}, > p_0
\end{aligned}
:::
:::::
::::::

:::::: column
::::: pink
::: pink-ttl
Hypotheses test for example
:::

::: pink-cont
\begin{aligned}
H_0 &: p = 0.30\\
\text{vs. } H_A&: p \neq 0.30
\end{aligned}
:::
:::::
::::::
:::::::::::

We call $p_0$ the *null value* (hypothesized population mean difference from $H_0$)

:::::::: columns
::: {.column width="32%"}
$H_A: p \neq p_0$

-   not choosing a priori whether we believe the population proportion is greater or less than the null value $p_0$
:::

::: {.column width="2%"}
:::

::: {.column width="32%"}
$H_A: p < p_0$

-   believe the population proportion is **less** than the null value $p_0$
:::

::: {.column width="2%"}
:::

::: {.column width="32%"}
$H_A: p > p_0$

-   believe the population population proportion is **greater** than the null value $p_0$
:::
::::::::

-   $H_A: p \neq p_0$ is the most common option, since it's the most conservative

## Step 3: Null & Alternative Hypotheses (2/2)

Null and alternative hypotheses in **words** and in **symbols**.

**One sample test**

-   $H_0$: For individuals who have advanced melanoma and received a treatment of nivolumab and ipilimumab, the population proportion of immune response is 0.30

-   $H_A$: For individuals who have advanced melanoma and received a treatment of nivolumab and ipilimumab, the population proportion of immune response is NOT 0.30

\begin{align}
H_0:& p = 0.30\\
H_A:& p \neq 0.30\\
\end{align}

## Step 4: Test statistic

Sampling distribution of $\hat{p}$ if we assume $H_0: p=p_0$ is true:

$$\hat{p} \sim N\left(\mu_{\hat{p}} = p, \sigma_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}} \right) 
\sim N\left(
\mu_{\hat{p}}=p_0, \sigma_{\hat{p}}=\sqrt{\frac{p_0\cdot(1-p_0)}{n}}
\right)$$

Test statistic for a one sample proportion test:

$$
\begin{aligned}
\text{test stat} = & \frac{\text{point estimate}-\text{null value}}{SE}\\
z_{\hat{p}} = & \frac{\hat{p} - p_0}{\sqrt{\frac{p_0\cdot(1-p_0)}{n}}}
\end{aligned}
$$

## Step 4: Test statistic

From our example: Recall that $\hat{p} = \dfrac{21}{52}= `r round(21/52, 4)`$, $n=52$, and $p_0 = 0.30$

```{r}
#| include: false
p0 <- 0.30
n <- 52
n*.30 # ~94
(ph <- 21/n)

(SEp <- sqrt(p0*(1-p0)/n))
(zp <- (ph-p0)/SEp)
```

The test statistic is:

$$
\begin{align}
z_{\hat{p}} &= \frac{\hat{p} - p_0}{\sqrt{\frac{p_0\cdot(1-p_0)}{n}}} = \frac{21/52 - 0.30}{\sqrt{\frac{0.30\cdot(1-0.30)}{52}}} = `r zp`
\end{align}
$$

-   Let's see the z-score on a Z-distribution (Standard Normal curve)

```{r}
#| fig.height: 4
#| fig.width: 10
#| fig-align: center
#| echo: false
ggplot(data = data.frame(x = c(-4, 4)), aes(x)) + 
  stat_function(fun = dnorm) + 
  ylab("") + 
  xlab("z-scores") +
  scale_y_continuous(breaks = NULL) + 
  geom_vline(xintercept = c(-zp,zp), 
             color = "red")
```

## Poll Everywhere Question 2



## Step 5: p-value

The [**p-value**]{style="color:#C83532"} is the **probability** of obtaining a test statistic *just as extreme or more extreme* than the observed test statistic assuming the null hypothesis $H_0$ is true.

::::: columns
::: {.column width="50%"}
```{r}
#| fig.width: 10
#| fig.height: 3.5
#| echo: false
# specify upper and lower bounds of shaded region below
mu <- 0.30
std <- SEp

# The following figure is only an approximation of the 
# sampling distribution since I used a normal instead
# of t-distribution to make it.

ggplot(data.frame(x = c(mu-3*std, mu+3*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 0.03*(1:5), mu + 0.03*(1:5))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "p-hat distribution") +
  geom_vline(xintercept = c(21/52, 0.6 - 21/52), 
             color = "red")
```

```{r}
#| fig.height: 3.5
#| fig.width: 10
#| echo: false
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = 1)) + 
  ylab("") + 
  xlab("z-dist") +
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks=c(mu, mu - (1:5), mu + (1:5))) +
  geom_vline(xintercept = c(-1.624, 1.624), 
             color = "red")
```
:::

::: {.column width="50%"}
Calculate the *p*-value:

```{r}
#| include: false

pnorm(zp)
2*pnorm(zp)
```

\begin{align}
2 &\cdot P(\hat{p}> `r round(ph, 3)`) \\
&= 2 \cdot P\left(Z_{\hat{p}} > \frac{`r round(ph, 3)` - 0.30}{\sqrt{\frac{0.30\cdot(1-0.30)}{52}}}\right)\\
&=2 \cdot P(Z_{\hat{p}} > `r round(zp, 3)`)\\
&= `r 2*pnorm(zp, lower.tail = F)`
\end{align}

```{r}
2*pnorm(1.634, lower.tail = F)
```
:::
:::::

## Step 4-5: test statistic and p-value together using `prop.test()`

```{r}
prop.test(x = 21, n = 52, p = 0.30, correct = T)
```

```{r}
#| code-fold: true
#| code-summary: "Tidying the output of `prop.test()`"
prop.test(x = 21, n = 52, p = 0.30, correct = T) %>% 
  tidy() %>% gt() %>% tab_options(table.font.size = 40)
```

- Note: We expect some differences between the test statistic and p-value calculated by hand vs. by R. R uses a slightly different method to calculate. 

## Step 6: Conclusion to hypothesis test

\begin{align}
H_0:& p = 0.30\\
H_A:& p \neq 0.30\\
\end{align}

-   Recall the $p$-value = `r 2*pnorm(zp, lower.tail = F)`
-   Use $\alpha$ = 0.05.
-   Do we reject or fail to reject $H_0$?

**Conclusion statement**:

-   Stats class conclusion
    -   There is insufficient evidence that the (population) proportion of individuals who had an immune response is different than 0.30 ( $p$-value = 0.102).
-   More realistic manuscript conclusion:
    -   In a sample of 52 individuals receiving treatment, 40.4% had an immune response, which is not different from 30% ( $p$-value = 0.102).

# Learning Objectives

1.  Remind ourselves of the Normal approximation of the binomial distribution and define the sampling distribution of a sample proportion
2.  Run a hypothesis test for a single proportion and interpret the results.

::: lob
3.  Construct and interpret confidence intervals for a single proportion.
:::

4.  Understand how CLT applies to a difference in binomial random variables

5.  Run a hypothesis test for a difference in proportions and interpret the results.

6.  Construct and interpret confidence intervals for a difference in proportions.

## Conditions for one proportion: **test vs. CI**

::::::::::: columns
:::::: {.column width="50%"}
::::: green2
::: green2-ttl
Confidence interval conditions
:::

::: green2-cont
1.  *Independent observations*
    -   The observations were collected independently.

<br>

2.  The number of successes and failures is at least 10:

$$n\hat{p} \ge 10, \ \ n(1-\hat{p})\ge 10$$
:::
:::::
::::::

:::::: {.column width="50%"}
::::: pink
::: pink-ttl
Hypothesis test conditions
:::

::: pink-cont
1.  *Independent observations*
    -   The observations were collected independently.

<br>

2.  The number of **expected** successes and **expected** failures is at least 10.

$$n p_0 \ge 10, \ \ n(1-p_0)\ge 10$$
:::
:::::
::::::
:::::::::::

```{r}
#| include: false
p0 <- 0.36
ph <- 0.35
n <- 269

n*p0
n*(1-p0)

n*ph
n*(1-ph)
```

## 95% CI for population proportion

::::: columns
::: {.column width="50%"}
What to use for SE in CI formula?
:::

::: {.column width="50%"}
$$\hat{p} \pm z^* \cdot SE_{\hat{p}}$$
:::
:::::

::::: columns
::: {.column width="50%"}
Sampling distribution of $\hat{p}$:
:::

::: {.column width="50%"}
$$\hat{p} \sim N\left(\mu_{\hat{p}} = p, \sigma_{\hat{p}} = \sqrt{\frac{p(1-p)}{n}} \right)$$
:::
:::::

::::: columns
::: {.column width="50%"}
Problem: We don't know what $p$ is - it's what we're estimating with the CI.\
Solution: [approximate $p$ with $\hat{p}$]{style="color:#5BAFF8"}:
:::

::: {.column width="50%"}
$$SE_{\hat{p}} = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$
:::
:::::

- Note that I am not using a continuity correction here! This means our "by hand" calculation will be different than our R calculation
  - Using the continuity correction is more widely accepted
  - So I would suggest using R to calculate the confidence intervals when you can!

## 95% CI for population proportion of immune response by hand

95% CI for population mean difference $p$:

```{r}
#| include: false
p0 <- 0.30
ph <- 21/52
n <- 52

(SEph <- sqrt(ph*(1-ph)/n))

alpha <- 0.05
(p_area <- 1-alpha/2)

(zstar <- qnorm(p_area)) 

(moe <- zstar * SEph) 
(LB <- ph - moe)
(UB <- ph + moe)
```

::::: columns
::: {.column width="50%"}
\begin{align}
\hat{p} &\pm z^* \cdot SE_{\hat{p}}\\
\hat{p} &\pm z^* \cdot \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \\ 
`r round(ph,3)` &\pm `r round(zstar,3)`\cdot  \sqrt{\frac{`r round(ph,3)`(1-`r round(ph,3)`)}{`r n`}}  \\
`r round(ph,3)` &\pm `r round(zstar,3)`\cdot `r round(SEph,3)`\\
`r round(ph,3)` &\pm `r round(moe,3)`\\
(`r round(LB, 3)`&, `r round(UB, 3)`)
\end{align}
:::

::: {.column width="50%"}
Used $z^*$ = `qnorm(0.975)` = `r round(qnorm(0.975),3)`

 

**"By hand" Conclusion:**\
We are 95% confident that the (population) proportion of individuals with an immune response is between `r round(LB, 3)` and `r round(UB, 3)`.
:::
:::::

## 95% CI for population proportion of immune response using R

-   We can use R to get similar values

```{r}
prop.test(x = 21, n = 52, conf.level = 0.95, correct = T)
```

```{r}
#| echo: false

tidy_test = prop.test(x = 21, n = 52, conf.level = 0.95, correct = T) %>% tidy()
```

**R Conclusion:**\
We are 95% confident that the (population) proportion of individuals with an immune response is between `r round(tidy_test$conf.low, 3)` and `r round(tidy_test$conf.high, 3)`.

- Note: We expect some differences between the confidence interval calculated by hand vs. by R. R uses a slightly different method to calculate. 

# Break Time!

# Learning Objectives

1.  Remind ourselves of the Normal approximation of the binomial distribution and define the sampling distribution of a sample proportion
2.  Run a hypothesis test for a single proportion and interpret the results.

3.  Construct and interpret confidence intervals for a single proportion.

::: lob
4.  Understand how CLT applies to a difference in binomial random variables
:::

5.  Run a hypothesis test for a difference in proportions and interpret the results.

6.  Construct and interpret confidence intervals for a difference in proportions.

## Inference for difference of two independent proportions <br> $\hat{p}_1-\hat{p}_2$

-   For means, we went from *inferences on single sample mean* to *inferences on difference in means from two independent samples*

 

-   We can do the same thing for proportions

 

-   We will go from [*inferences on single sample proportion*]{style="color:#BF396F"} to [*inferences on difference in proportions from two independent samples*]{style="color:#367B79"}

## Poll Everywhere Question 3

## For difference in proportions: Population parameters vs. sample statistics

::::: columns
::: {.column width="50%"}
[**Population parameter**]{style="color:#C83532;"}

-   Population 1 proportion: $p_1$, $\pi_1$ ("pi")

-   Population 2 proportion: $p_2$, $\pi_2$ ("pi")  

 

-   Difference in proportions: $p_1 - p_2$
:::

::: {.column width="50%"}
[**Sample statistic (point estimate)**]{style="color:#EF85B3;"}

-   Sample 1 proportion: $\hat{p}_1$, $\hat{\pi}_1$ ("pi")

-   Sample 1 proportion: $\hat{p}_2$, $\hat{\pi}_2$ ("pi")  

 

-   Difference in proportions: $\hat{p}_1 - \hat{p}_2$
:::
:::::

 

## Sampling distribution of $\hat{p}_1-\hat{p}_2$

-   $\hat{p}_1=\frac{X_1}{n_1}$ and $\hat{p}_2=\frac{X_2}{n_2}$,
    -   $X_1$ & $X_2$ are the number of "successes"
    -   $n_1$ & $n_2$ are the sample sizes of the 1st & 2nd samples

 

-   Each $\hat{p}$ can be approximated by a normal distribution, for "big enough" $n$
-   Since the difference of independent normal random variables is also normal, it follows that for "big enough" $n_1$ and $n_2$

$$\hat{p}_1 - \hat{p}_2 \sim N \left(\mu_{\hat{p}_1 - \hat{p}_2} = p_1 - p_2, ~~
\sigma_{\hat{p}_1 - \hat{p}_2} =
\sqrt{
\frac{p_1\cdot(1-p_1)}{n_1} + \frac{p_2\cdot(1-p_2)}{n_2}} 
\right)$$

-   What is "big enough"? At least 10 successes and 10 failures are expected in the sample: $n_1p \geq 10$, $n_1(1-p) \geq 10$, $n_2p \geq 10$, and $n_2(1-p) \geq 10$

## Approaches to answer a research question

-   **Research question is a generic form for a single proportion:** Is there evidence to support that the population proportions are different from each other?

:::::::::::::: columns
:::::: {.column width="50%"}
::::: green2
::: green2-ttl
Calculate **CI for the proportion difference** $p_1 - p_2$:
:::

::: green2-cont
```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 2.2
#| fig-align: center
#| warning: false
#| message: false

sample_prop_plot = ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = 1), 
                size = 3, 
                color = "#EF85B3") +
  geom_vline(xintercept = 2, size = 3, color = "#C00000") +
  theme_classic() +
  theme(text = element_text(size = 35), 
        axis.ticks.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.title = element_blank()) +
  scale_x_continuous(breaks=seq(0,2,2), labels=c(0, expression(hat(p)[1] - hat(p)[2] )))

sample_prop_plot +
    annotate("rect", xmin = 2 - 1.96*0.5, 
           xmax = 2 + 1.96*0.5, 
           ymin = 0, ymax =0.4,
           alpha = .3, fill = "#C00000")
```

$$\hat{p}_1 - \hat{p}_2 \pm z^* \cdot SE_{\hat{p}_1 - \hat{p}_2}$$

-   with $z^*$ = z-score that aligns with specific confidence interval
:::
:::::
::::::

::::::::: {.column width="50%"}
:::::::: pink
::: pink-ttl
Run a **hypothesis test**:
:::

:::::: pink-cont
```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 2.2
#| fig-align: center
#| warning: false
#| message: false


sample_prop_plot +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = 1), 
                xlim = c(2, 4), 
                geom = "area", fill = "#C00000", 
                alpha = 0.6) +
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = 1), 
                xlim = c(-2, -4), 
                geom = "area", fill = "#C00000", 
                alpha = 0.6)
```

::::: columns
::: column
Hypotheses

\begin{align}
H_0:& p_1 - p_2 = 0 \\
H_A:& p_1 - p_2 \neq 0 \\
(or&~ <, >)
\end{align}
:::

::: column
Test statistic

$$
z_{\hat{p}_1 - \hat{p}_2} = \frac{\hat{p}_1 - \hat{p}_2}{SE_{pool}}
$$
:::
:::::
::::::
::::::::
:::::::::
::::::::::::::


# Learning Objectives

1.  Remind ourselves of the Normal approximation of the binomial distribution and define the sampling distribution of a sample proportion
2.  Run a hypothesis test for a single proportion and interpret the results.

3.  Construct and interpret confidence intervals for a single proportion.

4.  Understand how CLT applies to a difference in binomial random variables

::: lob
5.  Run a hypothesis test for a difference in proportions and interpret the results.
:::

6.  Construct and interpret confidence intervals for a difference in proportions.

## Motivating example: effectiveness of mammograms

A 30-year study to investigate the effectiveness of mammograms versus a standard non-mammogram breast cancer exam was conducted in Canada with 89,835 participants. Each person was randomized to receive either annual mammograms or standard physical exams for breast cancer over a 5-year screening period.

By the end of the 25-year follow-up period, 1,005 people died from breast cancer. The results are summarized in the following table.


```{r}
#| results: hide
#| echo: false
BC = tibble(Group = c(rep("Mammogram Group", 
                   500+44425), 
                   rep("Control Group", 505+44405)),
              Death = c(rep("Yes", 
                  500),#Not diabetic
          rep("No", 44425),
          rep("Yes", 
              505), #Diabetic
          rep("No", 44405)))
```

```{r}
#| code-fold: true
#| code-summary: "Displaying the contingency table in R"
BC %>% tabyl(Group, Death) %>% 
  adorn_totals(where = c("row", "col")) %>% 
  gt() %>% 
  tab_stubhead(label = "Group") %>%
  tab_spanner(label = "Death from breast cancer?", 
              columns = c("Yes", "No")) %>%
  tab_options(table.font.size = 45)
```

## Reference: Steps in a Hypothesis Test

1.  Check the [**assumptions**]{style="color:#3070BF"}

2.  Set the [**level of significance**]{style="color:#3070BF"} $\alpha$

3.  Specify the [**null**]{style="color:#3070BF"} ( $H_0$ ) and [**alternative**]{style="color:#3070BF"} ( $H_A$ ) [**hypotheses**]{style="color:#3070BF"}

    1.  In symbols
    2.  In words
    3.  Alternative: one- or two-sided?

4.  Calculate the [**test statistic**]{style="color:#3070BF"}.

5.  Calculate the [**p-value**]{style="color:#3070BF"} based on the observed test statistic and its sampling distribution

6.  Write a [**conclusion**]{style="color:#3070BF"} to the hypothesis test

    1.  Do we reject or fail to reject $H_0$?
    2.  Write a conclusion in the context of the problem

## Before we start, we need to calculate the pooled proportion

- Often, our null hypothesis is that the two proportions are equal
  - And that both populations are the same

- Thus, we calculate a pooled proportion to represent the proportion under the null distribution

$$\text{pooled proportion} = \hat{p}_{pool} = \dfrac{\text{total number of successes} }{ \text{total number of cases}} = \frac{x_1+x_2}{n_1+n_2}$$

- In this example:

$$\hat{p}_{pool} = \frac{x_1+x_2}{n_1+n_2} = \frac{500 + 505}{(500 + 44425) + (505 + 44405)} = `r round((500+505)/(500 + 44425+505 + 44405),5)`$$

## Poll Everywhere Question 4


## Step 1: Check the assumptions

**Conditions**:

-   *Independent observations & samples*
    -   The observations were collected independently.
    -   In particular, observations from the two groups weren't paired in any meaningful way.
-   The number of expected successes and expected failures is at least 10 *for each group* - using the pooled proportion:
    -   $n_1\hat{p}_{pool} \ge 10, \ \ n_1(1-\hat{p}_{pool}) \ge 10$
    -   $n_2\hat{p}_{pool} \ge 10, \ \ n_2(1-\hat{p}_{pool}) \ge 10$
    
```{r}
#| include: false

n1 <- 500 + 44425
n2 <- 505 + 44405
x1 <- 500
x2 <- 505
p1 <- x1/n1
p2 <- x2/n2
ppool <- (x1+x2)/(n1+n2)

n1*ppool
n1*(1-ppool)
n2*ppool
n2*(1-ppool)
```

 

- In the example, we check:
    -   $n_1\hat{p}_{pool}=`r format(n1, scientific=F)` \cdot `r round(ppool, 4)` = `r round(n1*ppool, 4)` \ge 10$
    -   $n_1(1-\hat{p}_{pool})=  `r format(n1, scientific=F)` (1-`r round(ppool, 4)`) = `r format(round(n1*(1-ppool), 4), scientific=F)` \ge 10$
    -   $n_2\hat{p}_{pool}=`r format(n2, scientific=F)` \cdot `r round(ppool, 4)` = `r round(n2*ppool, 4)` \ge 10$
    -   $n_2(1-\hat{p}_{pool})=  `r format(n2, scientific=F)` (1-`r round(ppool, 4)`) = `r format(round(n2*(1-ppool), 4), scientific=F)` \ge 10$

## Step 3: Null and Alternative Hypothesis test

**Two samples test**

-   $H_0$: The difference in population proportions of deaths from breast cancer among people who received annual mammograms and annual physical check-ups is 0.

-   $H_A$: The difference in population proportions of deaths from breast cancer among people who received annual mammograms and annual physical check-ups is not 0.

\begin{align}
H_0:& p_{mamm} - p_{ctrl} = 0\\
H_A:& p_{mamm} - p_{ctrl} \neq 0\\
\end{align}

## Step 4: Test statistic (1/2)

Sampling distribution of $\hat{p}_1 - \hat{p}_2$: $$\hat{p}_1 - \hat{p}_2 \sim N \left(\mu_{\hat{p}_1 - \hat{p}_2} = p_1 - p_2, ~~
\sigma_{\hat{p}_1 - \hat{p}_2} =
\sqrt{
\frac{p_1\cdot(1-p_1)}{n_1} + \frac{p_2\cdot(1-p_2)}{n_2}} 
\right)$$

Since we assume $H_0: p_1 - p_2 = 0$ is true, we "pool" the proportions of the two samples to calculate the SE:

$$\text{pooled proportion} = \hat{p}_{pool} = \dfrac{\text{total number of successes} }{ \text{total number of cases}} = \frac{x_1+x_2}{n_1+n_2}$$

Test statistic:

$$
\text{test statistic} = z_{\hat{p}_1 - \hat{p}_2} = \frac{\hat{p}_1 - \hat{p}_2 - 0}{\sqrt{\frac{\hat{p}_{pool}(1-\hat{p}_{pool})}{n_1} + \frac{\hat{p}_{pool}(1-\hat{p}_{pool})}{n_2}}}
$$

## Step 4: Test statistic (2/2)

From our example: Recall that $\hat{p}_1 = \dfrac{500}{44925}= `r round(p1, 4)`$, $\hat{p}_2 = \dfrac{505}{44910}= `r round(p2, 4)`$, $n_1=44925$, $n_2=44910$, and $\hat{p}_{pool} = 0.01119$

```{r}
#| include: false
(ppool <- (x1+x2)/(n1+n2))

(SEpool <- sqrt(ppool*(1-ppool)*(1/n1+1/n2)))
(zpool <- (p1-p2)/SEpool)
```

The test statistic is:

$$
\begin{align}
z_{\hat{p}_1 - \hat{p}_2} = \frac{\hat{p}_1 - \hat{p}_2 - 0}{\sqrt{\frac{\hat{p}_{pool}\cdot(1-\hat{p}_{pool})}{n_1} + \frac{\hat{p}_{pool}\cdot(1-\hat{p}_{pool})}{n_2}}} = \frac{`r round(p1, 4)` -`r round(p2, 4)`}{\sqrt{\frac{`r round(ppool, 5)`\cdot(1-`r round(ppool, 5)`)}{`r format(n1, scientific=F)`} + \frac{`r round(ppool, 5)`\cdot(1-`r round(ppool, 5)`)}{`r format(n2, scientific=F)`}}} = `r zpool`
\end{align}
$$

-   Let's see the z-score on a Z-distribution (Standard Normal curve)

```{r}
#| fig.height: 4
#| fig.width: 10
#| fig-align: center
#| echo: false
ggplot(data = data.frame(x = c(-4, 4)), aes(x)) + 
  stat_function(fun = dnorm) + 
  ylab("") + 
  xlab("z-scores") +
  scale_y_continuous(breaks = NULL) + 
  geom_vline(xintercept = c(-zpool,zpool), 
             color = "red")
```


## Step 5: p-value

The [**p-value**]{style="color:#C83532"} is the **probability** of obtaining a test statistic *just as extreme or more extreme* than the observed test statistic assuming the null hypothesis $H_0$ is true.

::::: columns
::: {.column width="45%"}
```{r}
#| fig.width: 10
#| fig.height: 4
#| echo: false
# specify upper and lower bounds of shaded region below
mu <- 0
std <- SEpool

# The following figure is only an approximation of the 
# sampling distribution since I used a normal instead
# of t-distribution to make it.

ggplot(data.frame(x = c(mu-3*std, mu+3*std)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = mu, sd = std)) + 
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks=c(mu, mu - 0.0003*(1:5), mu + 0.0003*(1:5))) +
  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +
  labs(y = "", 
       x = "p-hat distribution") +
  geom_vline(xintercept = c(p1-p2, p2-p1), 
             color = "red")
```

```{r}
#| fig.height: 3.5
#| fig.width: 10
#| echo: false
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = 0, sd = 1)) + 
  ylab("") + 
  xlab("z-dist") +
  scale_y_continuous(breaks = NULL) + 
  scale_x_continuous(breaks=c(mu, mu - (1:5), mu + (1:5))) +
  geom_vline(xintercept = c(zpool, -zpool), 
             color = "red")
```
:::

::: {.column width="55%"}
Calculate the *p*-value:

```{r}
#| include: false

pnorm(zpool)
2*pnorm(zpool)
```

\begin{align}
& 2 \cdot P(\hat{p}_1 - \hat{p}_2< `r round(p1, 4)` - `r round(p2, 4)`) \\
&= P\left(Z_{\hat{p}_1 - \hat{p}_2} < \frac{`r round(p1, 4)` - `r round(p2, 4)`}{\sqrt{\frac{`r round(ppool, 5)`\cdot(1-`r round(ppool, 5)`)}{`r format(n1, scientific=F)`} + \frac{`r round(ppool, 5)`\cdot(1-`r round(ppool, 5)`)}{`r format(n2, scientific=F)`}}}\right) \\
&= 2 \cdot P(Z_{\hat{p}} > `r round(zpool, 3)`)\\
&= `r 2*pnorm(zpool)`
\end{align}

```{r}
2*pnorm(-0.1639)
```
:::
:::::

## Step 4-5: test statistic and p-value together using `prop.test()`

```{r}
prop.test(x = c(505, 500), n = c(44910, 44925)) # no p needed
```

```{r}
#| code-fold: true
#| code-summary: "Tidying the output of `prop.test()`"
prop.test(x = c(505, 500), n = c(44910, 44925)) %>% 
  tidy() %>% gt() %>% tab_options(table.font.size = 33)
```

- Note: We expect some differences between the test statistic and p-value calculated by hand vs. by R. R uses a slightly different method to calculate. 

## Step 6: Conclusion to hypothesis test

\begin{align}
H_0:& p_{mamm} - p_{ctrl} = 0\\
H_A:& p_{mamm} - p_{ctrl} \neq 0\\
\end{align}

-   Recall the $p$-value = `r round(2*pnorm(-0.1639), 4)`
-   Use $\alpha$ = 0.05
-   Do we reject or fail to reject $H_0$?

**Conclusion statement**:

-   Stats class conclusion
    -   There is insufficient evidence that the difference in (population) proportions of deaths from breast cancer among people who received annual mammograms and annual physical check-ups different ($p$-value = `r round(2*pnorm(-0.1639), 2)`).
-   More realistic manuscript conclusion:
    -   `r round(p1*100, 2)`% of people receiving annual mammograms (n=`r format(n1, scientific=F)`) and `r round(p2*100, 2)`% of people receiving annual physical exams (n=`r format(n1, scientific=F)`) died from breast cancer ($p$-value = `r round(2*pnorm(-0.1639), 2)`).

# Learning Objectives

1.  Remind ourselves of the Normal approximation of the binomial distribution and define the sampling distribution of a sample proportion
2.  Run a hypothesis test for a single proportion and interpret the results.

3.  Construct and interpret confidence intervals for a single proportion.

4.  Understand how CLT applies to a difference in binomial random variables

5.  Run a hypothesis test for a difference in proportions and interpret the results.

::: lob
6.  Construct and interpret confidence intervals for a difference in proportions.
:::

## Conditions for difference in proportions: **test vs. CI**

::::::::::: columns
:::::: {.column width="50%"}
::::: green2
::: green2-ttl
Confidence interval conditions
:::

::: green2-cont
1.  *Independent observations & samples*
    -   The observations were collected independently.
    -   In particular, observations from the two groups weren't paired in any meaningful way.

<br>

2.  The number of successes and failures is at least 10 *for each group*.
    -   $n_1\hat{p}_1 \ge 10, \ \ n_1(1-\hat{p}_1) \ge 10$
    -   $n_2\hat{p}_2 \ge 10, \ \ n_2(1-\hat{p}_2) \ge 10$
:::
:::::
::::::

:::::: {.column width="50%"}
::::: pink
::: pink-ttl
Hypothesis test conditions
:::

::: pink-cont
1.  *Independent observations & samples*
    -   The observations were collected independently.
    -   In particular, observations from the two groups weren't paired in any meaningful way.

<br>

2.  The number of **expected** successes and **expected** failures is at least 10 *for each group* - using the pooled proportion:
    -   $n_1\hat{p}_{pool} \ge 10, \ \ n_1(1-\hat{p}_{pool}) \ge 10$
    -   $n_2\hat{p}_{pool} \ge 10, \ \ n_2(1-\hat{p}_{pool}) \ge 10$
:::
:::::
::::::
:::::::::::

```{r}
#| include: false
p0 <- 0.36
ph <- 0.35
n <- 269

n*p0
n*(1-p0)

n*ph
n*(1-ph)
```

## Poll Everywhere Question 5

## 95% CI for population difference in proportions

::::: columns
::: {.column width="50%"}
What to use for SE in CI formula?
:::

::: {.column width="50%"}
$$\hat{p}_1 - \hat{p}_2 \pm z^* \cdot SE_{\hat{p}_1 - \hat{p}_2}$$
:::
:::::

::::: columns
::: {.column width="50%"}
SE in sampling distribution of $\hat{p}_1 - \hat{p}_2$
:::

::: {.column width="50%"}
$$\sigma_{\hat{p}_1 - \hat{p}_2} =
\sqrt{
\frac{p_1\cdot(1-p_1)}{n_1} + \frac{p_2\cdot(1-p_2)}{n_2}} $$
:::
:::::

::::: columns
::: {.column width="50%"}
Problem: We don't know what $p$ is - it's what we're estimating with the CI.\
Solution: [approximate $p_1$, $p_2$ with $\hat{p}_1$, $\hat{p}_2$]{style="color:#5BAFF8"}:
:::

::: {.column width="50%"}
$$SE_{\hat{p}_1 - \hat{p}_2} = \sqrt{
\frac{\hat{p}_1\cdot(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2\cdot(1-\hat{p}_2)}{n_2}}$$
:::
:::::

## 95% CI for the population difference in proportions

95% CI for population mean difference $p_1 - p_2$:

```{r}
#| include: false
alpha <- 0.05
(p_area <- 1-alpha/2)

(zstar <- qnorm(p_area)) 

(SEp1p2 <- sqrt(p1*(1-p1)/n1 + p1*(2-p2)/n2))

(moe <- zstar * SEp1p2) 
(LB <- p1-p2 - moe)
(UB <- p1-p2 + moe)
```

\begin{align}
\hat{p}_1 - \hat{p}_2 &\pm z^* \cdot SE_{\hat{p}_1 - \hat{p}_2}\\
\hat{p}_1 - \hat{p}_2 &\pm z^* \cdot \sqrt{
\frac{\hat{p}_1\cdot(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2\cdot(1-\hat{p}_2)}{n_2}} \\
`r round(p1, 5)` - `r round(p2, 5)` &\pm 1.96 \cdot \sqrt{\frac{`r round(p1, 5)`\cdot(1-`r round(p1, 5)`)}{`r format(n1, scientific=F)`} + \frac{`r round(p2, 5)`\cdot(1-`r round(p2, 5)`)}{`r format(n2, scientific=F)`}}\\
`r round(ph,3)` &\pm `r round(zstar,3)`\cdot `r round(SEp1p2,3)`\\
`r round(ph,3)` &\pm `r round(moe,3)`\\
(`r round(LB, 3)`&, `r round(UB, 3)`)
\end{align}

Used $z^*$ = `qnorm(0.975)` = `r round(qnorm(0.975),3)`

 

**Interpretation**:\
We are 95% confident that the difference in (population) proportions of deaths due to breast cancer comparing people who received annual mammograms to annual physical check-ups is between `r round(LB, 3)` and `r round(UB, 3)`.


## 95% CI for the population difference in proportions

-   We can use R to get similar values

```{r}
prop.test(x = c(505, 500), n = c(44910, 44925))
```

```{r}
#| echo: false

tidy_2test = prop.test(x = c(505, 500), n = c(44910, 44925), conf.level = 0.95, correct = T) %>% tidy()
```

**R Conclusion:**\
We are 95% confident that the difference in (population) proportions of deaths due to breast cancer comparing people who received annual mammograms to annual physical check-ups is between `r round(tidy_2test$conf.low, 4)` and `r round(tidy_2test$conf.high, 4)`.

- Note: We expect some differences between the confidence interval calculated by hand vs. by R. R uses a slightly different method to calculate. 
