[
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or Slack for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nLink to Webex!!\n\nTBD"
  },
  {
    "objectID": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "href": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or Slack for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nLink to Webex!!\n\nTBD"
  },
  {
    "objectID": "instructors.html#teaching-assistants",
    "href": "instructors.html#teaching-assistants",
    "title": "Instructors",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\nAntara and Ariel were great students in my BSTA 513 class last year! They will have the following office hours and will help answer questions on Slack.\n\nAntara Vidyarthi\nLink to Zoom!!\n\nTuesdays 5:30 - 7pm\n\n\n\nAriel Weingarten\nLink to Webex!!\n\nThursdays 3:30 - 5pm"
  },
  {
    "objectID": "lessons/06_RVs_Binomial/06_RVs_Binomial.html",
    "href": "lessons/06_RVs_Binomial/06_RVs_Binomial.html",
    "title": "Day 6: Random variables and Binomial distribution",
    "section": "",
    "text": "Download slides\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Assistant Professor of Biostatistics in the OHSU-PSU School of Public Health\nFaculty Biostatistician for OHSU’s Biostatistics & Design Program\nAffiliate Investigator for the HSR&D Center to Improve Veteran Involvement in Care (CIVIC) at the Portland VA"
  },
  {
    "objectID": "about.html#meike-niederhausen",
    "href": "about.html#meike-niederhausen",
    "title": "About",
    "section": "",
    "text": "Assistant Professor of Biostatistics in the OHSU-PSU School of Public Health\nFaculty Biostatistician for OHSU’s Biostatistics & Design Program\nAffiliate Investigator for the HSR&D Center to Improve Veteran Involvement in Care (CIVIC) at the Portland VA"
  },
  {
    "objectID": "about.html#course-info",
    "href": "about.html#course-info",
    "title": "About",
    "section": "Course info",
    "text": "Course info\nThis webpage was created for BSTA 511/611 in the OHSU-PSU School of Public Health. BSTA 511 is taken primarily by students in the Biostatistics MS/MPH and Epidemiology MPH programs. BSTA 611 is taken by PhD students in various programs across OHSU."
  },
  {
    "objectID": "about.html#acknowledgements",
    "href": "about.html#acknowledgements",
    "title": "About",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nThank you to Nicky Wakim for creating her amazing webpage for BSTA 550, and sharing her code on GitHub. Having her template made creating this website a much smoother process.\nThank you also to Andrew Bray for presenting the From R Markdown to Quarto ASA Travelling Workshop to the Oregon Chapter of the ASA in June 2023. This workshop was a great starting point in making the switch from RMarkdown to Quarto and also create Quarto webpages."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nDate\nLes-son\nTopic\nKey Info\nSlides QMD\nSlides PDF\nSlides Notes\nRecord-ing\nMuddy Points\n\n\n\n\n1\n9/30\n0\nWelcome\n\n\n\n\n\n\n\n\n\n\n1\nIntroduction to R & RStudio\n\n\n\n\n\n\n\n\n\n10/2\n2\nIntro to Data, Data collection, Summarizing numerical data\n\n\n\n\n\n\n\n\n\n10/3\n\nHW 0 due @ 11 pm\n\n\n\n\n\n\n\n\n2\n10/7\n3\nData visualization, exploratory data analysis (EDA), and summarizing categorical data\n\n\n\n\n\n\n\n\n\n10/9\n4\nProbability\n\n\n\n\n\n\n\n\n\n10/10\n\nHW 1 due @ 11 pm\n\n\n\n\n\n\n\n\n3\n10/14\n5\nBayes’ Theorem\n\n\n\n\n\n\n\n\n\n10/16\n6\nRandom variables, Binomial distribution\n\n\n\n\n\n\n\n\n\n10/17\n\nHW 2 due @ 11 pm\n\n\n\n\n\n\n\n\n4\n10/21\n7\nNormal and Poisson distributions\n\n\n\n\n\n\n\n\n\n10/23\n8\nVariability in estimates\n\n\n\n\n\n\n\n\n\n10/25\n\nHW 3 due @ 11 pm\n\n\n\n\n\n\n\n\n5\n10/28\n\n\n\n\n\n\n\n\n\n\n\n10/30\n9\nConfidence Intervals\n\n\n\n\n\n\n\n\n\n10/31\n\nHW 4 due @ 11 pm\n\n\n\n\n\n\n\n\n6\n11/4\n10\nHypothesis Testing: Single-sample with t-distribution & Two-sample paired data\n\n\n\n\n\n\n\n\n\n11/6\n11\nHypothesis Testing: Two-sample independent data; Power\n\n\n\n\n\n\n\n\n\n11/7\n\nHW 5 due @ 11 pm\n\n\n\n\n\n\n\n\n7\n11/11\n12\nInference for a single proportion or difference in proportions\n\n\n\n\n\n\n\n\n\n11/13\n13\nChi-squared tests, Fishers exact test\n\n\n\n\n\n\n\n\n\n11/14\n\nHW 6 due @ 11 pm\n\n\n\n\n\n\n\n\n8\n11/18\n\n\n\n\n\n\n\n\n\n\n\n11/20\n14\nComparing Means with ANOVA\n\n\n\n\n\n\n\n\n\n11/21\n\nHW 7 due @ 11 pm\n\n\n\n\n\n\n\n\n9\n11/25\n15\nSimple Linear Regression\n\n\n\n\n\n\n\n\n\n11/26?\n\nHW 8 due @ 11 pm\n\n\n\n\n\n\n\n\n10\n12/2\n16\nSimple Linear Regression\n\n\n\n\n\n\n\n\n\n12/4\n17\nNon-parametric tests (sign, signed-rank, rank-sum, Kruskal-Wallis)\n\n\n\n\n\n\n\n\n\n12/5\n\nHW ?? due @ 11 pm\n\n\n\n\n\n\n\n\n11\n12/9\n\nCatch up day"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework Assignments and Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n10/7/23\n\n\nHomework 1\n\n\n4 min\n\n\n\n\n10/14/23\n\n\nHomework 2\n\n\n6 min\n\n\n\n\n10/28/23\n\n\nHomework 4\n\n\n5 min\n\n\n\n\n11/4/23\n\n\nHomework 5\n\n\n3 min\n\n\n\n\n11/11/23\n\n\nHomework 6\n\n\n6 min\n\n\n\n\n11/18/23\n\n\nHomework 7\n\n\n4 min\n\n\n\n\n11/27/23\n\n\nHomework 8\n\n\n5 min\n\n\n\n\n12/4/23\n\n\nHomework 9\n\n\n8 min\n\n\n\n\n10/21/24\n\n\nHomework 3\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "homework.html#assignments",
    "href": "homework.html#assignments",
    "title": "Homework Assignments and Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n10/7/23\n\n\nHomework 1\n\n\n4 min\n\n\n\n\n10/14/23\n\n\nHomework 2\n\n\n6 min\n\n\n\n\n10/28/23\n\n\nHomework 4\n\n\n5 min\n\n\n\n\n11/4/23\n\n\nHomework 5\n\n\n3 min\n\n\n\n\n11/11/23\n\n\nHomework 6\n\n\n6 min\n\n\n\n\n11/18/23\n\n\nHomework 7\n\n\n4 min\n\n\n\n\n11/27/23\n\n\nHomework 8\n\n\n5 min\n\n\n\n\n12/4/23\n\n\nHomework 9\n\n\n8 min\n\n\n\n\n10/21/24\n\n\nHomework 3\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "homework.html#solutions",
    "href": "homework.html#solutions",
    "title": "Homework Assignments and Solutions",
    "section": "Solutions",
    "text": "Solutions\nPlease note that you need to download the .html file to see the LaTeX math properly.\n\n\n\n\n\n\n\n\nHomework\n.qmd file\n.html file\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n5\n\n\n\n\n6\n\n\n\n\n7\n\n\n\n\n8\n\n\n\n\n9"
  },
  {
    "objectID": "syllabus.html#key-course-info",
    "href": "syllabus.html#key-course-info",
    "title": "EPI 525 Syllabus",
    "section": "Key Course Info",
    "text": "Key Course Info\n\nIf an assignment on Sakai is closed, please email me AND the TA your assignment"
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "EPI 525 Syllabus",
    "section": "Description",
    "text": "Description\nWelcome to BSTA 513/613! In this course, we will continue to learn about regression analysis, but not with categorical outcomes. We will build some theoretical understanding in order to interpret and apply logistic regression models appropriately. We will learn how to build a logistic regression model, interpret the model and coefficients, and diagnose potential issues with our model.\n\nCourse Learning Objectives\nAt the end of this course, students should be able to…\n\nApply and interpret a variety of hypothesis-testing procedures for two-way and three-way contingency tables\nCompute and interpret measures of association for binary and ordinal data.\nCalculate and correctly interpret odds ratios using logistic regression, make comparison across groups and examine relationship between binary outcome and predictor variables.\nApply appropriate model-building strategies for logistic regression. Effectively use statistical computing packages for contingency table and logistic regression procedures.\nPerform Poisson regression analysis using count data and interpret model estimates, make comparison across groups and examine relationship between outcome and predictor variables.\nCoherently summarize methods and results of data analyses, and discuss in context of original health-related research questions to audiences with varied statistical background."
  },
  {
    "objectID": "syllabus.html#instructors",
    "href": "syllabus.html#instructors",
    "title": "EPI 525 Syllabus",
    "section": "Instructors",
    "text": "Instructors\nHere is the instructor page. This also has office hours!"
  },
  {
    "objectID": "syllabus.html#meeting-times",
    "href": "syllabus.html#meeting-times",
    "title": "EPI 525 Syllabus",
    "section": "Meeting Times",
    "text": "Meeting Times\nMondays          1:00 PM – 2:50 PM PST in RPV Room A/B\nWednesdays    1:00 PM – 2:50 PM PST in RPV Room A/B"
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "EPI 525 Syllabus",
    "section": "Materials",
    "text": "Materials\n\nTextbooks\n\nApplied Logistic Regression by David Hosmer, Stanley Lemeshow, and Rodney Sturdivant\nAn Introduction to Categorical Data Analysis by Alan Agresti\nIntroduction to Regression Methods for Public Health Using R by Ramzi W. Nahhas\n\nSpecifically Chapter 6\n\n\n\nSupplemental Readings (Optional)\n\nAn Introduction to R (free pdf available)\n\n\n\n\nOnline Resources\n\nSlack\nWe will use Slack as our main form of communication for the class. If you are unsure how to do a homework problem or have other questions, please ask me by posting your question(s) on Slack. Please know that Slack is not guarded by the OSHU firewall, so if you have a question about accommodations or any sensitive topics, you may wish to message me via email. You can still message me regarding sensitive information on Slack, but I will not initiate those conversations on Slack.\n**Please use this invitation link for our Slack workspace!**\nTips on asking questions:\n\nWhen reaching out for help for a homework problem, please include some context on what you have already tried.\n\nFor example, including a photo of your work thus far with an explanation of where you think you might be wrong is a quick way for me to look over your work and help you troubleshoot. This helps me see what parts you already understand and which you need help with. If your question involves code, please include the copied code (not a screenshot) and an attachment to your full file. This way I can see the exact line you need help with and the full code in case the problem starts earlier.\nIf you are unsure how to start a problem, look through your notes and the book for examples that you think might be similar. When reaching out, mention these examples and why you think they might be helpful, but also why you are still unsure on how to proceed.\nIf you only write something similar to “I don’t know how to do problem xxx,” then my response will be to ask you what you tried so far. Thus, it will be quicker for you to let me know this information right away.\n\nIf asking for help about a specific example that you don’t understand, please also provide some detail beyond “I don’t understand example xxx.”\n\nWhich steps of the problem do you not understand? You can refer to a line number, for example.\nIf you don’t understand the first step, do you understand the ones following it?\n\nIn general, when asking a question, please provide the homework number it is from along with the chapter and problem number. If it’s an example from the notes or book, an example number or slide number will help finding it.\nWant more tips on asking questions? This list on this site will help!\nFinally, if I don’t respond within a day and you need a response soon, please remind me by emailing or messaging me again.\n\n\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nWebex\nWebex software will be used for virtual office hours. To give everyone the best possible experience with Webex, I recommend the following best practices:\n\nPlease stay muted until you want to participate\nDuring office hours, please send a message in chat with your question or with a statement like “I have a question.” This makes sure I or the TA can address everyone’s questions in order. \nI encourage you to attend office hours with your video on. This helps me recognize you, and keep mental notes on what techniques/concepts I emphasize to facilitate your specific understanding. \n\n\n\nPoll Everywhere\nWe will use the Poll Everywhere tool as an interactive feature of the course. Poll Everywhere is a web-based application that allows students to participate by responding via text messages or by visiting a web page on an internet-enabled device (smartphone, tablet, laptop). Instructions will be displayed on-screen. The poll that is embedded within the presentation will update in real time. While there is no cost to use this software, standard text messaging rates will apply if you use your phone. Please make sure that you have a Poll Everywhere account before our first class. You are not required to use your OHSU/PSU email to make an account. \nDuring lectures I will pose questions to the class. These questions are designed to provide real-time feedback to both students and the instructor on how well students are grasping the material. This is meant to be an interactive, learning activity with NO contribution to your grade. Your identity will never be connected to your answers, so I encourage you to answer honestly.\n\n\nPennState STAT 504 Website\nPennState has a class offered to online MS students that has some overlap with our class. They have all their course notes posted on this page. This is a great source if you would like to see class notes with different phrasing.\nNot all of our topics are covered in their notes, but the most important ones are. If you are having trouble finding our course’s concepts on their page, please make ask me at Office Hours, after class, or in a private meeting. I do not explicitly state corresponding sections under our schedule because I believe it is important for you to develop skills involving resources and learning key words that can help you find answers. \n\n\nR: Statistical Computing Software\nStudents will use statistical software to complete homework assignments. Students are required to use R/RStudio for this course. R can be freely downloaded. Helpful documentation on installing R is available. I encourage you to install R prior to attending our first lecture. Please email me if you need help installing R or RStudio.\nYou will need to download the following three things:\n\nR https://www.r-project.org/\nRstudio https://posit.co/download/rstudio-desktop/\nQuarto https://quarto.org/docs/get-started/\n\n\nAdditional R Resources\nYour learning and practicing of R will hopefully not be limited to this course. One of the best aspects of programming in R is that many resources are freely available online. Here are just a few additional resources you may explore beyond this class to continue your training in R.\n\n\nUseful online R resources\n\nR for the rest of us\nStatistical tools for high-throughput data analysis. ggplot2 essentials\nR-bloggers\nStack Overflow for troubleshooting\nR Graphical Manual\nQuick-R. Accessing the power of R\nR for SAS, STATA, and SPSS Users\nggplot2\nLearn R 4 free\nJoin a local R user groups\nLearning Machines\n\n\n\n\nOnline R courses to complement or refresh material from class\n\nR for the rest of us\nCoursera: R programming\nedX: R basics\nData Carpentry: For Biologists\nData Carpentry: For Ecologists\nPsychiatric R\nR coder"
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "EPI 525 Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThe course is structured around the following four components:\n\n\n\n\n\n\n\n\n\nComponent\nModality\nFrequency\nDescription\n\n\nLecture\nIn person\nTwice, Weekly\nCourse content is provided through in-person lectures. Lectures will consist of didactic lessons, interactive examples, and PollEverywhere questions. Sessions will be recorded through Explain Everything and posted to Sakai. Attending or viewing the lecture within 7 days of the original lecture date is mandatory. Class attendance will be taken through an Exit Ticket. If viewing the lecture asynchronously, you must take the Exit Ticket to verify your attendance.\n\n\nHomework\nOnline\nWeekly\nThe course includes 7 homework assignments. They are an opportunity for you to engage with important concepts, practice coding, and apply calculating skills. Homework assignments should be submitted online, and will be graded by me. Students are encouraged to work in groups for homework assignments, but each person should do their own summary and hand in their work. Homework assignments will be due on Thursday at 11 PM.\n\n\nQuizzes\nIn person\nEvery 3 weeks\nThe purpose of the quizzes is to assess how well you have achieved the learning objectives through questions covering important concepts, conducting statistical processes, and interpreting output. We will have our quizzes in-class, and it will be open book. Students must work on the quizzes independently.\n\n\nProject (Labs and Report)\nOnline\n4 labs, 1 final report\nThe project will be a combination of submitted labs that will span the quarter and one final report submitted at the end of the quarter. This is meant to translate the tools learned in the course to the work one may do in the workforce. This will help instill the procedure for shaping research goals, model selection, analyzing data, and interpreting meaningful results. Labs will guide you through the needed analysis and background for the project. The final report will summarize your work over the labs and more closely align with a journal article. Students will work independently on each lab.\n\n\n\n\n\n\n\n\n\n\nTypes of assessments\nThis class will use a combination of formative and summative assessments to build and test our knowledge. Below I define each of these types of assessments:\n\nFormative assessment: Activity or work meant to help students learn and practice. Feedback on these assessments are meant to help the instructor and student identify gaps in knowledge and highlight accomplishments.\nSummative assessment: Work meant to test how well students have achieved learning objectives. Grading of these assessments are meant to gauge how well a student grasps the learning objectives and will be able to use their knowledge outside of the classroom."
  },
  {
    "objectID": "syllabus.html#assessment-breakdown",
    "href": "syllabus.html#assessment-breakdown",
    "title": "EPI 525 Syllabus",
    "section": "Assessment Breakdown",
    "text": "Assessment Breakdown\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\nGrades will be based on homework assignments, midterm exam, class “attendance”, and final exam, as follows:\n\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade (BSTA 513)\nPercentage of final grade (BSTA 613)\n\n\nHomework\nFormative\nEvery 1-2 weeks\n33%\n28%\n\n\nQuizzes\nSummative\n4/22, 5/13, 6/3\n25%\n25%\n\n\nProject Labs\nFormative\nEvery 2-3 weeks\n25%\n25%\n\n\nProject Report\nSummative\n6/13\n10%\n10%\n\n\nExit tickets (Attendance)\nN/A\nTwice Weekly\n5%\n5%\n\n\nMid-Quarter Feedback\nN/A\n5/2\n2%\n2%\n\n\n613 Readings\nFormative\nApprox. every other week\n0%\n5%\n\n\n\n\n\nHomework grading\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homeworks are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect or complete homework, I have a very simple grading policy: Your homework will be given a check mark if you turn in 50% of the questions parts completed (whether the 50% is correct or wrong). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nIf you turn in the homework on time, the TAs will give you feedback (on one or more complete problems). There is no penalty for turning in the homework late, but you will not get feedback on your work. Please make sure to check the solutions or go to office hours to assess your work.\n\n\nLab grading\nWhile these are formative assignments, it is important to complete the whole lab and put in your best effort. Points will be deducted based on the rubric if it is clear that effort is not made or tasks are skipped.\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points."
  },
  {
    "objectID": "syllabus.html#course-instructor-evaluations",
    "href": "syllabus.html#course-instructor-evaluations",
    "title": "EPI 525 Syllabus",
    "section": "Course & Instructor Evaluations",
    "text": "Course & Instructor Evaluations\n\nOngoing Course Feedback\nThroughout the duration of the course, you are also welcome to informally and anonymously submit your feedback through this Microsoft Form or Class Exit Tickets. This form will be available on Sakai. Students can submit feedback at any time and this form will be reviewed regularly by me. Your responses will be anonymous unless you elect to leave your email address. If I have done anything to make you feel uncomfortable, please give me feedback so I can change my behavior. Ultimately, this class is for you, and my individual social identity/behavior should not inhibit your learning. Thank you for your help making BSTA 513/613 a more successful class! Examples of ongoing feedback are:\n\nNicky talks a little fast during lecture time. May you speak slower?\nDuring Office Hours, Dr. Wakim made a face when I asked a question. This face made me feel self-conscious about my question.\nDr. W asked me a question about my experience that made me feel like a monolith. Please do not assume I can speak on behalf of my social identity groups.\nThe in-class examples do not make me more interested in the material.\n\n\n\nMid-quarter Feedback\nDuring the middle of the quarter, I will ask you to submit guided, anonymous feedback. Completion of feedback will be count towards your grade. To insure anonymity, I will ask you to sign a separate, written statement that you completed the feedback.\n\n\nFinal Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement (or lack of). Since our class is on the smaller side, everyone’s participation is needed for feedback to be released."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "EPI 525 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nPlease refer to the Schedule page. I will make changes to this schedule if we need more or less time on a concept. You do not need to read the corresponding chapters in the textbook for each class."
  },
  {
    "objectID": "syllabus.html#how-to-succeed-in-this-course",
    "href": "syllabus.html#how-to-succeed-in-this-course",
    "title": "EPI 525 Syllabus",
    "section": "How to succeed in this course",
    "text": "How to succeed in this course\nEvery professor has different expectations when assigning certain work or providing certain resources. I want to walk through each class resource and assignment so that you know what you can do to succeed in this class. For resources, I want you to optimize the opportunities to learn. For assignments, I want you to know the strategies that students can use to learn the most and prepare for future exams.\n\nResources\n\n\n\n\n\n\n\n\nResource\nWhat is it?\nHow do I use it?\n\n\nOffice Hours\nBlocks of time a professor or TA dedicates for questions. The teaching staff will be located in a specific room. Several students may enter the space at a time and will ask specific or broad questions. If many students attend office hours, a queue will be created so that students can be served equally.\nThe main use of office hours is to ask questions about an assignment or lecture notes. You are welcome to sit and do homework in office hours. OH are also an informal way of meeting fellow students to collaborate with.\n\n\nLectures and lecture recordings\nTime shared between the professor and students where the professor conveys important class material. Material discussed in lectures include concepts, calculations, code, and examples. Lectures are a mix of presentation of information, working through examples together, interactive activities, and in-class polls.\nStudents should attend lectures in person if possible. You should attempt to understand new material presented by following the presentation slides, taking notes on additional details that may conveyed verbally, and working through examples with the professor. Students are encouraged to ask questions when you don’t understand the material at any point in the lecture.\n\n\nTextbooks\nWritten and published material that explains concepts, steps through calculations, provides examples, and provides practice problems. The listed textbooks is the basis for this course. While I am to cover all topics in class, the textbook provides alternative explanations and additional examples.\nWhile coming to class having read the accompanying textbook chapters helps understanding during class, I do not expect students to have read it. I see the textbook as a good resource if you are struggling with a specific topic after class, in need of an example while working on homework, or want additional practice when studying for the exam.\n\n\nWebsite\nThe course website is designed by me so that you have access to all the course materials in a more organized and flexible way. All resources delivered from me to you will be available on the website. Any assignments turned in will be through Sakai.\nYou can navigate through different course resources and information using the left-side tabs or top navigation bar. Course materials, like lecture notes, homework, data examples, and recordings, can be found under each week’s page under the schedule tab. You can also find the individual resources under the “Course Materials” tab on the left. Links to turn in assignments through Sakai will be given on the website. Please explore the tabs and get a sense of the organization.\n\n\nSakai\nSakai is a learning management system for higher ed. This is the university sanctioned LMS where we will submit assignments.\nYou will turn in assignments through Sakai under the “Submissions” tab. Generally, there will be a link to each assignment on the course website. You can also view your grades under “Gradebook” and links to Webex under “Webex.”\n\n\n\n\n\nAssignments\n\n\n\n\n\n\n\n\n\nAssignment\nType of assessment\nBefore you submit/take it\nAfter it is graded\n\n\nHomework\nFormative\n\nWork out each problem on your own as much as you can\nTalk through problems with a peer\nGo to Office Hours for help\nWrite down work that shows your thought process\nSearch your issue on Stack Exchange/Stack Overflow\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\n\n\n\nQuizzes\nSummative\n\nIdentify and achieve learning objectives in each lecture\nUnderstand why certain statistics tools are used for certain cases\nPractice testing yourself and others on concepts\nCome to Office Hours for help with specific problems or concepts\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\nDo not ask for a regrade unless you have viewed the solutions\n\n\n\nProject Labs and Report\nFormative and Summtive\n\nStart the lab as early as possible\nWork on R coding and check with classmates on work\nCome to Office Hours for help with specific R work\nFor the report, compile your work from the labs, and decide what is important in the analysis.\n\n\nThis will be graded at the end of the semester, so you will not have a chance to interact with my feedback as much\nIf you have questions about your grade, you may email me\nKeep the project paper for future reference\nYou can add this project to your resume!\n\n\n\nClass Exit Tickets\nN/A\n\nBring appropriate electronic device to participate in polls\nComplete the survey during the last 5 minutes of class or after class within 7 days\n\n\nReview muddiest and clearest points from the week\n\n\n\n\nIf you would like any other course resources explained in this format, please request it through the Ongoing Course Feedback."
  },
  {
    "objectID": "syllabus.html#course-policies-and-resources",
    "href": "syllabus.html#course-policies-and-resources",
    "title": "EPI 525 Syllabus",
    "section": "Course Policies and Resources",
    "text": "Course Policies and Resources\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on June 14, 2024. All coursework is expected to be completed by June 16, 2024 at 11pm. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, there is a due date posted, but you may turn in the assignment any time before the class ends. I will give you the check regardless of when you submit the assignment. However, if you would like feedback on the homework, you must turn it in on time OR email me asking for feedback for your late homework.\nFor non-homework assignments, including labs, I ask you to email me directly. You can explain your circumstances and may ask me for an extension, but I won’t necessarily grant one.\nFor labs, you will have ONE no-questions-asked, 3-day extension. Please use this wisely! You just need to send me a quick email saying “I am using my no-questions-asked extension for Lab __.”\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class, participate in-class polls, and complete the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality. For students who are unable to attend the class in-person and synchronously, viewing the recording within 7 days is acceptable. This is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket to demonstrate attendance.\n\n\nPlagiarism and Attribution\nPlease note that this section has been motivated by Dr. Steven Bedrick’s Course Policies and Grading site for BMI 525. (Note that this is a good example of informal attribution of someone else’s work.)\nIn this class, it is easy to use ChatGPT or other AI tools to solve your homework for you. Many problems follow a basic structure that is especially easy for ChatGPT to solve. In this class, you may use ChatGPT to help with your homework. You may even ask for direct answers. However, there are a few things I do not want you to do:\n\nDo not copy ChatGPT’s answer directly into your homework. Your homework is graded for full credit if you turn it in, in any state, so turning in ChatGPT’s answers is unacceptable. I rather see half-written answers that show what you’re thinking than see a correct answer from ChatGPT.\nDo not stop once ChatGPT answered a question. If it gives an explanation, interact with it! Make sure you understand the thought process of ChatGPT. Try writing out the process to help cement it in your head. Check the answer with what we learn in class.\nDo not use ChatGPT on our quizzes! Hence, you need to really understand how to solve these problems even if you use ChatGPT on the homework.\n\nAt the end of the day, ChatGPT is a resource that will be available to you in a job and outside of school. Thus, we should use it as a tool in school as well! Let me know if ChatGPT helped you understand something! I would love to incorporate it into future classes!\n\n\n\n\n\n\nImportant\n\n\n\nYou can think of this class as assembling a toolbox. When a handyperson starts working for the first time, they need to buy their tools. For their first few jobs, they might need help finding their tools, or remembering which tool is best used for what action. Eventually, they get to know their tools well, and using them appropriately becomes second nature.\nFor now, ChatGPT can help us find and use our tools, but we need to work towards using them as second nature!"
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "EPI 525 Syllabus",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nInstructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple cultural perspectives, and I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. I will provide detailed feedback on your submissions and will update grades promptly in Sakai.\n\n\nStudent Expectations and Resources\nAttend class\nYou are expected to attend all scheduled class meetings synchronously or watch the recording within 7 days. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and online discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the quarter so that I can help you find a solution.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed.  I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact me.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of exams, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "syllabus.html#course-communications",
    "href": "syllabus.html#course-communications",
    "title": "EPI 525 Syllabus",
    "section": "Course Communications",
    "text": "Course Communications\nSakai/Slack announcements\nFor important/urgent matters, I will communicate with you using announcements via Sakai that will be delivered to your OHSU Email account as well as displayed in the Sakai course site Announcements section. I will copy these announcements in Slack if they do not involve changes to the schedule. Unfortunately, there are certain announcements that OHSU requires I initiate behind the firewall.\nGeneral course questions\nIt is normal to have many questions about things that relate to the course, such as clarification about assignments, course materials, or assessments. Please post these on our Slack Workspace. Please use the channels that I created for questions. You are encouraged to give answers and help each other. I will monitor these threads, so I will endorse or correct responses as needed. Please give me 24 hours to respond to questions within Monday-Friday. Work-life balance is important for me as well, so I will try to respond as quickly as I can within my healthy limits. \nE-mail\nE-mail should be used only for messages that are private in nature. Please send private messages to my OHSU email address (wakim@ohsu.edu). Messages sent through Sakai Inbox will not be answered. Do not send messages asking general information about the class; please post those on Slack instead."
  },
  {
    "objectID": "syllabus.html#further-student-resources",
    "href": "syllabus.html#further-student-resources",
    "title": "EPI 525 Syllabus",
    "section": "Further Student Resources",
    "text": "Further Student Resources\n\nSPH Writing Lab\nThe School of Public Health Writing Support serves graduate students (master’s and PhD) in SPH, offering help on all professional writing tasks, including class papers, dissertations, job application documents, personal statements, and grant applications, to name a few. Leslie Bienen, MFA, DVM offers one-on-one writing support and other workshops. Appointments are virtual for the time being. You can make an appointment by contacting writingsupportsph@pdx.edu or making an appointment through Calendly.\n\n\nGrammarly Subscription\nThe School of Public Health students have access to a subscription version of Grammarly. While Grammarly cannot improve the argument and flow of your work, it can help with spelling, grammar, and sentence structure. If you are interested in this tool, please add your name to this email form and they will get you added to the subscription. Be sure to use your PSU login credentials to access the form.\n\n\nStudent Wellness\nI am committed to supporting the physical and emotional well-being of my students. Both PSU and OHSU have designated centers for student health. For OHSU, students can visit the Behavioral Health site, where you can find more information including the number to make an appointment. All student visits are free. OHSU students also have access to PSU’s Counseling Services through the school’s Student Health & Counseling. Information on additional student resources for OHSU students are available on the OHSU Health and Wellness Resource page. \n\n\nSupport for Food Insecurity\nStudents across the country experience food insecurity at alarming rates. OHSU and PSU both provide a list of resources to help combat food insecurity. Of note, the Committee to Improve Student Food Security (CISFS) at PSU provides a Free Food Market on the second Monday of each month. OHSU also provides SNAP Enrollment Assistance. The Supplemental Nutrition Assistance Program (SNAP) allocates money towards food for individuals below a certain income level. If you make less than $2,430 monthly, you may wish to enroll.\n\n\nSupport for Students with Children\nStudents who have children can use the PSU resource: Resource Center for Students with Children. Resources are mostly focused on students with younger children. There are several great resources available, including: family-friendly study spaces, new baby starter packs, free kids clothing, and further information on financial resources for childcare."
  },
  {
    "objectID": "syllabus.html#school-policies-and-resources",
    "href": "syllabus.html#school-policies-and-resources",
    "title": "EPI 525 Syllabus",
    "section": "School Policies and Resources",
    "text": "School Policies and Resources\n\nSchool of Public Health Handbook\nAll students are responsible for following the policies and expectations outlined in the student handbook for their program of study. Students are responsible for their own academic work and are expected to have read and practice principles of academic honesty, as presented in the handbook.\n\n\nStudent Access & Accommodations\nThe School of Public Health values diversity and inclusion; we are committed to fostering mutual respect and full participation for all students. My goal is to create a learning environment that is equitable, usable, inclusive, and welcoming. If any aspects of instruction or course design result in barriers to your inclusion or learning, please notify me. \n\nIf you are already registered with disability services at either OHSU or PSU and you are taking a course at the opposite institution, you need to contact the office you’re registered with to transfer your accommodations.\nIf you are not already registered with a disability services office, and you have, or think you may have, a disability that may affect your work in this class, and feel you need accommodations, use the following table for guidance about which office to contact to initiate accommodations.\n\nResource Table\n\n\n\nEnrollment University and Standing\nWhere to Seek Accommodations\n\n\nUndergraduate School of Public Health major\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\n\n\n\nAll PSU-registering Dual Degree (MSW/MPH and MURP/MPH) Graduate School of Public Health Majors and all PSU-registering PhD students admitted prior to fall 2016.\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\n\nGraduate School of Public Health major (irrespective of institution at which you register)\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\nNon-SPH major, PSU-enrolled student\nPSU’s Disability Resource Center\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\nNon-SPH major, OHSU-enrolled student\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\n\n \nFor more information related accessibility and accommodations, please see the “Statement Regarding Students with Disabilities” within the Institutional Policies section of this syllabus.\n\n\nTitle IX\nThe School of Public Health is committed to providing an environment free of all forms of prohibited discrimination and discriminatory harassment. The School of Public Health students who have questions about an incident related to Title IX are welcome to contact either the OHSU or PSU’s Title IX Coordinator and they will direct you to the appropriate resource or office. Title IX pertains to any form of sex/gender discrimination, discriminatory harassment, sexual harassment or sexual violence.\n\nPSU’s Title IX Coordinator is Julie Caron, she may be reached at titleixccordinator@pdx.edu or 503-725-4410. Julie’s office is located at 1600 SW 4th Ave, In the Richard and Maureen Neuberger Center RMNC - Suite 830.\nThe OHSU Title IX Coordinator’s may be reachedat 503-494-0258 or titleix@ohsu.edu and is located at 2525 SW 3rd St.\n\nPlease note that faculty and the Title IX Coordinators will keep the information you disclose private but are not confidential. If you would like to speak with a confidential advocate, who will not disclose the information to a university official without your written consent, you may contact an advocate at PSU or OHSU.\n\nPSU’s confidential advocates are available in Women’s Resource Center (serving all genders) in Smith Student Memorial Union 479. You may schedule an appointment by (503-725-5672) or schedule on line at https://psuwrc.youcanbook.me. For more information about resources at PSU, please see PSU’s Response to Sexual Misconduct website.\nOHSU’s advocates are available through the Confidential Advocacy Program (CAP) at 833-495-CAPS (2277) or by email CAPsupport@ohsu.edu, but please note, email is not a secure form of communication. Also visit www.ohsu.edu/CAP.\n\nAt OHSU, if you encounter any harassment, or discrimination based on race, color, religion, age, national origin or ancestry, veteran or military status, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity or expression, disability or any other protected status, please contact the Affirmative Action and Equal Opportunity (AAEO) Department at 503-494-5148 or aaeo@ohsu.edu.\nAt PSU, you may contact the Office of Equity and Compliance if you experience any form of discrimination or discriminatory harassment as listed above at equityandcompliance@pdx.edu or by calling 503-725-5919.\n\n\nTechnical Support\nThe OHSU ITG Help Desk is available to assist students with email account or network account access issues between 6 a.m. and 6 p.m., Monday through Friday at 503-494-2222. For technical support in using the Sakai Course Management System, please contact the Sakai Help Desk at 877-972-5249 or email us at sakai@ohsu.edu"
  },
  {
    "objectID": "syllabus.html#ohsu-competencies",
    "href": "syllabus.html#ohsu-competencies",
    "title": "EPI 525 Syllabus",
    "section": "OHSU Competencies",
    "text": "OHSU Competencies\n\nList of OHSU Graduation Core Competencies\n\nProfessional Knowledge and Skills\nProfessionalism\nInformation Literacy\nCommunication\nTeamwork\nCommunity Engagement, Social Justice and Equity\nPatient Centered Care\n\nTo access a descriptive list of OHSU Graducation Core Competencies: OHSU Graduation Core Competencies"
  },
  {
    "objectID": "syllabus.html#institutional-policies-and-resources",
    "href": "syllabus.html#institutional-policies-and-resources",
    "title": "EPI 525 Syllabus",
    "section": "Institutional Policies and Resources",
    "text": "Institutional Policies and Resources\n\nStatement Regarding Students with Disabilities\nOHSU is committed to inclusive and accessible learning environments in compliance with federal and state law. If you have a disability or think you may have a disability (mental health, attention-related, learning, vision, hearing, physical or health impacts) contact the Office for Student Access at (503) 494-0082 or OHSU Student Access to have a confidential conversation about academic accommodations. Information is also available at Student Access Website. Because accommodations may take time to implement and cannot be applied retroactively, it is important to have this discussion as soon as possible.\nPortland State students also have similar resources available via the PSU Disability Resource Center (website http://www.pdx.edu/drc ). Please contact the DRC at tel. (503) 725-4150 or email at drc@pdx.edu\n\n\nStudent Evaluation of Courses\nCourse evaluation results are extremely important and used to help improve courses and the learning experience of future students. Responses will always remain anonymous and will only be available to instructors after grades have been posted. The results of scaled questions and comments go to both the instructor and their unit head/supervisor. Refer to Student Evaluation of Courses and Instructional Effectiveness, *Policy No. 02-50-035.\n*To access the OHSU Student Evaluation of Courses and Instructional Effectiveness Policy, you must log into the OHSU O2 website.\n\n\nCopyright Information\nCopyright laws and fair use policies protect the rights of those who have produced the material. The copy in this course has been provided for private study, scholarship, or research. Other uses may require permission from the copyright holder. The user of this work is responsible for adhering to copyright law of the U.S. (Title 17, U.S. Code). To help you familiarize yourself with copyright and fair use policies, the University encourages you to visit its Copyright Web Page\nSakai course web sites contain material protected by copyrights held by the instructor, other individuals or institutions. Such material is used for educational purposes in accord with copyright law and/or with permission given by the owners of the original material. You may download one copy of the materials on any single computer for non-commercial, personal, or educational purposes only, provided that you (1) do not modify it, (2) use it only for the duration of this course, and (3) include both this notice and any copyright notice originally included with the material. Beyond this use, no material from the course web site may be copied, reproduced, re-published, uploaded, posted, transmitted, or distributed in any way without the permission of the original copyright holder. The instructor assumes no responsibility for individuals who improperly use copyrighted material placed on the web site.\n\n\nSyllabi Changes and Retention\nSyllabi are considered to be a learning agreement between students and the faculty of record. Information contained in syllabi, other than the minimum requirements, may be subject to change as deemed appropriate by the faculty of record in concurrence with the academic program and the Office of the Provost. Refer to the *Course Syllabi Policy, 02-50-050.\n*To access the OHSU Course Syllabus Policy, you must log into the OHSU O2 website.\n\n\nCommitment to Diversity & Inclusion\nOHSU is committed to creating and fostering a learning and working environment based on open communication and mutual respect. If you encounter sexual harassment, sexual misconduct, sexual assault, or discrimination based on race, color, religion, age, national origin, veteran’s status, ancestry, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity, disability or any other protected status please contact the Affirmative Action and Equal Opportunity Department at 503-494-5148 or aaeo@ohsu.edu. Inquiries about Title IX compliance or sex/gender discrimination and harassment may be directed to the OHSU Title IX Coordinator at 503-494-0258 or titleix@ohsu.edu.\n\n\nModified Operations, Policy 01-40-010\nPortland Campus:  Marquam Hill and South Waterfront\nStudents should review O2 or call OHSU’s weather alert line at 503-494-9021 for the most up-to-date information on OHSU-wide modified operations which include but are not limited to delays or closures for inclement weather.\nIf your home institution is not on the Portland campus (Marquam Hill or South Waterfront, contact your home institution for more information.\n\n\nOHSU Resources Available to Students*:\nRemote Learning Resources\nThe Remote Learning webpage on O2 contains concise, practical resources, and strategies for students that need to quickly transition to a fully remote instructional format.\nRegistrar’s Office\nMackenzie Hall, Rm. 1120\n503-494-7800; Email the Registrar\nStudent Registration Information: \nTo Register for Classes\nOHSU ITG Help Desk\nRegular staff hours are 6 a.m. to 6 p.m., Monday through Friday, but phones are answered seven days a week, 24 hours a day. Call 503 494-2222.\nTeaching and Learning Center\nAcademic Support Counseling and Sakai Course Management System, please contact the TLC Help Desk at 877-972-5249 or email TLC Help Desk\nStudent Academic Support Services\nFor resources on improving student’s study strategies, time management, motivation, test-taking skills and more, Please access the Student Academic Support Services Sakai page. For one-on-one appointments or to arrange a workshop for students, please contact Emily Hillhouse.\nConfidential Advocacy Program\nSupport for OHSU employees, students, and volunteers who have experienced any form of sexual misconduct, including sexual harassment, sexual assault, intimate-partner violence, stalking, relationship/dating violence, and other forms — regardless of when or where it took place. Contact Us.\nConcourse Syllabus Management\nFor help with accessing your Concourse Syllabus:  Please contact the Sakai help Desk for all other Concourse inquiries please visit the Concourse Support - Sakai or please contact the Mark Rivera at rivermar@ohsu.edu or call 503-494-0934\nPublic Safety\nOHSU Public Safety-Portland Campus (Marquam Hill and South Waterfront)\n\nEmergency on Campus: 503-494-4444 (Portland)\nNon-emergency: 503-494-7744; Contact Public Safety\n\nStudent Health & Wellness Center \nBaird Hall, Rm. 18 (Primary Care) and Rm. 6 (Behavioral Health)\n503-494-8665; For urgent care after hours, 503-494-8311 and ask for the Nurse on call.\nWellness Center Information  \nWellness Center Website\nIf your home institution is not on the Portland campus, contact your home institution student support services for more information.\nOmbudsman Office\nGaines Hall, Rm. 117\n707 SW Gaines Street, Portland, OR 97239\n503-494-5397; Contact Ombudsman; Ombudsman Website\nLibrary: Biomedical Information Communication Center\nBICC Library Hours of Operation\n\n\nPrivacy While Learning\nStudents may be asked to take classes remotely through videoconferencing software like WebEx. Some of these remote classes will be recorded. Any recording will capture the presenter’s audio, video, and computer screen. Student video and audio will be recorded if and when you unmute your audio and share your video during the recorded sessions. These recordings will not be shared with or accessible to the public without prior written consent. \n\n\nStudent Central\nKey information for students across OHSU’s Schools of Dentistry, Medicine, Nursing, the OHSU-PSU School of Public Health and the College of Pharmacy. Student Central helps you find out more about student services, resources, policies and technology."
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#goals-for-today",
    "href": "lessons/08_Variability/08_Variability.html#goals-for-today",
    "title": "Lesson 8: Variability in estimates",
    "section": "Goals for today",
    "text": "Goals for today\nSection 4.1\n\nSampling from a population\n\npopulation parameters vs. point estimates\nsampling variation\n\nSampling distribution of the mean\n\nCentral Limit Theorem\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#moritzs-tip-of-the-day-add-a-code-pane-in-rstudio",
    "href": "lessons/08_Variability/08_Variability.html#moritzs-tip-of-the-day-add-a-code-pane-in-rstudio",
    "title": "Lesson 8: Variability in estimates",
    "section": "MoRitz’s tip of the day: add a code pane in RStudio",
    "text": "MoRitz’s tip of the day: add a code pane in RStudio\nDo you want to be able to view two code files side-by-side?\nYou can do that by adding a column to the RStudio layout.\n\n\n\n\n\nSee https://posit.co/blog/rstudio-1-4-preview-multiple-source-columns/ for more information."
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#population-vs.-sample-from-section-1.3",
    "href": "lessons/08_Variability/08_Variability.html#population-vs.-sample-from-section-1.3",
    "title": "Lesson 8: Variability in estimates",
    "section": "Population vs. sample (from section 1.3)",
    "text": "Population vs. sample (from section 1.3)\n\n\n(Target) Population\n\ngroup of interest being studied\ngroup from which the sample is selected\n\nstudies often have inclusion and/or exclusion criteria\n\n\nSample\n\ngroup on which data are collected\noften a small subset of the population\n\n\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nrandomly sampled\nconsidered best way to sample"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#population-parameters-vs.-sample-statistics",
    "href": "lessons/08_Variability/08_Variability.html#population-parameters-vs.-sample-statistics",
    "title": "Lesson 8: Variability in estimates",
    "section": "Population parameters vs. sample statistics",
    "text": "Population parameters vs. sample statistics\n\n\nPopulation parameter\n\nSample statistic (point estimate)"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#our-hypothetical-population-yrbss",
    "href": "lessons/08_Variability/08_Variability.html#our-hypothetical-population-yrbss",
    "title": "Lesson 8: Variability in estimates",
    "section": "Our hypothetical population: YRBSS",
    "text": "Our hypothetical population: YRBSS\nYouth Risk Behavior Surveillance System (YRBSS)\n\nYearly survey conducted by the US Centers for Disease Control (CDC)\n“A set of surveys that track behaviors that can lead to poor health in students grades 9 through 12.”1\nDataset yrbss from oibiostat pacakge contains responses from n = 13,583 participants in 2013 for a subset of the variables included in the complete survey data\n\n\n\n\nlibrary(oibiostat)\ndata(\"yrbss\")  #load the data\n# ?yrbss\n\n\n\ndim(yrbss)\n\n[1] 13583    13\n\n\n\n\n\nnames(yrbss)\n\n [1] \"age\"                      \"gender\"                  \n [3] \"grade\"                    \"hispanic\"                \n [5] \"race\"                     \"height\"                  \n [7] \"weight\"                   \"helmet.12m\"              \n [9] \"text.while.driving.30d\"   \"physically.active.7d\"    \n[11] \"hours.tv.per.school.day\"  \"strength.training.7d\"    \n[13] \"school.night.hours.sleep\""
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#getting-to-know-the-dataset-glimpse",
    "href": "lessons/08_Variability/08_Variability.html#getting-to-know-the-dataset-glimpse",
    "title": "Lesson 8: Variability in estimates",
    "section": "Getting to know the dataset: glimpse()",
    "text": "Getting to know the dataset: glimpse()\n\nglimpse(yrbss)  # from tidyverse package (dplyr)\n\nRows: 13,583\nColumns: 13\n$ age                      &lt;int&gt; 14, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 1…\n$ gender                   &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"fema…\n$ grade                    &lt;chr&gt; \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", …\n$ hispanic                 &lt;chr&gt; \"not\", \"not\", \"hispanic\", \"not\", \"not\", \"not\"…\n$ race                     &lt;chr&gt; \"Black or African American\", \"Black or Africa…\n$ height                   &lt;dbl&gt; NA, NA, 1.73, 1.60, 1.50, 1.57, 1.65, 1.88, 1…\n$ weight                   &lt;dbl&gt; NA, NA, 84.37, 55.79, 46.72, 67.13, 131.54, 7…\n$ helmet.12m               &lt;chr&gt; \"never\", \"never\", \"never\", \"never\", \"did not …\n$ text.while.driving.30d   &lt;chr&gt; \"0\", NA, \"30\", \"0\", \"did not drive\", \"did not…\n$ physically.active.7d     &lt;int&gt; 4, 2, 7, 0, 2, 1, 4, 4, 5, 0, 0, 0, 4, 7, 7, …\n$ hours.tv.per.school.day  &lt;chr&gt; \"5+\", \"5+\", \"5+\", \"2\", \"3\", \"5+\", \"5+\", \"5+\",…\n$ strength.training.7d     &lt;int&gt; 0, 0, 0, 0, 1, 0, 2, 0, 3, 0, 3, 0, 0, 7, 7, …\n$ school.night.hours.sleep &lt;chr&gt; \"8\", \"6\", \"&lt;5\", \"6\", \"9\", \"8\", \"9\", \"6\", \"&lt;5\"…"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#height-weight-variables",
    "href": "lessons/08_Variability/08_Variability.html#height-weight-variables",
    "title": "Lesson 8: Variability in estimates",
    "section": "Height & weight variables",
    "text": "Height & weight variables\n\n\n\nyrbss %&gt;% \n  select(height, weight) %&gt;% \n  summary()\n\n     height          weight      \n Min.   :1.270   Min.   : 29.94  \n 1st Qu.:1.600   1st Qu.: 56.25  \n Median :1.680   Median : 64.41  \n Mean   :1.691   Mean   : 67.91  \n 3rd Qu.:1.780   3rd Qu.: 76.20  \n Max.   :2.110   Max.   :180.99  \n NA's   :1004    NA's   :1004    \n\n\n\n\nggplot(data = yrbss, \n       aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1004 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#transform-height-weight-from-metric-to-to-standard",
    "href": "lessons/08_Variability/08_Variability.html#transform-height-weight-from-metric-to-to-standard",
    "title": "Lesson 8: Variability in estimates",
    "section": "Transform height & weight from metric to to standard",
    "text": "Transform height & weight from metric to to standard\nAlso, drop missing values and add a column of id values\n\nyrbss2 &lt;- yrbss %&gt;%                 # save new dataset with new name\n  mutate(                           # add variables for \n    height.ft = 3.28084*height,     #     height in feet\n    weight.lb = 2.20462*weight      #     weight in pounds\n  ) %&gt;% \n  drop_na(height.ft, weight.lb) %&gt;% # drop rows w/ missing height/weight values\n  mutate(id = 1:nrow(.)) %&gt;%        # add id column\n  select(id, height.ft, weight.lb)  # restrict dataset to columns of interest\n\nhead(yrbss2)  \n\n  id height.ft weight.lb\n1  1  5.675853  186.0038\n2  2  5.249344  122.9957\n3  3  4.921260  102.9998\n4  4  5.150919  147.9961\n5  5  5.413386  289.9957\n6  6  6.167979  157.0130\n\ndim(yrbss2)\n\n[1] 12579     3\n\n# number of rows deleted that had missing values for height and/or weight:\nnrow(yrbss) - nrow(yrbss2) \n\n[1] 1004"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#yrbss2-summary",
    "href": "lessons/08_Variability/08_Variability.html#yrbss2-summary",
    "title": "Lesson 8: Variability in estimates",
    "section": "yrbss2 summary",
    "text": "yrbss2 summary\n\nsummary(yrbss2)\n\n       id          height.ft       weight.lb     \n Min.   :    1   Min.   :4.167   Min.   : 66.01  \n 1st Qu.: 3146   1st Qu.:5.249   1st Qu.:124.01  \n Median : 6290   Median :5.512   Median :142.00  \n Mean   : 6290   Mean   :5.549   Mean   :149.71  \n 3rd Qu.: 9434   3rd Qu.:5.840   3rd Qu.:167.99  \n Max.   :12579   Max.   :6.923   Max.   :399.01  \n\n\nAnother summary:\n\nyrbss2 %&gt;% \n  get_summary_stats(type = \"mean_sd\") %&gt;% \n  kable()\n\n\n\n\nvariable\nn\nmean\nsd\n\n\n\n\nid\n12579\n6290.000\n3631.389\n\n\nheight.ft\n12579\n5.549\n0.343\n\n\nweight.lb\n12579\n149.708\n37.254"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#random-sample-of-size-n-5-from-yrbss2",
    "href": "lessons/08_Variability/08_Variability.html#random-sample-of-size-n-5-from-yrbss2",
    "title": "Lesson 8: Variability in estimates",
    "section": "Random sample of size n = 5 from yrbss2",
    "text": "Random sample of size n = 5 from yrbss2\n\n\nTake a random sample of size n = 5 from yrbss2:\n\nlibrary(moderndive)\nsamp_n5_rep1 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 1,\n               replace = FALSE)\nsamp_n5_rep1\n\n# A tibble: 5 × 4\n# Groups:   replicate [1]\n  replicate    id height.ft weight.lb\n      &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1         1  5869      5.15      145.\n2         1  6694      5.41      127.\n3         1  2517      5.74      130.\n4         1  5372      6.07      180.\n5         1  5403      6.07      163.\n\n\n\nCalculate the mean of the random sample:\n\nmeans_hght_samp_n5_rep1 &lt;- \n  samp_n5_rep1 %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep1\n\n# A tibble: 1 × 2\n  replicate mean_height\n      &lt;int&gt;       &lt;dbl&gt;\n1         1        5.69\n\n\n\n\nWould we get the same mean height if we took another sample?"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#sampling-variation",
    "href": "lessons/08_Variability/08_Variability.html#sampling-variation",
    "title": "Lesson 8: Variability in estimates",
    "section": "Sampling variation",
    "text": "Sampling variation\n\nIf a different random sample is taken, the mean height (point estimate) will likely be different\n\nthis is a result of sampling variation\n\n\n\n\nTake a 2nd random sample of size\nn = 5 from yrbss2:\n\nsamp_n5_rep1 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 1,\n               replace = FALSE)\nsamp_n5_rep1\n\n# A tibble: 5 × 4\n# Groups:   replicate [1]\n  replicate    id height.ft weight.lb\n      &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1         1  2329      6.07      182.\n2         1  8863      5.25      125.\n3         1  8058      5.84      135.\n4         1   335      6.17      235.\n5         1  4698      5.58      124.\n\n\n\nCalculate the mean of the 2nd random sample:\n\nmeans_hght_samp_n5_rep1 &lt;- \n  samp_n5_rep1 %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep1\n\n# A tibble: 1 × 2\n  replicate mean_height\n      &lt;int&gt;       &lt;dbl&gt;\n1         1        5.78\n\n\n\n\nDid we get the same mean height with our 2nd sample?"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#random-samples-of-size-n-5-from-yrbss2",
    "href": "lessons/08_Variability/08_Variability.html#random-samples-of-size-n-5-from-yrbss2",
    "title": "Lesson 8: Variability in estimates",
    "section": "100 random samples of size n = 5 from yrbss2",
    "text": "100 random samples of size n = 5 from yrbss2\n\n\nTake 100 random samples of size\nn = 5 from yrbss2:\n\nsamp_n5_rep100 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 100,\n               replace = FALSE)\nsamp_n5_rep100\n\n# A tibble: 500 × 4\n# Groups:   replicate [100]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  6483      5.51     145. \n 2         1  9899      4.92      90.0\n 3         1  6103      5.68     118. \n 4         1  2702      5.68     150. \n 5         1 11789      5.35     115. \n 6         2 10164      5.51     140. \n 7         2  5807      5.41     215. \n 8         2  9382      5.15      98.0\n 9         2  4904      6.00     196. \n10         2   229      6.07     101. \n# ℹ 490 more rows\n\n\n\nCalculate the mean for each of the 100 random samples:\n\nmeans_hght_samp_n5_rep100 &lt;- \n  samp_n5_rep100 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep100\n\n# A tibble: 100 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.43\n 2         2        5.63\n 3         3        5.34\n 4         4        5.70\n 5         5        5.90\n 6         6        5.37\n 7         7        5.49\n 8         8        5.60\n 9         9        5.50\n10        10        5.68\n# ℹ 90 more rows\n\n\n\n\nHow close are the mean heights for each of the 100 random samples?"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#distribution-of-100-sample-mean-heights-n-5",
    "href": "lessons/08_Variability/08_Variability.html#distribution-of-100-sample-mean-heights-n-5",
    "title": "Lesson 8: Variability in estimates",
    "section": "Distribution of 100 sample mean heights (n = 5)",
    "text": "Distribution of 100 sample mean heights (n = 5)\n\n\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n5_rep100, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCalculate the mean and SD of the 100 mean heights from the 100 samples:\n\nstats_means_hght_samp_n5_rep100 &lt;- \n  means_hght_samp_n5_rep100 %&gt;% \n  summarise(\n   mean_mean_height = mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n5_rep100\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.58          0.150\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#random-samples-of-size-n-5-from-yrbss2-1",
    "href": "lessons/08_Variability/08_Variability.html#random-samples-of-size-n-5-from-yrbss2-1",
    "title": "Lesson 8: Variability in estimates",
    "section": "10,000 random samples of size n = 5 from yrbss2",
    "text": "10,000 random samples of size n = 5 from yrbss2\n\n\nTake 10,000 random samples of size\nn = 5 from yrbss2:\n\nsamp_n5_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 10000,\n               replace = FALSE)\nsamp_n5_rep10000\n\n# A tibble: 50,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  6383      5.35      126.\n 2         1  4019      5.41      107.\n 3         1  4856      5.25      135.\n 4         1  9988      5.58      120.\n 5         1  2245      6.17      270.\n 6         2 10580      5.68      155.\n 7         2  2254      5.84      159.\n 8         2  8081      5.09      110.\n 9         2 10194      5.35      115.\n10         2  7689      5.35      135.\n# ℹ 49,990 more rows\n\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n5_rep10000 &lt;- \n  samp_n5_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.55\n 2         2        5.46\n 3         3        5.49\n 4         4        5.60\n 5         5        5.47\n 6         6        5.83\n 7         7        5.68\n 8         8        5.47\n 9         9        5.37\n10        10        5.15\n# ℹ 9,990 more rows\n\n\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#distribution-of-10000-sample-mean-heights-n-5",
    "href": "lessons/08_Variability/08_Variability.html#distribution-of-10000-sample-mean-heights-n-5",
    "title": "Lesson 8: Variability in estimates",
    "section": "Distribution of 10,000 sample mean heights (n = 5)",
    "text": "Distribution of 10,000 sample mean heights (n = 5)\n\n\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n5_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCalculate the mean and SD of the 10,000 mean heights from the 10,000 samples:\n\nstats_means_hght_samp_n5_rep10000 &lt;- \n  means_hght_samp_n5_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n5_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55          0.153\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#samples-of-size-n-30-from-yrbss2",
    "href": "lessons/08_Variability/08_Variability.html#samples-of-size-n-30-from-yrbss2",
    "title": "Lesson 8: Variability in estimates",
    "section": "10,000 samples of size n = 30 from yrbss2",
    "text": "10,000 samples of size n = 30 from yrbss2\n\n\nTake 10,000 random samples of size\nn = 30 from yrbss2:\n\nsamp_n30_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 30, \n               reps = 10000,\n               replace = FALSE)\nsamp_n30_rep10000\n\n# A tibble: 300,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  3871      5.25      115.\n 2         1 12090      5.15      125.\n 3         1   241      5.58      119.\n 4         1  4570      5.58      140.\n 5         1  4131      5.35      143.\n 6         1 11513      5.35      135.\n 7         1  9663      5.25      125.\n 8         1  3789      5.25      160.\n 9         1   442      5.15      130.\n10         1 11528      5.51      200.\n# ℹ 299,990 more rows\n\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n30_rep10000 &lt;- \n  samp_n30_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(mean_height = \n            mean(height.ft))\n\nmeans_hght_samp_n30_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.48\n 2         2        5.63\n 3         3        5.46\n 4         4        5.46\n 5         5        5.51\n 6         6        5.54\n 7         7        5.56\n 8         8        5.51\n 9         9        5.51\n10        10        5.50\n# ℹ 9,990 more rows\n\n\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#distribution-of-10000-sample-mean-heights-n-30",
    "href": "lessons/08_Variability/08_Variability.html#distribution-of-10000-sample-mean-heights-n-30",
    "title": "Lesson 8: Variability in estimates",
    "section": "Distribution of 10,000 sample mean heights (n = 30)",
    "text": "Distribution of 10,000 sample mean heights (n = 30)\n\n\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCalculate the mean and SD of the 10,000 mean heights from the 10,000 samples:\n\nstats_means_hght_samp_n30_rep10000&lt;- \n  means_hght_samp_n30_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n30_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#compare-distributions-of-10000-sample-mean-heights-when-n-5-left-vs.-n-30-right",
    "href": "lessons/08_Variability/08_Variability.html#compare-distributions-of-10000-sample-mean-heights-when-n-5-left-vs.-n-30-right",
    "title": "Lesson 8: Variability in estimates",
    "section": "Compare distributions of 10,000 sample mean heights when n = 5 (left) vs. n = 30 (right)",
    "text": "Compare distributions of 10,000 sample mean heights when n = 5 (left) vs. n = 30 (right)\nHow are the center, shape, and spread similar and/or different?\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55          0.153\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#sampling-high-schoolers-weights",
    "href": "lessons/08_Variability/08_Variability.html#sampling-high-schoolers-weights",
    "title": "Lesson 8: Variability in estimates",
    "section": "Sampling high schoolers’ weights",
    "text": "Sampling high schoolers’ weights\n\n\nWhich figure is which?\n\nPopulation distribution of weights\nSampling distribution of mean weights when \\(n=5\\)\nSampling distribution of mean weights when \\(n=30\\).\n\n\n\n\n\nA\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nB\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nC\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#the-sampling-distribution-of-the-mean",
    "href": "lessons/08_Variability/08_Variability.html#the-sampling-distribution-of-the-mean",
    "title": "Lesson 8: Variability in estimates",
    "section": "The sampling distribution of the mean",
    "text": "The sampling distribution of the mean\n\n\n\nThe sampling distribution of the mean is the distribution of sample means calculated from repeated random samples of the same size from the same population\nOur simulations show approximations of the sampling distribution of the mean for various sample sizes\nThe theoretical sampling distribution is based on all possible samples of a given sample size \\(n\\).\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`)."
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#the-central-limit-theorem-clt",
    "href": "lessons/08_Variability/08_Variability.html#the-central-limit-theorem-clt",
    "title": "Lesson 8: Variability in estimates",
    "section": "The Central Limit Theorem (CLT)",
    "text": "The Central Limit Theorem (CLT)\n\nFor “large” sample sizes ( \\(n\\geq 30\\) ),\n\nthe sampling distribution of the sample mean\ncan be approximated by a normal distribution,with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\n  \n\nFor small sample sizes, if the population is known to be normally distributed, then\n\nthe sampling distribution of the sample mean\nis a normal distribution, with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#the-cutest-statistics-video-on-youtube",
    "href": "lessons/08_Variability/08_Variability.html#the-cutest-statistics-video-on-youtube",
    "title": "Lesson 8: Variability in estimates",
    "section": "The cutest statistics video on YouTube",
    "text": "The cutest statistics video on YouTube\n\nBunnies, Dragons and the ‘Normal’ World: Central Limit Theorem\n\nCreature Cast from the New York Times\nhttps://www.youtube.com/watch?v=jvoxEYmQHNM&feature=youtu.be"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#sampling-distribution-of-mean-heights-when-n-30-12",
    "href": "lessons/08_Variability/08_Variability.html#sampling-distribution-of-mean-heights-when-n-30-12",
    "title": "Lesson 8: Variability in estimates",
    "section": "Sampling distribution of mean heights when n = 30 (1/2)",
    "text": "Sampling distribution of mean heights when n = 30 (1/2)\n\n\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCLT tells us that we can model the sampling distribution of mean heights using a normal distribution."
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#sampling-distribution-of-mean-heights-when-n-30-22",
    "href": "lessons/08_Variability/08_Variability.html#sampling-distribution-of-mean-heights-when-n-30-22",
    "title": "Lesson 8: Variability in estimates",
    "section": "Sampling distribution of mean heights when n = 30 (2/2)",
    "text": "Sampling distribution of mean heights when n = 30 (2/2)\n\n\nMean and SD of population:\n\n(mean_height.ft &lt;- mean(yrbss2$height.ft))\n\n[1] 5.548691\n\n(sd_height.ft &lt;- sd(yrbss2$height.ft))\n\n[1] 0.3434949\n\nsd_height.ft/sqrt(30)\n\n[1] 0.06271331\n\n\nMean and SD of simulated sampling distribution:\n\nstats_means_hght_samp_n30_rep10000&lt;- \n  means_hght_samp_n30_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n30_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#why-is-the-mean-mu-the-standard-error-fracsigmasqrtn",
    "href": "lessons/08_Variability/08_Variability.html#why-is-the-mean-mu-the-standard-error-fracsigmasqrtn",
    "title": "Lesson 8: Variability in estimates",
    "section": "Why is the mean \\(\\mu\\) & the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) ?",
    "text": "Why is the mean \\(\\mu\\) & the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) ?"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#applying-the-clt",
    "href": "lessons/08_Variability/08_Variability.html#applying-the-clt",
    "title": "Lesson 8: Variability in estimates",
    "section": "Applying the CLT",
    "text": "Applying the CLT\nWhat is the probability that for a random sample of 30 high schoolers, that their mean height is greater than 5.6 ft?"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#class-discussion",
    "href": "lessons/08_Variability/08_Variability.html#class-discussion",
    "title": "Lesson 8: Variability in estimates",
    "section": "Class Discussion",
    "text": "Class Discussion\n\nSlide 21: match figures to distribution (Sampling high schoolers’ weights)\n\nProblems from Homework 4:\n\nR1: Youth weights (YRBSS)\nBook exercise: 4.2\nNon-book exercise: Ethan Allen"
  },
  {
    "objectID": "lessons/08_Variability/08_Variability.html#footnotes",
    "href": "lessons/08_Variability/08_Variability.html#footnotes",
    "title": "Lesson 8: Variability in estimates",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYouth Risk Behavior Surveillance System https://www.cdc.gov/healthyyouth/data/yrbs/index.htm (YRBSS)↩︎"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "",
    "text": "Sampling from a population\n\npopulation parameters vs. point estimates\nsampling variation\n\n\n\n\nSampling distribution of a mean\nCentral Limit Theorem\n\n\n\n\n\n\n\nWhat are Confidence Intervals?\n\nHow to calculate CI’s?\nHow to interpret & NOT interpret CI’s?\n\n\n\nWhat if we don’t know \\(\\sigma\\)?\nStudent’s t-distribution"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#last-time---goals-for-today",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#last-time---goals-for-today",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "",
    "text": "Sampling from a population\n\npopulation parameters vs. point estimates\nsampling variation\n\n\n\n\nSampling distribution of a mean\nCentral Limit Theorem\n\n\n\n\n\n\n\nWhat are Confidence Intervals?\n\nHow to calculate CI’s?\nHow to interpret & NOT interpret CI’s?\n\n\n\nWhat if we don’t know \\(\\sigma\\)?\nStudent’s t-distribution"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#where-are-we",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#where-are-we",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#our-hypothetical-population-yrbss",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#our-hypothetical-population-yrbss",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Our hypothetical population: YRBSS",
    "text": "Our hypothetical population: YRBSS\nYouth Risk Behavior Surveillance System (YRBSS)\n\nYearly survey conducted by the US Centers for Disease Control (CDC)\n“A set of surveys that track behaviors that can lead to poor health in students grades 9 through 12.”1\nDataset yrbss from oibiostat pacakge contains responses from n = 13,583 participants in 2013 for a subset of the variables included in the complete survey data\n\n\n\n\nlibrary(oibiostat)\ndata(\"yrbss\")  #load the data\n# ?yrbss\n\n\n\ndim(yrbss)\n\n[1] 13583    13\n\n\n\n\n\nnames(yrbss)\n\n [1] \"age\"                      \"gender\"                  \n [3] \"grade\"                    \"hispanic\"                \n [5] \"race\"                     \"height\"                  \n [7] \"weight\"                   \"helmet.12m\"              \n [9] \"text.while.driving.30d\"   \"physically.active.7d\"    \n[11] \"hours.tv.per.school.day\"  \"strength.training.7d\"    \n[13] \"school.night.hours.sleep\""
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#transform-height-weight-from-metric-to-to-standard",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#transform-height-weight-from-metric-to-to-standard",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Transform height & weight from metric to to standard",
    "text": "Transform height & weight from metric to to standard\nAlso, drop missing values and add a column of id values\n\nyrbss2 &lt;- yrbss %&gt;%                 # save new dataset with new name\n  mutate(                           # add variables for \n    height.ft = 3.28084*height,     #     height in feet\n    weight.lb = 2.20462*weight      #     weight in pounds\n  ) %&gt;% \n  drop_na(height.ft, weight.lb) %&gt;% # drop rows w/ missing height/weight values\n  mutate(id = 1:nrow(.)) %&gt;%        # add id column\n  select(id, height.ft, weight.lb)  # restrict dataset to columns of interest\n\nhead(yrbss2)  \n\n  id height.ft weight.lb\n1  1  5.675853  186.0038\n2  2  5.249344  122.9957\n3  3  4.921260  102.9998\n4  4  5.150919  147.9961\n5  5  5.413386  289.9957\n6  6  6.167979  157.0130\n\ndim(yrbss2)\n\n[1] 12579     3\n\n# number of rows deleted that had missing values for height and/or weight:\nnrow(yrbss) - nrow(yrbss2) \n\n[1] 1004"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#yrbss2-stats-for-height-in-feet",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#yrbss2-stats-for-height-in-feet",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "yrbss2: stats for height in feet",
    "text": "yrbss2: stats for height in feet\n\nsummary(yrbss2)\n\n       id          height.ft       weight.lb     \n Min.   :    1   Min.   :4.167   Min.   : 66.01  \n 1st Qu.: 3146   1st Qu.:5.249   1st Qu.:124.01  \n Median : 6290   Median :5.512   Median :142.00  \n Mean   : 6290   Mean   :5.549   Mean   :149.71  \n 3rd Qu.: 9434   3rd Qu.:5.840   3rd Qu.:167.99  \n Max.   :12579   Max.   :6.923   Max.   :399.01  \n\n(mean_height.ft &lt;- mean(yrbss2$height.ft))\n\n[1] 5.548691\n\n(sd_height.ft &lt;- sd(yrbss2$height.ft))\n\n[1] 0.3434949"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#samples-of-size-n-30-from-yrbss2",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#samples-of-size-n-30-from-yrbss2",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "10,000 samples of size n = 30 from yrbss2",
    "text": "10,000 samples of size n = 30 from yrbss2\n\n\nTake 10,000 random samples of size\nn = 30 from yrbss2:\n\nsamp_n30_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 30, \n               reps = 10000,\n               replace = FALSE)\nsamp_n30_rep10000\n\n# A tibble: 300,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  5869      5.15      145.\n 2         1  6694      5.41      127.\n 3         1  2517      5.74      130.\n 4         1  5372      6.07      180.\n 5         1  5403      6.07      163.\n 6         1  2329      6.07      182.\n 7         1  8863      5.25      125.\n 8         1  8058      5.84      135.\n 9         1   335      6.17      235.\n10         1  4698      5.58      124.\n# ℹ 299,990 more rows\n\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n30_rep10000 &lt;- \n  samp_n30_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(mean_height = \n            mean(height.ft))\n\nmeans_hght_samp_n30_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.59\n 2         2        5.59\n 3         3        5.51\n 4         4        5.65\n 5         5        5.64\n 6         6        5.57\n 7         7        5.61\n 8         8        5.60\n 9         9        5.52\n10        10        5.64\n# ℹ 9,990 more rows\n\n\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#simulated-sampling-distribution-for-n-30-using-10000-sample-mean-heights",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#simulated-sampling-distribution-for-n-30-using-10000-sample-mean-heights",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Simulated sampling distribution for n = 30  using 10,000 sample mean heights",
    "text": "Simulated sampling distribution for n = 30  using 10,000 sample mean heights\n\n\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram() +\n  labs(title = \"Simulated \\n sampling distribution\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCLT tells us that we can model the sampling distribution of mean heights using a normal distribution."
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#given-barx-what-are-plausible-values-of-mu",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#given-barx-what-are-plausible-values-of-mu",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Given \\(\\bar{x}\\), what are plausible values of \\(\\mu\\)?",
    "text": "Given \\(\\bar{x}\\), what are plausible values of \\(\\mu\\)?"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#confidence-interval-c-i-for-the-mean-mu",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#confidence-interval-c-i-for-the-mean-mu",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Confidence interval (C I) for the mean \\(\\mu\\)",
    "text": "Confidence interval (C I) for the mean \\(\\mu\\)\n\n\n\\[\\overline{x}\\ \\pm\\ z^*\\times \\text{SE}\\]\nwhere\n\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(z^*\\) depends on the confidence level\n\nFor a 95% CI, \\(z^*\\) is chosen such that 95% of the standard normal curve is between \\(-z^*\\) and \\(z^*\\)\n\n\n\n\nqnorm(.975)\n\n[1] 1.959964\n\nqnorm(.995)\n\n[1] 2.575829\n\n\n\n\nWhen can this be applied?"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#example-c-i-for-mean-height",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#example-c-i-for-mean-height",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Example: C I for mean height",
    "text": "Example: C I for mean height\n\nA random sample of 30 high schoolers has mean height 5.6 ft.\nFind the 95% confidence interval for the population mean, assuming that the population standard deviation is 0.34 ft."
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#how-to-interpret-a-c-i-12",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#how-to-interpret-a-c-i-12",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "How to interpret a C I? (1/2)",
    "text": "How to interpret a C I? (1/2)\n\n\nSimulating Confidence Intervals:\n\nhttp://www.rossmanchance.com/applets/ConfSim.html\n\nThe figure shows CI’s from 100 simulations.\n\nThe true value of \\(\\mu\\) = 5.55 is the vertical black line.\nThe horizontal lines are 95% CI’s from 100 samples.\n\nGreen: the CI “captured” the true value of \\(\\mu\\)\nRed: the CI did not “capture” the true value of \\(\\mu\\)\n\n\n\n\nQuestion:\nWhat percent of CI’s captured the true value of \\(\\mu\\) ?"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#how-to-interpret-a-c-i-22",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#how-to-interpret-a-c-i-22",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "How to interpret a C I? (2/2)",
    "text": "How to interpret a C I? (2/2)\nActual interpretation:\n\nIf we were to\n\nrepeatedly take random samples from a population and\ncalculate a 95% CI for each random sample,\n\nthen we would expect 95% of our CI’s to contain the true population parameter \\(\\mu\\).\n\n\n\n\nWhat we typically write as “shorthand”:\n\nWe are 95% confident that (the 95% confidence interval) captures the value of the population parameter.\n\nWRONG interpretation:\n\nThere is a 95% chance that (the 95% confidence interval) captures the value of the population parameter.\n\nFor one CI on its own, it either does or doesn’t contain the population parameter with probability 0 or 1. We just don’t know which!"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#what-percent-c-i-was-being-simulated-in-this-figure",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#what-percent-c-i-was-being-simulated-in-this-figure",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What percent C I was being simulated in this figure?",
    "text": "What percent C I was being simulated in this figure?\n\n\n\n\n\n\n\n\n100 CI’s are shown in the figure."
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#interpretation-of-the-mean-heights-c-i",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#interpretation-of-the-mean-heights-c-i",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Interpretation of the mean heights C I",
    "text": "Interpretation of the mean heights C I\nCorrect interpretation:\n\nWe are 95% confident that the mean height for high schoolers is between 5.43 and 5.67 feet.\n\nWRONG:\n\nThere is a 95% chance that the mean height for high schoolers is between 5.43 and 5.67 feet."
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#what-if-we-dont-know-sigma-13",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#what-if-we-dont-know-sigma-13",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What if we don’t know \\(\\sigma\\) ? (1/3)",
    "text": "What if we don’t know \\(\\sigma\\) ? (1/3)\nSimulating Confidence Intervals: http://www.rossmanchance.com/applets/ConfSim.html\n\n\n\n\n\nThe normal distribution doesn’t have a 95% “coverage rate”\nwhen using \\(s\\) instead of \\(\\sigma\\)"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#what-if-we-dont-know-sigma-23",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#what-if-we-dont-know-sigma-23",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What if we don’t know \\(\\sigma\\) ? (2/3)",
    "text": "What if we don’t know \\(\\sigma\\) ? (2/3)\n\nIn real life, we don’t know what the population sd is ( \\(\\sigma\\) )\nIf we replace \\(\\sigma\\) with \\(s\\) in the SE formula, we add in additional variability to the SE! \\[\\frac{\\sigma}{\\sqrt{n}} ~~~~\\textrm{vs.} ~~~~ \\frac{s}{\\sqrt{n}}\\]\nThus when using \\(s\\) instead of \\(\\sigma\\) when calculating the SE, we need a different probability distribution with thicker tails than the normal distribution.\n\nIn practice this will mean using a different value than 1.96 when calculating the CI."
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#what-if-we-dont-know-sigma-33",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#what-if-we-dont-know-sigma-33",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What if we don’t know \\(\\sigma\\) ? (3/3)",
    "text": "What if we don’t know \\(\\sigma\\) ? (3/3)\n\n\nThe Student’s t-distribution:\n\nIs bell shaped and symmetric with mean = 0.\nIts tails are a thicker than that of a normal distribution\n\nThe “thickness” depends on its degrees of freedom: \\(df = n–1\\) , where n = sample size.\n\nAs the degrees of freedom (sample size) increase,\n\nthe tails are less thick, and\nthe t-distribution is more like a normal distribution\nin theory, with an infinite sample size the t-distribution is a normal distribution."
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#calculating-the-c-i-for-the-population-mean-using-s",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#calculating-the-c-i-for-the-population-mean-using-s",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Calculating the C I for the population mean using \\(s\\)",
    "text": "Calculating the C I for the population mean using \\(s\\)\nCI for \\(\\mu\\):\n\\[\\bar{x} \\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\]\nwhere \\(t^*\\) is determined by the t-distribution and dependent on the\ndf = \\(n-1\\) and the confidence level\n\n\n\nqt gives the quartiles for a t-distribution. Need to specify\n\nthe percent under the curve to the left of the quartile\nthe degrees of freedom = n-1\n\nNote in the R output to the right that \\(t^*\\) gets closer to 1.96 as the sample size increases.\n\n\n\nqt(.975, df=9)  # df = n-1\n\n[1] 2.262157\n\nqt(.975, df=49)\n\n[1] 2.009575\n\nqt(.975, df=99)\n\n[1] 1.984217\n\nqt(.975, df=999)\n\n[1] 1.962341"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#using-a-t-table-to-get-t",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#using-a-t-table-to-get-t",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Using a \\(t\\)-table to get \\(t^*\\)",
    "text": "Using a \\(t\\)-table to get \\(t^*\\)"
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#example-c-i-for-mean-height-revisited",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#example-c-i-for-mean-height-revisited",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Example: C I for mean height (revisited)",
    "text": "Example: C I for mean height (revisited)\n\nA random sample of 30 high schoolers has mean height 5.6 ft and standard deviation 0.34 ft.\nFind the 95% confidence interval for the population mean."
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#z-vs-t-important-comment-about-chapter-4-of-textbook",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#z-vs-t-important-comment-about-chapter-4-of-textbook",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "\\(z\\) vs \\(t\\)??  (& important comment about Chapter 4 of textbook)",
    "text": "\\(z\\) vs \\(t\\)??  (& important comment about Chapter 4 of textbook)\n\n\nTextbook’s rule of thumb\n\n\n(Ch 4) If \\(n \\geq 30\\) and population distribution not strongly skewed:\n\nUse normal distribution\nNo matter if using \\(\\sigma\\) or \\(s\\) for the \\(SE\\)\nIf there is skew or some large outliers, then need \\(n \\geq 50\\)\n\n(Ch 5) If \\(n &lt; 30\\) and data approximately symmetric with no large outliers:\n\nUse Student’s t-distribution\n\n\n\n\n\n\nBSTA 511 rule of thumb\n\nUse normal distribution ONLY if know \\(\\sigma\\)\n\nIf using \\(s\\) for the \\(SE\\), then use the Student’s t-distribution\n\n\nFor either case, can apply if either\n\n\\(n \\geq 30\\) and population distribution not strongly skewed\n\nIf there is skew or some large outliers, then \\(n \\geq 50\\) gives better estimates\n\n\\(n &lt; 30\\) and data approximately symmetric with no large outliers\n\nIf do not know population distribution, then check the distribution of the data."
  },
  {
    "objectID": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#footnotes",
    "href": "lessons/09_Confidence_intervals/09_Confidence_intervals.html#footnotes",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYouth Risk Behavior Surveillance System https://www.cdc.gov/healthyyouth/data/yrbs/index.htm (YRBSS)↩︎"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#goals-for-today-sections-6.3-6.4",
    "href": "lessons/16_SLR_02/16_SLR_02.html#goals-for-today-sections-6.3-6.4",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Goals for today (Sections 6.3-6.4)",
    "text": "Goals for today (Sections 6.3-6.4)\nSimple Linear Regression Part 2\n\nReview of\n\nbest-fit line (aka regression line or least-squares line)\nresiduals\npopulation model\n\nLINE conditions and how to assess them\n\nNew diagnostic tools:\n\nNormal QQ plots of residuals\nResidual plots\n\n\nCoefficient of determination (\\(R^2\\))\nRegression inference\n\nInference for population slope \\(\\beta_1\\)\n\nCI & hypothesis test\n\nCI for mean response \\(\\mu_{Y|x^*}\\)\nPrediction interval for predicting individual observations\n\n\nConfidence bands vs. predictions bands"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#life-expectancy-vs.-female-adult-literacy-rate",
    "href": "lessons/16_SLR_02/16_SLR_02.html#life-expectancy-vs.-female-adult-literacy-rate",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Life expectancy vs. female adult literacy rate",
    "text": "Life expectancy vs. female adult literacy rate\nhttps://www.gapminder.org/tools/#$model$markers$bubble$encoding$x$data$concept=literacy_rate_adult_female_percent_of_females_ages_15_above&source=sg&space@=country&=time;;&scale$domain:null&zoomed:null&type:null;;&frame$value=2011;;;;;&chart-type=bubbles&url=v1"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#dataset-description",
    "href": "lessons/16_SLR_02/16_SLR_02.html#dataset-description",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Dataset description",
    "text": "Dataset description\n\nData file: lifeexp_femlit_water_2011.csv\nData were downloaded from https://www.gapminder.org/data/\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same. Source: https://www.gapminder.org/data/documentation/gd004/\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life. Source: http://data.uis.unesco.org/\nAt least basic water source (%) = the percentage of people using at least basic water services. This indicator encompasses both people using basic water services as well as those using safely managed water services. Basic drinking water services is defined as drinking water from an improved source, provided collection time is not more than 30 minutes for a round trip. Improved water sources include piped water, boreholes or tubewells, protect dug wells, protected springs, and packaged or delivered water."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#get-to-know-the-data",
    "href": "lessons/16_SLR_02/16_SLR_02.html#get-to-know-the-data",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Get to know the data",
    "text": "Get to know the data\nLoad data\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\nRows: 194 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, water_2011_quart\ndbl (3): life_expectancy_years_2011, female_literacy_rate_2011, water_basic_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nGlimpse of the data\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\nNote the missing values for our variables of interest\n\ngapm_original %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…   187  47.5  82.9   72.7  64.3  76.9  12.6  9.04  70.7  8.44 0.617\n2 female_lit…    80  13    99.8   91.6  71.0  98.0  27.0 11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#remove-missing-values",
    "href": "lessons/16_SLR_02/16_SLR_02.html#remove-missing-values",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Remove missing values",
    "text": "Remove missing values\nRemove rows with missing data for life expectancy and female literacy rate\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\nNo missing values now for our variables of interest\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#regression-line-best-fit-line",
    "href": "lessons/16_SLR_02/16_SLR_02.html#regression-line-best-fit-line",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Regression line = best-fit line",
    "text": "Regression line = best-fit line\n\n\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\n\n\\(\\hat{y}\\) is the predicted outcome for a specific value of \\(x\\).\n\\(b_0\\) is the intercept\n\\(b_1\\) is the slope of the line, i.e., the increase in \\(\\hat{y}\\) for every increase of one (unit increase) in \\(x\\).\n\nslope = rise over run\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nIntercept\n\nThe expected outcome for the \\(y\\)-variable when the \\(x\\)-variable is 0.\n\nSlope\n\nFor every increase of 1 unit in the \\(x\\)-variable, there is an expected increase of, on average, \\(b_1\\) units in the \\(y\\)-variable.\nWe only say that there is an expected increase and not necessarily a causal increase."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#regression-in-r-lm-summary-tidy",
    "href": "lessons/16_SLR_02/16_SLR_02.html#regression-in-r-lm-summary-tidy",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Regression in R: lm(), summary(), & tidy()",
    "text": "Regression in R: lm(), summary(), & tidy()\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\nRegression equation for our model:\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#residuals",
    "href": "lessons/16_SLR_02/16_SLR_02.html#residuals",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Residuals",
    "text": "Residuals\n\n\n\nObserved values \\(y_i\\)\n\nthe values in the dataset\n\nFitted values \\(\\widehat{y}_i\\)\n\nthe values that fall on the best-fit line for a specific \\(x_i\\)\n\nResiduals \\(e_i = y_i - \\widehat{y}_i\\)\n\nthe differences between the observed and fitted values\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#the-population-regresison-model",
    "href": "lessons/16_SLR_02/16_SLR_02.html#the-population-regresison-model",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "The (population) regresison model",
    "text": "The (population) regresison model\n\n\n\nThe (population) regression model is denoted by\n\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable:\n\n\\(\\epsilon \\sim N(0, \\sigma^2)\\)\nvariance \\(\\sigma^2\\) is constant\n\n\n\n\n\n\n\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions\n\n\n\n\nThe line is the average (expected) value of \\(Y\\) given a value of \\(x\\): \\(E(Y|x)\\).\nThe point estimates for \\(\\beta_0\\) and \\(\\beta_1\\) based on a sample are denoted by \\(b_0, b_1, s_{residuals}^2\\)\n\nNote: also common notation is \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\sigma}^2\\)"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#what-are-the-line-conditions",
    "href": "lessons/16_SLR_02/16_SLR_02.html#what-are-the-line-conditions",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "What are the LINE conditions?",
    "text": "What are the LINE conditions?\nFor “good” model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.\nBriefly:\n\nL inearity of relationship between variables\nI ndependence of the Y values\nN ormality of the residuals\nE quality of variance of the residuals (homoscedasticity)\n\nMore in depth:\n\nL : there is a linear relationship between the mean response (Y) and the explanatory variable (X),\nI : the errors are independent—there’s no connection between how far any two points lie from the regression line,\nN : the responses are normally distributed at each level of X, and\nE : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#l-linearity-of-relationship-between-variables",
    "href": "lessons/16_SLR_02/16_SLR_02.html#l-linearity-of-relationship-between-variables",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\nIs the association between the variables linear?\n\nDiagnostic tools:\n\nScatterplot\nResidual plot (see later section for E : Equality of variance of the residuals)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#i-independence-of-the-residuals-y-values",
    "href": "lessons/16_SLR_02/16_SLR_02.html#i-independence-of-the-residuals-y-values",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#n-normality-of-the-residuals-1",
    "href": "lessons/16_SLR_02/16_SLR_02.html#n-normality-of-the-residuals-1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\n\n\n\n\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#extract-models-residuals-in-r",
    "href": "lessons/16_SLR_02/16_SLR_02.html#extract-models-residuals-in-r",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Extract model’s residuals in R",
    "text": "Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 8\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#check-normality-with-usual-distribution-plots",
    "href": "lessons/16_SLR_02/16_SLR_02.html#check-normality-with-usual-distribution-plots",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Check normality with “usual” distribution plots",
    "text": "Check normality with “usual” distribution plots\nNote that below I save each figure, and then combine them together in one row of output using grid.arrange() from the gridExtra package.\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#normal-qq-plots-qq-quantile-quantile",
    "href": "lessons/16_SLR_02/16_SLR_02.html#normal-qq-plots-qq-quantile-quantile",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Normal QQ plots (QQ = quantile-quantile)",
    "text": "Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n\n\n\n\n\n\n\n\n\nData are approximately normal if points fall on a line.\n\nSee more info at https://data.library.virginia.edu/understanding-QQ-plots/"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#examples-of-normal-qq-plots-15",
    "href": "lessons/16_SLR_02/16_SLR_02.html#examples-of-normal-qq-plots-15",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (1/5)",
    "text": "Examples of Normal QQ plots (1/5)\n\nData:\n\nBody measurements from 507 physically active individuals\nin their 20’s or early 30’s\nwithin normal weight range."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#examples-of-normal-qq-plots-25",
    "href": "lessons/16_SLR_02/16_SLR_02.html#examples-of-normal-qq-plots-25",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (2/5)",
    "text": "Examples of Normal QQ plots (2/5)\nSkewed right distribution"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#examples-of-normal-qq-plots-35",
    "href": "lessons/16_SLR_02/16_SLR_02.html#examples-of-normal-qq-plots-35",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (3/5)",
    "text": "Examples of Normal QQ plots (3/5)\nLong tails in distribution"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#examples-of-normal-qq-plots-45",
    "href": "lessons/16_SLR_02/16_SLR_02.html#examples-of-normal-qq-plots-45",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (4/5)",
    "text": "Examples of Normal QQ plots (4/5)\nBimodal distribution"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#examples-of-normal-qq-plots-55",
    "href": "lessons/16_SLR_02/16_SLR_02.html#examples-of-normal-qq-plots-55",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (5/5)",
    "text": "Examples of Normal QQ plots (5/5)"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#qq-plot-of-residuals-of-model1",
    "href": "lessons/16_SLR_02/16_SLR_02.html#qq-plot-of-residuals-of-model1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "QQ plot of residuals of model1",
    "text": "QQ plot of residuals of model1\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#compare-to-randomly-generated-normal-qq-plots",
    "href": "lessons/16_SLR_02/16_SLR_02.html#compare-to-randomly-generated-normal-qq-plots",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Compare to randomly generated Normal QQ plots",
    "text": "Compare to randomly generated Normal QQ plots\nHow “good” we can expect a QQ plot to look depends on the sample size.\n\nThe QQ plots on the next slides are randomly generated\n\nusing random samples from actual standard normal distributions \\(N(0,1)\\).\n\nThus, all the points in the QQ plots should theoretically fall in a line\nHowever, there is sampling variability…"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#randomly-generated-normal-qq-plots-n100",
    "href": "lessons/16_SLR_02/16_SLR_02.html#randomly-generated-normal-qq-plots-n100",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Randomly generated Normal QQ plots: n=100",
    "text": "Randomly generated Normal QQ plots: n=100\n\nNote that stat_qq_line() doesn’t work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is \\(y=x\\) in this case.)\n\n\n\n\n\nsamplesize &lt;- 100\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#examples-of-simulated-normal-qq-plots-n10",
    "href": "lessons/16_SLR_02/16_SLR_02.html#examples-of-simulated-normal-qq-plots-n10",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of simulated Normal QQ plots: n=10",
    "text": "Examples of simulated Normal QQ plots: n=10\nWith fewer data points,\n\nsimulated QQ plots are more likely to look “less normal”\neven though the data points were sampled from normal distributions.\n\n\n\n\n\nsamplesize &lt;- 10  # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#examples-of-simulated-normal-qq-plots-n1000",
    "href": "lessons/16_SLR_02/16_SLR_02.html#examples-of-simulated-normal-qq-plots-n1000",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of simulated Normal QQ plots: n=1,000",
    "text": "Examples of simulated Normal QQ plots: n=1,000\nWith more data points,\n\nsimulated QQ plots are more likely to look “more normal”\n\n\n\n\n\nsamplesize &lt;- 1000 # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#back-to-our-example",
    "href": "lessons/16_SLR_02/16_SLR_02.html#back-to-our-example",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Back to our example",
    "text": "Back to our example\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with n = 80\n\n\n\n# number of observations \n# in fitted model\nnobs(model1) \n\n[1] 80\n\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#residual-plot",
    "href": "lessons/16_SLR_02/16_SLR_02.html#residual-plot",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Residual plot",
    "text": "Residual plot\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\n\n\nnames(aug1)\n\n[1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n[3] \".fitted\"                    \".resid\"                    \n[5] \".hat\"                       \".sigma\"                    \n[7] \".cooksd\"                    \".std.resid\"                \n\n\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "href": "lessons/16_SLR_02/16_SLR_02.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "E: Equality of variance of the residuals (Homoscedasticity)",
    "text": "E: Equality of variance of the residuals (Homoscedasticity)\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#r2-coefficient-of-determination-12",
    "href": "lessons/16_SLR_02/16_SLR_02.html#r2-coefficient-of-determination-12",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) = Coefficient of determination (1/2)",
    "text": "\\(R^2\\) = Coefficient of determination (1/2)\n\nRecall that the correlation coefficient \\(r\\) measures the strength of the linear relationship between two numerical variables\n\\(R^2\\) is usually used to measure the strength of a linear fit\n\nFor a simple linear regression model (one numerical predictor), \\(R^2\\) is just the square of the correlation coefficient\n\nIn general, \\(R^2\\) is the proportion of the variability of the dependent variable that is explained by the independent variable(s)\n\n\\[R^2 = \\frac{\\textrm{variance of predicted y-values}}\n{\\textrm{variance of observed y-values}} = \\frac{\\sum_{i=1}^n(\\widehat{y}_i-\\bar{y})^2}\n{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n= \\frac{s_y^2 - s_{\\textrm{residuals}}^2}\n{s_y^2}\\] \\[R^2 = 1- \\frac{s_{\\textrm{residuals}}^2}\n{s_y^2}\\] where \\(\\frac{s_{\\textrm{residuals}}^2}{s_y^2}\\) is the proportion of “unexplained” variability in the \\(y\\) values,\nand thus \\(R^2 = 1- \\frac{s_{\\textrm{residuls}}^2}{s_y^2}\\) is the proportion of “explained” variability in the \\(y\\) values"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#r2-coefficient-of-determination-22",
    "href": "lessons/16_SLR_02/16_SLR_02.html#r2-coefficient-of-determination-22",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) = Coefficient of determination (2/2)",
    "text": "\\(R^2\\) = Coefficient of determination (2/2)\n\nRecall, \\(-1&lt;r&lt;1\\)\nThus, \\(0&lt;R^2&lt;1\\)\nIn practice, we want “high” \\(R^2\\) values, i.e. \\(R^2\\) as close to 1 as possible.\n\nCalculating \\(R^2\\) in R using glance() from the broom package:\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(model1)$r.squared\n\n[1] 0.4109366\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nA model can have a high \\(R^2\\) value when there is a curved pattern.\nAlways first check whether a linear model is reasonable or not."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#r2-in-summary-r-output",
    "href": "lessons/16_SLR_02/16_SLR_02.html#r2-in-summary-r-output",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) in summary() R output",
    "text": "\\(R^2\\) in summary() R output\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\nCompare to the square of the correlation coefficient \\(r\\):\n\nr &lt;- cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\nr\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#inference-for-population-slope-beta_1",
    "href": "lessons/16_SLR_02/16_SLR_02.html#inference-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Inference for population slope \\(\\beta_1\\)",
    "text": "Inference for population slope \\(\\beta_1\\)\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1, conf.int = TRUE) %&gt;% gt() # conf.int = TRUE part is new! \n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\n\\[\\begin{align}\n\\widehat{y} =& b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{life expectancy}} =& 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{align}\\]\n\nWhat are \\(H_0\\) and \\(H_A\\)?\nHow do we calculate the standard error, statistic, p-value, and CI?\n\n\n\n\n\n\n\nNote\n\n\n\n\nWe can also test & calculate CI for the population intercept\nThis will be covered in BSTA 512"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "href": "lessons/16_SLR_02/16_SLR_02.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Inference for the population slope: CI and hypothesis test",
    "text": "Inference for the population slope: CI and hypothesis test\n\n\nPopulation model\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\nSample best-fit (least-squares) line:\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\nNote: Some sources use \\(\\widehat{\\beta}\\) instead of \\(b\\).\n\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)\n\n\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#ci-for-population-slope-beta_1",
    "href": "lessons/16_SLR_02/16_SLR_02.html#ci-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "CI for population slope \\(\\beta_1\\)",
    "text": "CI for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\textrm{Point Estimate} \\pm t^*\\cdot SE_{\\textrm{Point Estimate}}\\]\nFor the CI of the coefficient \\(b_1\\) this translates to\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\] where \\(t^*\\) is the critical value from a \\(t\\)-distribution with \\(df = n -2\\).\n\nHow is \\(\\text{SE}_{b_1}\\) calculated? See next slide.\n\n\ntidy(model1, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)             50.9      2.66       19.1  3.33e-31   45.6      56.2  \n2 female_literacy_rate…    0.232    0.0315      7.38 1.50e-10    0.170     0.295"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#standard-error-of-fitted-slope-b_1",
    "href": "lessons/16_SLR_02/16_SLR_02.html#standard-error-of-fitted-slope-b_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Standard error of fitted slope \\(b_1\\)",
    "text": "Standard error of fitted slope \\(b_1\\)\n\n\n\\[\\text{SE}_{b_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{b_1}\\) is the variability of the statistic \\(b_1\\)\n\n\n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\n\n\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] 0.03147744"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#calculate-ci-for-population-slope-beta_1",
    "href": "lessons/16_SLR_02/16_SLR_02.html#calculate-ci-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Calculate CI for population slope \\(\\beta_1\\)",
    "text": "Calculate CI for population slope \\(\\beta_1\\)\n\n\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nSave regression output for the row with the slope’s information:\n\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\n\nCompare CI bounds below with the ones in the regression table above.\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#hypothesis-test-for-population-slope-beta_1",
    "href": "lessons/16_SLR_02/16_SLR_02.html#hypothesis-test-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Hypothesis test for population slope \\(\\beta_1\\)\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nThe test statistic for \\(b_1\\) is\n\\[t = \\frac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}} = \\frac{ b_1}{ \\text{SE}_{b_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nCalculate the test statistic using the values in the regression table:\n\n# recall model1_b1 is regression table restricted to b1 row\n(TestStat &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\nCompare this test statistic value to the one from the regression table above"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#p-value-for-testing-population-slope-beta_1",
    "href": "lessons/16_SLR_02/16_SLR_02.html#p-value-for-testing-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(p\\)-value for testing population slope \\(\\beta_1\\)",
    "text": "\\(p\\)-value for testing population slope \\(\\beta_1\\)\n\nAs usual, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\nTo calculate the \\(p\\)-value, we need to know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true.\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\nRecall that this is a 2-sided test:\n\n\n(pv = 2*pt(TestStat, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\nCompare the \\(p\\)-value to the one from the regression table below\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()  # compare p-value calculated above to p-value in table\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#prediction-with-regression-line",
    "href": "lessons/16_SLR_02/16_SLR_02.html#prediction-with-regression-line",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\nWhat is the predicted life expectancy for a country with female literacy rate 60%?\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot 60 = 64.82\\]\n\n(y_60 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n\n\n\nHow do we interpret the predicted value?\nHow variable is it?"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#prediction-with-regression-line-1",
    "href": "lessons/16_SLR_02/16_SLR_02.html#prediction-with-regression-line-1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\n\nWhen we take the expected value, at a given value \\(x^*\\), we have that the predicted response is the average expected response at \\(x^*\\):\n\n\\[\\widehat{E[Y|x^*]} = b_0 + b_1 x^*\\]\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThese are the points on the regression line.\nThe mean responses has variability, and we can calculate a CI for it, for every value of \\(x^*\\)."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#ci-for-mean-response-mu_yx",
    "href": "lessons/16_SLR_02/16_SLR_02.html#ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "CI for mean response \\(\\mu_{Y|x^*}\\)\n\\[\\widehat{E[Y|x^*]} \\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\]\n\n\\(SE_{\\widehat{E[Y|x^*]}}\\) is calculated using\n\n\\[SE_{\\widehat{E[Y|x^*]}} = s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\n\\(\\widehat{E[Y|x^*]}\\) is the predicted value at the specified point \\(x^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\bar{x}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#example-ci-for-mean-response-mu_yx",
    "href": "lessons/16_SLR_02/16_SLR_02.html#example-ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 0.9675541\n\n\n\n\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] 1.926252\n\n\n\n\n\n\nY60 - MOE_Yx\n\n[1] 62.93335\n\n\n\n\n\n\nY60 + MOE_Yx\n\n[1] 66.78586"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "href": "lessons/16_SLR_02/16_SLR_02.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 40, 60, and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(x^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 60.21570 57.26905 63.16236\n2 64.85961 62.93335 66.78586\n3 69.50351 68.13244 70.87457\n\n\n\n\nInterpretation\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#confidence-bands-for-mean-response-mu_yx",
    "href": "lessons/16_SLR_02/16_SLR_02.html#confidence-bands-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nOften we plot the CI for many values of X, creating confidence bands\nThe confidence bands are what ggplot creates when we set se = TRUE within geom_smooth\nFor what values of x are the confidence bands (intervals) narrowest?\n\n\nggplot(gapm,\n       aes(x=female_literacy_rate_2011, \n           y=life_expectancy_years_2011)) +\n  geom_point()+\n  geom_smooth(method = lm, se=TRUE)+\n  ggtitle(\"Life expectancy vs. female literacy rate\") \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "href": "lessons/16_SLR_02/16_SLR_02.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nFor what values of \\(x^*\\) are the confidence bands (intervals) narrowest? widest?\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\n\\end{align}\\]\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#prediction-interval-for-predicting-individual-observations",
    "href": "lessons/16_SLR_02/16_SLR_02.html#prediction-interval-for-predicting-individual-observations",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction interval for predicting individual observations",
    "text": "Prediction interval for predicting individual observations\n\nWe do not call this interval a CI since \\(Y\\) is a random variable instead of a parameter\nThe form is similar to a CI though:\n\n\\[\\widehat{Y|x^*} \\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\nNote that the only difference to the CI for a mean value of y is the additional 1+ under the square root.\n\nThus the width is wider!"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#example-prediction-interval",
    "href": "lessons/16_SLR_02/16_SLR_02.html#example-prediction-interval",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Prediction interval",
    "text": "Example: Prediction interval\nFind the 95% prediction interval for the life expectancy when the female literacy rate is 60.\n\\[\\begin{align}\n\\widehat{Y|x^*} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{1+\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n(52.48072 &, 77.23849)\n\\end{align}\\]\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Ypred &lt;- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 6.217898\n\n\n\n\n\n(MOE_Ypred &lt;- SE_Ypred*tstar)\n\n[1] 12.37888\n\n\n\n\n\n\nY60 - MOE_Ypred\n\n[1] 52.48072\n\n\n\n\n\n\nY60 + MOE_Ypred\n\n[1] 77.23849"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#example-using-r-for-prediction-interval",
    "href": "lessons/16_SLR_02/16_SLR_02.html#example-using-r-for-prediction-interval",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Using R for prediction interval",
    "text": "Example: Using R for prediction interval\nFind the 95% prediction intervals for the life expectancy when the female literacy rate is 40, 60, and 80.\n\nnewdata  # previously defined for CI's\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\npredict(model1, \n        newdata=newdata, \n        interval=\"prediction\")  # prediction instead of \"confidence\"\n\n       fit      lwr      upr\n1 60.21570 47.63758 72.79382\n2 64.85961 52.48072 77.23849\n3 69.50351 57.19879 81.80823\n\n\n\n\nInterpretation\nWe are 95% confident that a new selected country with a 60% female literacy rate will have a life expectancy between 52.5 and 77.2 years."
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#prediction-bands-vs.-confidence-bands-12",
    "href": "lessons/16_SLR_02/16_SLR_02.html#prediction-bands-vs.-confidence-bands-12",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction bands vs. confidence bands (1/2)",
    "text": "Prediction bands vs. confidence bands (1/2)\nCreate a scatterplot with the regression line, 95% confidence bands, and 95% prediction bands.\n\nFirst create a data frame with the original data points (both x and y values), their respective predicted values, andtheir respective prediction intervals\nCan do this with augment() from the broom package.\n\n\nmodel1_pred_bands &lt;- augment(model1, interval = \"prediction\")\n\n# take a look at new object:\nnames(model1_pred_bands) \n\n [1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n [3] \".fitted\"                    \".lower\"                    \n [5] \".upper\"                     \".resid\"                    \n [7] \".hat\"                       \".sigma\"                    \n [9] \".cooksd\"                    \".std.resid\"                \n\n# glimpse of select variables of interest:\nmodel1_pred_bands %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011, \n         .fitted:.upper) %&gt;% \n  glimpse()\n\nRows: 80\nColumns: 5\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .lower                     &lt;dbl&gt; 40.91166, 60.81324, 52.14572, 61.65365, 61.…\n$ .upper                     &lt;dbl&gt; 66.98121, 85.48470, 76.92334, 86.36253, 86.…"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#prediction-bands-vs.-confidence-bands-22",
    "href": "lessons/16_SLR_02/16_SLR_02.html#prediction-bands-vs.-confidence-bands-22",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction bands vs. confidence bands (2/2)",
    "text": "Prediction bands vs. confidence bands (2/2)\n\nnames(model1_pred_bands) \n\n [1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n [3] \".fitted\"                    \".lower\"                    \n [5] \".upper\"                     \".resid\"                    \n [7] \".hat\"                       \".sigma\"                    \n [9] \".cooksd\"                    \".std.resid\"                \n\n\n\nggplot(model1_pred_bands, \n       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +\n  geom_point() +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands\n              alpha = 0.2, fill = \"red\") +\n  geom_smooth(method=lm) +  # confidence bands\n  labs(title = \"SLR with Confidence & Prediction Bands\") \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#corrrelation-doesnt-imply-causation",
    "href": "lessons/16_SLR_02/16_SLR_02.html#corrrelation-doesnt-imply-causation",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Corrrelation doesn’t imply causation*!",
    "text": "Corrrelation doesn’t imply causation*!\n\nThis might seem obvious, but make sure to not write your analysis results in a way that implies causation if the study design doesn’t warrant it (such as an observational study).\nBeware of spurious correlations: http://www.tylervigen.com/spurious-correlations\n\n\n\n*Caveat: there is a whole field of statistics/epidemiology on causal inference. https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf"
  },
  {
    "objectID": "lessons/16_SLR_02/16_SLR_02.html#whats-next",
    "href": "lessons/16_SLR_02/16_SLR_02.html#whats-next",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "What’s next?",
    "text": "What’s next?"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "Previous research\n\nResearchers examined DDS expenditures for developmentally disabled residents by ethnicity\nFound that the mean annual expenditures on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?\n\n\n\n\n\n\nThe textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window.\n\n\n\n\n\n\nNew: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nethnicity, age, and expenditures (code on next slide)\n\n\n\n\n\n\n\n\n\nPlot on previous slide\n\n\ndds.discr_Hips_WhnH &lt;- dds.discr %&gt;%  \n  filter(ethnicity == \"White not Hispanic\" | ethnicity == \"Hispanic\" ) %&gt;% \n  droplevels()   # remove empty factor levels\n\nggplot(data = dds.discr_Hips_WhnH,   \n       aes(x = expenditures,\n           y = age.cohort)) + \n  geom_boxplot(color=\"darkgrey\") + \n  facet_grid(rows = \"ethnicity\") +   \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(     \n      height = 0.4))      \n\n\n\n\n\n\n\nmean_expend &lt;- \n  dds.discr_Hips_WhnH %&gt;% \n  group_by(\n    ethnicity, age.cohort)%&gt;% \n  summarize(\n    ave = mean(expenditures))\n\n`summarise()` has grouped output by 'ethnicity'. You can override using the\n`.groups` argument.\n\n\n\n\nmean_expend\n\n# A tibble: 12 × 3\n# Groups:   ethnicity [2]\n   ethnicity          age.cohort    ave\n   &lt;fct&gt;              &lt;fct&gt;       &lt;dbl&gt;\n 1 Hispanic           0-5         1393.\n 2 Hispanic           6-12        2312.\n 3 Hispanic           13-17       3955.\n 4 Hispanic           18-21       9960.\n 5 Hispanic           22-50      40924.\n 6 Hispanic           51+        55585 \n 7 White not Hispanic 0-5         1367.\n 8 White not Hispanic 6-12        2052.\n 9 White not Hispanic 13-17       3904.\n10 White not Hispanic 18-21      10133.\n11 White not Hispanic 22-50      40188.\n12 White not Hispanic 51+        52670.\n\n\n\n\n\n\n\n\n\n\nmean_expend_wide &lt;- \n  mean_expend %&gt;% \n  pivot_wider(\n    names_from = ethnicity,\n    values_from = ave)\n\n\n\nmean_expend_wide\n\n# A tibble: 6 × 3\n  age.cohort Hispanic `White not Hispanic`\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n1 0-5           1393.                1367.\n2 6-12          2312.                2052.\n3 13-17         3955.                3904.\n4 18-21         9960.               10133.\n5 22-50        40924.               40188.\n6 51+          55585                52670.\n\n\n\n\n\n\n\n\nmean_expend_wide &lt;- mean_expend_wide %&gt;% \n  mutate(diff_mean = `White not Hispanic` - Hispanic)\n\nmean_expend_wide\n\n# A tibble: 6 × 4\n  age.cohort Hispanic `White not Hispanic` diff_mean\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 0-5           1393.                1367.     -26.3\n2 6-12          2312.                2052.    -260. \n3 13-17         3955.                3904.     -50.9\n4 18-21         9960.               10133.     173. \n5 22-50        40924.               40188.    -736. \n6 51+          55585                52670.   -2915. \n\n\n\nQuestion: Are the data sufficient evidence of ethnic discrimination in DDS expenditures when comparing Hispanics with White non-Hispanics?\n\n\n\n\n\n\nThis case study is an example of confounding known as Simpson’s paradox\nSimpson’s paradox happens when an association observed in several groups disappears or reverses direction when the groups are combined.\nIn other words, an association between two variables \\(X\\) and \\(Y\\) may disappear or reverse direction once data are partitioned into subpopulations based on a third variable \\(Z\\) (i.e., a confounding variable)."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "Previous research\n\nResearchers examined DDS expenditures for developmentally disabled residents by ethnicity\nFound that the mean annual expenditures on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#load-dds.discr-dataset-from-oibiostat-package",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#load-dds.discr-dataset-from-oibiostat-package",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "The textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#glimpse",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#glimpse",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "New: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#visualize-in-more-detail",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#visualize-in-more-detail",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "ethnicity, age, and expenditures (code on next slide)"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#code-for-visualize-in-more-detail-ethnicity-age-and-expenditures",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#code-for-visualize-in-more-detail-ethnicity-age-and-expenditures",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "Plot on previous slide\n\n\ndds.discr_Hips_WhnH &lt;- dds.discr %&gt;%  \n  filter(ethnicity == \"White not Hispanic\" | ethnicity == \"Hispanic\" ) %&gt;% \n  droplevels()   # remove empty factor levels\n\nggplot(data = dds.discr_Hips_WhnH,   \n       aes(x = expenditures,\n           y = age.cohort)) + \n  geom_boxplot(color=\"darkgrey\") + \n  facet_grid(rows = \"ethnicity\") +   \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(     \n      height = 0.4))"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#mean-annual-dds-expenditures-by-raceethnicity-default-long-format",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#mean-annual-dds-expenditures-by-raceethnicity-default-long-format",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "mean_expend &lt;- \n  dds.discr_Hips_WhnH %&gt;% \n  group_by(\n    ethnicity, age.cohort)%&gt;% \n  summarize(\n    ave = mean(expenditures))\n\n`summarise()` has grouped output by 'ethnicity'. You can override using the\n`.groups` argument.\n\n\n\n\nmean_expend\n\n# A tibble: 12 × 3\n# Groups:   ethnicity [2]\n   ethnicity          age.cohort    ave\n   &lt;fct&gt;              &lt;fct&gt;       &lt;dbl&gt;\n 1 Hispanic           0-5         1393.\n 2 Hispanic           6-12        2312.\n 3 Hispanic           13-17       3955.\n 4 Hispanic           18-21       9960.\n 5 Hispanic           22-50      40924.\n 6 Hispanic           51+        55585 \n 7 White not Hispanic 0-5         1367.\n 8 White not Hispanic 6-12        2052.\n 9 White not Hispanic 13-17       3904.\n10 White not Hispanic 18-21      10133.\n11 White not Hispanic 22-50      40188.\n12 White not Hispanic 51+        52670."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#mean-annual-dds-expenditures-by-raceethnicity-wide-format",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#mean-annual-dds-expenditures-by-raceethnicity-wide-format",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "mean_expend_wide &lt;- \n  mean_expend %&gt;% \n  pivot_wider(\n    names_from = ethnicity,\n    values_from = ave)\n\n\n\nmean_expend_wide\n\n# A tibble: 6 × 3\n  age.cohort Hispanic `White not Hispanic`\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n1 0-5           1393.                1367.\n2 6-12          2312.                2052.\n3 13-17         3955.                3904.\n4 18-21         9960.               10133.\n5 22-50        40924.               40188.\n6 51+          55585                52670."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#differences-in-mean-annual-dds-expenditures-by-age-cohort-and-raceethnicity",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#differences-in-mean-annual-dds-expenditures-by-age-cohort-and-raceethnicity",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "mean_expend_wide &lt;- mean_expend_wide %&gt;% \n  mutate(diff_mean = `White not Hispanic` - Hispanic)\n\nmean_expend_wide\n\n# A tibble: 6 × 4\n  age.cohort Hispanic `White not Hispanic` diff_mean\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 0-5           1393.                1367.     -26.3\n2 6-12          2312.                2052.    -260. \n3 13-17         3955.                3904.     -50.9\n4 18-21         9960.               10133.     173. \n5 22-50        40924.               40188.    -736. \n6 51+          55585                52670.   -2915. \n\n\n\nQuestion: Are the data sufficient evidence of ethnic discrimination in DDS expenditures when comparing Hispanics with White non-Hispanics?"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#simpsons-paradox",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#simpsons-paradox",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "This case study is an example of confounding known as Simpson’s paradox\nSimpson’s paradox happens when an association observed in several groups disappears or reverses direction when the groups are combined.\nIn other words, an association between two variables \\(X\\) and \\(Y\\) may disappear or reverse direction once data are partitioned into subpopulations based on a third variable \\(Z\\) (i.e., a confounding variable)."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#tools-for-wrangling-data",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#tools-for-wrangling-data",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Tools for wrangling data",
    "text": "Tools for wrangling data\n\n\ntidyverse functions\n\ntidyverse is a suite of packages that implement tidy methods for data importing, cleaning, wrangling, and visualizing\nload the tidyverse packages by running the code library(tidyverse)\n\nDon’t forget to first install tidyverse!\n\n\nFunctions to easily work with rows and columns, such as\n\nsubset rows/columns\nadd new rows/columns\njoin together different data sets\nmake data long or wide\n\nThere are often many steps to tidy data\n\nwe string together commands\nto be performed sequentially\nusing pipes %&gt;%"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#summary-of-data-wrangling-so-far",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#summary-of-data-wrangling-so-far",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Summary of data wrangling so far",
    "text": "Summary of data wrangling so far\n\n\nThe pipe %&gt;% to string together commands in sequence\nmutate() to add a new variable to a dataset\nselect() to select columns (or deselect columns with -variable)\nfilter() to select specific rows\npivot_wider() to reshape a dataset from a long to a wide format\n\nSummarizing data\n\ntabyl() from janitor package to make frequency tables of categorical variables\nsummarize() to get summary statistics of variables\ngroup_by() to group data by categorical variables before finding summaries"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#what-packages-are-included-in-the-tidyverse",
    "href": "lessons/03_Data_visualization/03_Data_visualization_part_02.html#what-packages-are-included-in-the-tidyverse",
    "title": "Day 3: Data visualization - Part 2",
    "section": "What packages are included in the tidyverse?",
    "text": "What packages are included in the tidyverse?\n\n\nCore packages\nThese automatically load when loading the tidyverse package\n\n\n\nhttps://www.tidyverse.org/\n\n\n\nList of all packages:\n\ntidyverse_packages(include_self = TRUE)\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\n\nPackages not a part of the core get installed with the tidyverse suite, but need to be loaded separately.\n\nSee https://www.tidyverse.org/packages/ for more info."
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "",
    "text": "(1.3) Data collection principles\n\nPopulation vs. sample\nSampling methods\nExperiments vs. Observational studies\n\n(1.2) Intro to Data\n\nData types\nHow are data stored in R?\nWorking with data in R\n\n(1.4) Summarizing numerical data\n\nMean, median, mode, SD, IQR, range, 5 number summary\nEmpirical Rule\nrobust statistics\n\nR packages -&gt; install for next class!!!"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#useful-keyboard-shortcuts",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#useful-keyboard-shortcuts",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Useful keyboard shortcuts",
    "text": "Useful keyboard shortcuts\nFull list of keyboard shortcuts\n \n\n\n\n\n\n\n\n\naction\nmac\nwindows/linux\n\n\n\n\nRun code in qmd (or script)\ncmd + enter\nctrl + enter\n\n\n&lt;-\noption + -\nalt + -\n\n\ninterrupt currently running command\nesc\nesc\n\n\nin console, retrieve previously run code\nup/down\nup/down\n\n\nkeyboard shortcut help\noption + shift + k\nalt + shift + k\n\n\n\n\n\n\nPractice\nTry typing code below in your qmd (with shortcut) and evaluating it:\n\ny &lt;- 5\ny"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#another-resource-for-an-introduction-to-r",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#another-resource-for-an-introduction-to-r",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Another resource for an introduction to R",
    "text": "Another resource for an introduction to R\n\nIf you would like another perspective on what we covered the first week, you might find Danielle Navarro’s online book Learning Statistics with R to be helpful.\nDownload free pdf: https://learningstatisticswithr.com/\nSee Sections 3.1-3.7.1 for some of the topics we covered on first day"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#population-vs.-sample",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#population-vs.-sample",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Population vs. sample",
    "text": "Population vs. sample\n\n\n\n(Target) Population\n\ngroup of interest being studied\ngroup from which the sample is selected\n\nstudies often have inclusion and/or exclusion criteria\n\n\n\n\n\n\nSample\n\ngroup on which data are collected\noften a small subset of the population"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#sampling-methods-14",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#sampling-methods-14",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Sampling methods (1/4)",
    "text": "Sampling methods (1/4)\nGoal is to get a representative sample of the population:\nthe characteristics of the sample are similar to the characteristics of the population\n\n\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nrandomly sampled\nconsidered best way to sample\n\n\n\n\n\n\n\n\n\nConvenience sample\n\neasily accessible individuals are more likely to be included in the sample than other individuals\na common “pitfall”"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#sampling-methods-24",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#sampling-methods-24",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Sampling methods (2/4)",
    "text": "Sampling methods (2/4)\nGood sampling plans don’t guarantee samples representative of the population\n\n\nNon-response bias\n\nnon-response rates can be high\nare all groups within a population being reached?\nunrepresentative sample\n=&gt; skewed results\n\n\n\n\n\n\n\n\n\n“Random” samples can be unrepresentative by random chance\n\nIn a SRS each case in the population has an equal chance of being included in the sample\nBut by random chance alone a random sample might contain a higher proportion of one group over another\nEx: a SRS might by chance include 70% men (unlikely, but theoretically possible)"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#sampling-methods-34",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#sampling-methods-34",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Sampling methods (3/4)",
    "text": "Sampling methods (3/4)\n\n\n\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nstatistical methods taught in this class assume a SRS!\n\nStratified sampling\n\ndivide population into groups (strata) before selecting cases within each stratum (often via SRS)\nusually cases within a strata are similar, but are different from other strata with respect to the outcome of interest, such as gender or age groups"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#sampling-methods-44",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#sampling-methods-44",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Sampling methods (4/4)",
    "text": "Sampling methods (4/4)\n\n\n\nCluster sample\n\nfirst divide population into groups (clusters)\nthen sample a fixed number of clusters, and include all observations from chosen clusters\nclusters are often hospitals, clinicians, schools, etc., where each cluster will have similar services/ policies/ etc.\ncases within clusters usually very diverse\n\nMultistage sample\n\nsimilar to a cluster sample, but select a random sample within each selected cluster instead of all individuals"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#experiments-12",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#experiments-12",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Experiments (1/2)",
    "text": "Experiments (1/2)\n\n\nResearchers assign individuals to different treatment or intervention groups\n\ncontrol group: often receive a placebo or usual care\ndifferent treatment groups are often called study arms\n\nRandomization\n\ngroup assignment is usually random to ensure similar (balanced) study arms for all variables (observed and unobserved)\nrandomization allows study arm differences in outcomes to be attributed to treatment rather than variability in patient characteristics\n\ntreatment is the only systematic difference between groups\nestablish causality\n\nblocking (stratification): group individuals into blocks (strata) before randomizing if there are certain characteristics that may influence the outcome other than treatment (i.e. gender, age group)"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#experiments-22",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#experiments-22",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Experiments (2/2)",
    "text": "Experiments (2/2)\n\n\nReplication\n\naccomplished by collecting a sufficiently large sample\nresults usually more reliable with a large sample size\n\noften less variability\nmore likely to be representative of population\n\n\nSome studies are not ethical to carry out as experiments"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#observational-studies",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#observational-studies",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Observational studies",
    "text": "Observational studies\n\n\ndata are observed and recorded without interference\noften done via surveys, electronic health records, or medical chart reviews\ncohorts\nassociations between variables can be established, but not causality\n\nIndividuals with different characteristics may also differ in other ways that influence response\n\nconfounding variables (lurking variable)\n\nvariables associated with both the explanatory and response variables\n\nprospective vs. retrospective studies"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#comparing-study-designs",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#comparing-study-designs",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Comparing study designs",
    "text": "Comparing study designs\n\n\n\nScience Media Centre"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#systematic-reviews-example",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#systematic-reviews-example",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Systematic Reviews example",
    "text": "Systematic Reviews example\n\n\n\nSTEM: Systematically Testing the Evidence on Marijuana\n\n\n\n\n\nSTEM is a collaborative project between the US Department of Veterans Affairs and the Center for Evidence-based Policy at Oregon Health & Science University.\nThe project is funded by the US Department of Veterans Affairs: Office of Rural Health."
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#how-are-data-stored-how-do-we-use-them",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#how-are-data-stored-how-do-we-use-them",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "How are data stored, how do we use them?",
    "text": "How are data stored, how do we use them?\n\nOften, data are in an Excel sheet, or a plain text file (.csv, .txt)\n.csv files open in Excel automatically, but actually are plain text\nUsually, columns are variables/measures and rows are observations (i.e. a person’s measurements)\n\n\nData in R\n\nWe can import data from many file types, including .csv, .txt., and .xlsx\n\nWe will cover this on a later date\n\nOnce imported, R typically stores data as data frames, or tibbles if using the tidyverse package (more on this later).\n\nFor our purposes, these are essentially the same, and I will tend to use the terms interchangeably.\nThese are examples of what we call object types in R."
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#data-frame-example",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#data-frame-example",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Data frame example",
    "text": "Data frame example\n\n\n\ndf &lt;- data.frame(\n  IDs=1:3, \n  gender=c(\"male\", \"female\", \"Male\"), \n  age=c(28, 35.5, 31),\n  trt = c(\"control\", \"1\", \"1\"),\n  Veteran = c(FALSE, TRUE, TRUE)\n  )\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\n\n\nVectors vs. data frames\n\na data frame is a collection (or array or table) of vectors\n\n\n\n\n\n\n\nDifferent columns can be of different data types (i.e. numeric vs. text)\nBoth numeric and text can be stored within a column (stored together as text).\nVectors and data frames are examples of objects in R.\n\nThere are other types of R objects to store data, such as matrices, lists."
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#observations-variables",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#observations-variables",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Observations & variables",
    "text": "Observations & variables\n\n\n\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\n\n\n\n\nISLBS\n\n\n\n\n\nBook refers to a dataset as a data matrix\nRows are usually observations\nColumns are usually variables\nHow many observations are in this dataset?\nWhat are the variable types in this dataset?"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#variable-column-types",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#variable-column-types",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Variable (column) types",
    "text": "Variable (column) types\n\n\n\n\n\n\n\n\n\nR type\nvariable type\ndescription\n\n\n\n\ninteger\ndiscrete\ninteger-valued numbers\n\n\ndouble or numeric\ncontinuous\nnumbers that are decimals\n\n\nfactor\ncategorical\ncategorical variables stored with levels (groups)\n\n\ncharacter\ncategorical\ntext, “strings”\n\n\nlogical\ncategorical\nboolean (TRUE, FALSE)\n\n\n\n\n\nView the structure of our data frame to see what the variable types are:\n\n\n\nstr(df)\n\n'data.frame':   3 obs. of  5 variables:\n $ IDs    : int  1 2 3\n $ gender : chr  \"male\" \"female\" \"Male\"\n $ age    : num  28 35.5 31\n $ trt    : chr  \"control\" \"1\" \"1\"\n $ Veteran: logi  FALSE TRUE TRUE"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#fishers-or-andersons-iris-data-set",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#fishers-or-andersons-iris-data-set",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Fisher’s (or Anderson’s) Iris data set",
    "text": "Fisher’s (or Anderson’s) Iris data set\nData description:\n\nn = 150\n3 species of Iris flowers (Setosa, Virginica, and Versicolour)\n\n50 measurements of each type of Iris\n\nvariables:\n\nsepal length, sepal width, petal length, petal width, and species\n\n\nCan the iris species be determined by these variables?\n\n\n\nGareth Duffy"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#view-the-iris-dataset",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#view-the-iris-dataset",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "View the iris dataset",
    "text": "View the iris dataset\n\n\nThe iris dataset is already pre-loaded in base R and ready to use.\nType the following command in the console window\n\nWarning: this command cannot be rendered. It will give an error.\n\n\n\n\n\n\n\n\nView(iris)\n\n\nA new tab in the scripting window should appear with the iris dataset."
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#data-structure",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#data-structure",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Data structure",
    "text": "Data structure\n\nWhat are the different variable types in this data set?\n\n\n\n\nstr(iris)   # structure of data\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#data-set-summary",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#data-set-summary",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Data set summary",
    "text": "Data set summary\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#data-set-info",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#data-set-info",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Data set info",
    "text": "Data set info\n\ndim(iris)\n\n[1] 150   5\n\nnrow(iris)\n\n[1] 150\n\nncol(iris)\n\n[1] 5\n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\""
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#view-the-beginning-or-end-of-a-dataset",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#view-the-beginning-or-end-of-a-dataset",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "View the beginning or end of a dataset",
    "text": "View the beginning or end of a dataset\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntail(iris)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#specify-how-many-rows-to-view-at-beginning-or-end-of-a-dataset",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#specify-how-many-rows-to-view-at-beginning-or-end-of-a-dataset",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Specify how many rows to view at beginning or end of a dataset",
    "text": "Specify how many rows to view at beginning or end of a dataset\n\nhead(iris, 3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\ntail(iris, 2)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#the",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#the",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "The $",
    "text": "The $\n\nSuppose we want to single out the column of petal width values.\nOne way to do this is to use the $\n\nDatSetName$VariableName\n\n\n\niris$Petal.Width\n\n  [1] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 0.2 0.2 0.1 0.1 0.2 0.4 0.4 0.3\n [19] 0.3 0.3 0.2 0.4 0.2 0.5 0.2 0.2 0.4 0.2 0.2 0.2 0.2 0.4 0.1 0.2 0.2 0.2\n [37] 0.2 0.1 0.2 0.2 0.3 0.3 0.2 0.6 0.4 0.3 0.2 0.2 0.2 0.2 1.4 1.5 1.5 1.3\n [55] 1.5 1.3 1.6 1.0 1.3 1.4 1.0 1.5 1.0 1.4 1.3 1.4 1.5 1.0 1.5 1.1 1.8 1.3\n [73] 1.5 1.2 1.3 1.4 1.4 1.7 1.5 1.0 1.1 1.0 1.2 1.6 1.5 1.6 1.5 1.3 1.3 1.3\n [91] 1.2 1.4 1.2 1.0 1.3 1.2 1.3 1.3 1.1 1.3 2.5 1.9 2.1 1.8 2.2 2.1 1.7 1.8\n[109] 1.8 2.5 2.0 1.9 2.1 2.0 2.4 2.3 1.8 2.2 2.3 1.5 2.3 2.0 2.0 1.8 2.1 1.8\n[127] 1.8 1.8 2.1 1.6 1.9 2.0 2.2 1.5 1.4 2.3 2.4 1.8 1.8 2.1 2.4 2.3 1.9 2.3\n[145] 2.5 2.3 1.9 2.0 2.3 1.8"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#example-using-the",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#example-using-the",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Example using the $",
    "text": "Example using the $\nThe $ is helpful if you want to create a new dataset for just that one variable, or, more commonly, if you want to calculate summary statistics for that one variable.\n\n\n\nmean(iris$Petal.Width)\n\n[1] 1.199333\n\nsd(iris$Petal.Width)\n\n[1] 0.7622377\n\nmedian(iris$Petal.Width)\n\n[1] 1.3"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#inline-code",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#inline-code",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Inline code",
    "text": "Inline code\n\n\nWith markdown you can also report R code output inline with the text instead of using a chunk.\n\n\n\nText in editor:\n\n\n\n\n\n\nOutput:\nThe mean petal width for all 3 species combined is 1.2 (SD = 0.8) cm.\n\n\n\nReporting summary statistics this way in a report, makes the numbers computationally reproducible.\nFor example, if this were for an abstract and a year later you are wondering where the numbers came from, your R code will tell you exactly which dataset was used to calculate the values."
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#table-1-example",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#table-1-example",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Table 1 example",
    "text": "Table 1 example\n\n\n\n\n\n\n\n\nAre We on the Same Page?: A Cross-Sectional Study of Patient-Clinician Goal Concordance in Rheumatoid Arthritis\nJ Barton et al.\nArthritis Care & Research.\n2021 Sep 27 https://pubmed.ncbi.nlm.nih.gov/34569172/"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-center-mean",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-center-mean",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Measures of center: mean",
    "text": "Measures of center: mean\n\nSample mean: the average value of observations\n\\[\\overline{x} = \\frac{x_1+x_2+\\cdots+x_n}{n} = \\sum_{i=1}^{n}\\frac{x_i}{n}\\]\nwhere \\(x_1, x_2, \\ldots, x_n\\) represent the \\(n\\) observed values in a sample\nExample: What is the mean age in the toy dataset df defined earlier?\n\n\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\nmean(df$age)\n\n[1] 31.5"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-center-median",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-center-median",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Measures of center: median",
    "text": "Measures of center: median\n\n\nThe median is the middle value of the observations in a sample.\nThe median is the 50th percentile, meaning\n\n50% of observations lie below and\n50% of observations lie above the median.\n\n\n\n\n\n\n\nIf the number of observations is\n\nodd: the median is the middle observed value\neven: the median is the average of the two middle observed values\n\n\n\n\n\ndf$age\n\n[1] 28.0 35.5 31.0\n\nmedian(df$age)\n\n[1] 31\n\nmedian(c(df$age, 67))\n\n[1] 33.25"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-center-mean-vs.-median",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-center-mean-vs.-median",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Measures of center: mean vs. median",
    "text": "Measures of center: mean vs. median\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-center-mode",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-center-mode",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Measures of center: mode",
    "text": "Measures of center: mode\nmode: the most frequent value in a dataset\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-spread-standard-deviation-sd-13",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-spread-standard-deviation-sd-13",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Measures of spread: standard deviation (SD) (1/3)",
    "text": "Measures of spread: standard deviation (SD) (1/3)\nstandard deviation is (approximately) the average distance between a typical observation and the mean\n\nAn observation’s deviation is the distance between its value \\(x\\) and the sample mean \\(\\overline{x}\\): deviation = \\(x - \\overline{x}\\).\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-spread-sd-23",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-spread-sd-23",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Measures of spread: SD (2/3)",
    "text": "Measures of spread: SD (2/3)\n\nThe sample variance \\(s^2\\) is the sum of squared deviations divided by the number of observations minus 1. \\[s^2 = \\frac{(x_1 - \\overline{x})^2+(x_2 - \\overline{x})^2+\\cdots+(x_n - \\overline{x})^2}{n-1} = \\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}\\] where \\(x_1, x_2, \\dots, x_n\\) represent the \\(n\\) observed values.\nThe standard deviation \\(s\\) is the square root of the variance. \\[s = \\sqrt{\\frac{({x_1 - \\overline{x})}^{2}+({x_2 - \\overline{x})}^{2}+\\cdots+({x_n - \\overline{x})}^{2}}{n-1}} = \\sqrt{\\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}}\\]"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-spread-sd-33",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-spread-sd-33",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Measures of spread: SD (3/3)",
    "text": "Measures of spread: SD (3/3)\n\n\n\nLet’s calculate the sample standard deviation for our toy example\n\n\ndf$age\n\n[1] 28.0 35.5 31.0\n\n\n\n\nmean(df$age)\n\n[1] 31.5\n\nsd(df$age)\n\n[1] 3.774917\n\n\n\n\n\\(s = \\sqrt{\\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}} =\\)"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#empirical-rule-one-way-to-think-about-the-sd-12",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#empirical-rule-one-way-to-think-about-the-sd-12",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Empirical Rule: one way to think about the SD (1/2)",
    "text": "Empirical Rule: one way to think about the SD (1/2)\n\n\n\nFor symmetric bell-shaped data, about\n\n68% of the data are within 1 SD of the mean\n95% of the data are within 2 SD’s of the mean\n99.7% of the data are within 3 SD’s of the mean\n\nThese percentages are based off of percentages of a true normal distribution.\n\n\n\n\nhttps://statistics-made-easy.com/empirical-rule/"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#empirical-rule-one-way-to-think-about-the-sd-22",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#empirical-rule-one-way-to-think-about-the-sd-22",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Empirical Rule: one way to think about the SD (2/2)",
    "text": "Empirical Rule: one way to think about the SD (2/2)\n\n\n\nhist(iris$Sepal.Width)\n\n\n\n\n\n\nmean(iris$Sepal.Width)\n\n[1] 3.057333\n\nsd(iris$Sepal.Width)\n\n[1] 0.4358663"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-spread-interquartile-range-iqr-12",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-spread-interquartile-range-iqr-12",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Measures of spread: interquartile range (IQR) (1/2)",
    "text": "Measures of spread: interquartile range (IQR) (1/2)\nThe \\(p^{th}\\) percentile is the observation such that \\(p\\%\\) of the remaining observations fall below this observation.\n\nThe first quartile \\(Q_1\\) is the \\(25^{th}\\) percentile.\nThe second quartile \\(Q_2\\), i.e., the median, is the \\(50^{th}\\) percentile.\nThe third quartile \\(Q_3\\) is the \\(75^{th}\\) percentile.\n\nThe interquartile range (IQR) is the distance between the third and first quartiles. \\[IQR = Q_3 - Q_1\\]\n\nIQR is the width of the middle half of the data"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-spread-iqr-22",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#measures-of-spread-iqr-22",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Measures of spread: IQR (2/2)",
    "text": "Measures of spread: IQR (2/2)\n5 number summary\n\nsummary(iris$Sepal.Width)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   2.800   3.000   3.057   3.300   4.400 \n\n\n\n\n\n\n\n\n\n\nWhat is the IQR of the sepal widths?\n\nquantile(iris$Sepal.Width, c(.25, .75))\n\n25% 75% \n2.8 3.3 \n\ndiff(quantile(iris$Sepal.Width, c(.25, .75)))\n\n75% \n0.5 \n\nIQR(iris$Sepal.Width)\n\n[1] 0.5"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#robust-estimates",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#robust-estimates",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Robust estimates",
    "text": "Robust estimates\nSummary statistics are called robust estimates if extreme observations have little effect on their values\n\n\n\nestimate\nrobust?\n\n\n\n\nmean\n\n\n\nmedian\n\n\n\nmode\n\n\n\nstandard deviaiton\n\n\n\nIQR\n\n\n\nrange"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#r-packages-1",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#r-packages-1",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "R Packages",
    "text": "R Packages\nA good analogy for R packages is that they\nare like apps you can download onto a mobile phone:\n\n\n\nModernDive Figure 1.4"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#installing-packages",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#installing-packages",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Installing packages",
    "text": "Installing packages\n\n\nPackages contain additional functions and data\n\nTwo options to install packages:\n\ninstall.packages() or\nThe “Packages” tab in Files/Plots/Packages/Help/Viewer window\n\n\n\ninstall.packages(\"dplyr\")   # only do this ONCE, use quotes\n\n\n\n\n\nOnly install packages once (unless you want to update them)\nInstalled from Comprehensive R Archive Network (CRAN) = package mothership"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#video-on-installing-packages",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#video-on-installing-packages",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Video on installing packages",
    "text": "Video on installing packages\n\nDanielle Navarro’s YouTube video on Installing and loading R packages: https://www.youtube.com/watch?v=kpHZVyDvEhQ"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#load-packages-with-library-command",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#load-packages-with-library-command",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Load packages with library() command",
    "text": "Load packages with library() command\n\n\nTip: at the top of your Rmd file, create a chunk that loads all of the R packages you want to use in that file.\nUse the library() command to load each required package.\nPackages need to be reloaded every time you open Rstudio.\n\n\n\nlibrary(dplyr)    # run this every time you open Rstudio\n\n\n\nYou can use a function without loading the package with PackageName::CommandName\n\n\n\ndplyr::arrange(iris, Petal.Width)   # what does arrange do?\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            4.9         3.1          1.5         0.1     setosa\n2            4.8         3.0          1.4         0.1     setosa\n3            4.3         3.0          1.1         0.1     setosa\n4            5.2         4.1          1.5         0.1     setosa\n5            4.9         3.6          1.4         0.1     setosa\n6            5.1         3.5          1.4         0.2     setosa\n7            4.9         3.0          1.4         0.2     setosa\n8            4.7         3.2          1.3         0.2     setosa\n9            4.6         3.1          1.5         0.2     setosa\n10           5.0         3.6          1.4         0.2     setosa\n11           5.0         3.4          1.5         0.2     setosa\n12           4.4         2.9          1.4         0.2     setosa\n13           5.4         3.7          1.5         0.2     setosa\n14           4.8         3.4          1.6         0.2     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.4         3.4          1.7         0.2     setosa\n17           4.6         3.6          1.0         0.2     setosa\n18           4.8         3.4          1.9         0.2     setosa\n19           5.0         3.0          1.6         0.2     setosa\n20           5.2         3.5          1.5         0.2     setosa\n21           5.2         3.4          1.4         0.2     setosa\n22           4.7         3.2          1.6         0.2     setosa\n23           4.8         3.1          1.6         0.2     setosa\n24           5.5         4.2          1.4         0.2     setosa\n25           4.9         3.1          1.5         0.2     setosa\n26           5.0         3.2          1.2         0.2     setosa\n27           5.5         3.5          1.3         0.2     setosa\n28           4.4         3.0          1.3         0.2     setosa\n29           5.1         3.4          1.5         0.2     setosa\n30           4.4         3.2          1.3         0.2     setosa\n31           5.1         3.8          1.6         0.2     setosa\n32           4.6         3.2          1.4         0.2     setosa\n33           5.3         3.7          1.5         0.2     setosa\n34           5.0         3.3          1.4         0.2     setosa\n35           4.6         3.4          1.4         0.3     setosa\n36           5.1         3.5          1.4         0.3     setosa\n37           5.7         3.8          1.7         0.3     setosa\n38           5.1         3.8          1.5         0.3     setosa\n39           5.0         3.5          1.3         0.3     setosa\n40           4.5         2.3          1.3         0.3     setosa\n41           4.8         3.0          1.4         0.3     setosa\n42           5.4         3.9          1.7         0.4     setosa\n43           5.7         4.4          1.5         0.4     setosa\n44           5.4         3.9          1.3         0.4     setosa\n45           5.1         3.7          1.5         0.4     setosa\n46           5.0         3.4          1.6         0.4     setosa\n47           5.4         3.4          1.5         0.4     setosa\n48           5.1         3.8          1.9         0.4     setosa\n49           5.1         3.3          1.7         0.5     setosa\n50           5.0         3.5          1.6         0.6     setosa\n51           4.9         2.4          3.3         1.0 versicolor\n52           5.0         2.0          3.5         1.0 versicolor\n53           6.0         2.2          4.0         1.0 versicolor\n54           5.8         2.7          4.1         1.0 versicolor\n55           5.7         2.6          3.5         1.0 versicolor\n56           5.5         2.4          3.7         1.0 versicolor\n57           5.0         2.3          3.3         1.0 versicolor\n58           5.6         2.5          3.9         1.1 versicolor\n59           5.5         2.4          3.8         1.1 versicolor\n60           5.1         2.5          3.0         1.1 versicolor\n61           6.1         2.8          4.7         1.2 versicolor\n62           5.8         2.7          3.9         1.2 versicolor\n63           5.5         2.6          4.4         1.2 versicolor\n64           5.8         2.6          4.0         1.2 versicolor\n65           5.7         3.0          4.2         1.2 versicolor\n66           5.5         2.3          4.0         1.3 versicolor\n67           5.7         2.8          4.5         1.3 versicolor\n68           6.6         2.9          4.6         1.3 versicolor\n69           5.6         2.9          3.6         1.3 versicolor\n70           6.1         2.8          4.0         1.3 versicolor\n71           6.4         2.9          4.3         1.3 versicolor\n72           6.3         2.3          4.4         1.3 versicolor\n73           5.6         3.0          4.1         1.3 versicolor\n74           5.5         2.5          4.0         1.3 versicolor\n75           5.6         2.7          4.2         1.3 versicolor\n76           5.7         2.9          4.2         1.3 versicolor\n77           6.2         2.9          4.3         1.3 versicolor\n78           5.7         2.8          4.1         1.3 versicolor\n79           7.0         3.2          4.7         1.4 versicolor\n80           5.2         2.7          3.9         1.4 versicolor\n81           6.1         2.9          4.7         1.4 versicolor\n82           6.7         3.1          4.4         1.4 versicolor\n83           6.6         3.0          4.4         1.4 versicolor\n84           6.8         2.8          4.8         1.4 versicolor\n85           6.1         3.0          4.6         1.4 versicolor\n86           6.1         2.6          5.6         1.4  virginica\n87           6.4         3.2          4.5         1.5 versicolor\n88           6.9         3.1          4.9         1.5 versicolor\n89           6.5         2.8          4.6         1.5 versicolor\n90           5.9         3.0          4.2         1.5 versicolor\n91           5.6         3.0          4.5         1.5 versicolor\n92           6.2         2.2          4.5         1.5 versicolor\n93           6.3         2.5          4.9         1.5 versicolor\n94           6.0         2.9          4.5         1.5 versicolor\n95           5.4         3.0          4.5         1.5 versicolor\n96           6.7         3.1          4.7         1.5 versicolor\n97           6.0         2.2          5.0         1.5  virginica\n98           6.3         2.8          5.1         1.5  virginica\n99           6.3         3.3          4.7         1.6 versicolor\n100          6.0         2.7          5.1         1.6 versicolor\n101          6.0         3.4          4.5         1.6 versicolor\n102          7.2         3.0          5.8         1.6  virginica\n103          6.7         3.0          5.0         1.7 versicolor\n104          4.9         2.5          4.5         1.7  virginica\n105          5.9         3.2          4.8         1.8 versicolor\n106          6.3         2.9          5.6         1.8  virginica\n107          7.3         2.9          6.3         1.8  virginica\n108          6.7         2.5          5.8         1.8  virginica\n109          6.5         3.0          5.5         1.8  virginica\n110          6.3         2.7          4.9         1.8  virginica\n111          7.2         3.2          6.0         1.8  virginica\n112          6.2         2.8          4.8         1.8  virginica\n113          6.1         3.0          4.9         1.8  virginica\n114          6.4         3.1          5.5         1.8  virginica\n115          6.0         3.0          4.8         1.8  virginica\n116          5.9         3.0          5.1         1.8  virginica\n117          5.8         2.7          5.1         1.9  virginica\n118          6.4         2.7          5.3         1.9  virginica\n119          7.4         2.8          6.1         1.9  virginica\n120          5.8         2.7          5.1         1.9  virginica\n121          6.3         2.5          5.0         1.9  virginica\n122          6.5         3.2          5.1         2.0  virginica\n123          5.7         2.5          5.0         2.0  virginica\n124          5.6         2.8          4.9         2.0  virginica\n125          7.7         2.8          6.7         2.0  virginica\n126          7.9         3.8          6.4         2.0  virginica\n127          6.5         3.0          5.2         2.0  virginica\n128          7.1         3.0          5.9         2.1  virginica\n129          7.6         3.0          6.6         2.1  virginica\n130          6.8         3.0          5.5         2.1  virginica\n131          6.7         3.3          5.7         2.1  virginica\n132          6.4         2.8          5.6         2.1  virginica\n133          6.9         3.1          5.4         2.1  virginica\n134          6.5         3.0          5.8         2.2  virginica\n135          7.7         3.8          6.7         2.2  virginica\n136          6.4         2.8          5.6         2.2  virginica\n137          6.4         3.2          5.3         2.3  virginica\n138          7.7         2.6          6.9         2.3  virginica\n139          6.9         3.2          5.7         2.3  virginica\n140          7.7         3.0          6.1         2.3  virginica\n141          6.9         3.1          5.1         2.3  virginica\n142          6.8         3.2          5.9         2.3  virginica\n143          6.7         3.0          5.2         2.3  virginica\n144          6.2         3.4          5.4         2.3  virginica\n145          5.8         2.8          5.1         2.4  virginica\n146          6.3         3.4          5.6         2.4  virginica\n147          6.7         3.1          5.6         2.4  virginica\n148          6.3         3.3          6.0         2.5  virginica\n149          7.2         3.6          6.1         2.5  virginica\n150          6.7         3.3          5.7         2.5  virginica"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#install-the-packages-listed-below-before-day-3",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#install-the-packages-listed-below-before-day-3",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Install the packages listed below before Day 3",
    "text": "Install the packages listed below before Day 3\n\nknitr\n\nthis might actually already be installed\ncheck your packages list\n\ntidyverse\n\nthis is actually a bundle of packages\nWarning: it will take a while to install!!!\nsee more info at https://tidyverse.tidyverse.org/\n\nrstatix\n\nfor summary statistics of a dataset\n\njanitor\n\nfor cleaning and exploring data\n\nggridges\n\nfor creating ridgeline plots\n\ndevtools\n\nused to create R packages\nfor our purposes, needed to install some packages\n\noi_biostat_data\n\nthis package is on github\nsee the next slide for directions on how to install oi_biostat_data"
  },
  {
    "objectID": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#directions-for-installing-package-oibiostat",
    "href": "lessons/02_Data_collection_num_summaries/02_Data_collection_num_summaries.html#directions-for-installing-package-oibiostat",
    "title": "Lesson 2: Data collection & numerical summaries",
    "section": "Directions for installing package oibiostat",
    "text": "Directions for installing package oibiostat\n\n\nThe textbook’s datasets are in the R package oibiostat\nExplanation of code below\n\nInstallation of oibiostat package requires first installing devtools package\nThe code devtools::install_github() tells R to use the command install_github() from the devtools package without loading the entire package and all of its commands (which library(devtools) would do).\n\n\n\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"OI-Biostat/oi_biostat_data\", force = TRUE)\n\n\n\nAfter running the code above, put # in front of the commands so that RStudio doesn’t evaluate them when rendering.\nNow load the oibiostat package\n\nthe code below needs to be run every time you restart R or knit an Rmd file\n\n\n\n\nlibrary(oibiostat)"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "",
    "text": "Add text to a plot using annotate():\n\nggplot(NULL, aes(c(0,4))) +  # no dataset, create axes for x from 0 to 4\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2), \n            fill = \"blue\", xlim = c(0, 1.0414)) +\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2),\n            fill = \"violet\", xlim = c(1.0414, 4)) +\n  geom_vline(xintercept = 1.0414) +  # vertical line at x = 1.0414\n  annotate(\"text\", x = 1.1, y = .4, # add text at specified (x,y) coordinate\n           label = \"chi-squared = 1.0414\", hjust=0, size=6) + \n  annotate(\"text\", x = 1.3, y = .1, \n           label = \"p-value = 0.59\", hjust=0, size=6)"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#moritzs-tip-of-the-day",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#moritzs-tip-of-the-day",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "",
    "text": "Add text to a plot using annotate():\n\nggplot(NULL, aes(c(0,4))) +  # no dataset, create axes for x from 0 to 4\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2), \n            fill = \"blue\", xlim = c(0, 1.0414)) +\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2),\n            fill = \"violet\", xlim = c(1.0414, 4)) +\n  geom_vline(xintercept = 1.0414) +  # vertical line at x = 1.0414\n  annotate(\"text\", x = 1.1, y = .4, # add text at specified (x,y) coordinate\n           label = \"chi-squared = 1.0414\", hjust=0, size=6) + \n  annotate(\"text\", x = 1.3, y = .1, \n           label = \"p-value = 0.59\", hjust=0, size=6)"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#where-are-we",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#where-are-we",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#where-are-we-categorical-outcome-zoomed-in",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#where-are-we-categorical-outcome-zoomed-in",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Where are we? Categorical outcome zoomed in",
    "text": "Where are we? Categorical outcome zoomed in"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#goals-for-today-sections-8.3-8.4",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#goals-for-today-sections-8.3-8.4",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Goals for today (Sections 8.3-8.4)",
    "text": "Goals for today (Sections 8.3-8.4)\n\nStatistical inference for categorical data when either are\n\ncomparing more than two groups,\nor have categorical outcomes that have more than 2 levels,\nor both\n\nChi-squared tests of association (independence)\n\nHypotheses\ntest statistic\nChi-squared distribution\np-value\ntechnical conditions (assumptions)\nconclusion\nR: chisq.test()\n\nFisher’s Exact Test\nChi-squared test vs. testing difference in proportions\n\nTest of Homogeneity"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#is-there-an-association-between-depression-and-being-physically-active",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#is-there-an-association-between-depression-and-being-physically-active",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Is there an association between depression and being physically active?",
    "text": "Is there an association between depression and being physically active?\n\nData sampled from the NHANES R package:\n\nAmerican National Health and Nutrition Examination Surveys\nCollected 2009-2012 by US National Center for Health Statistics (NCHS)\nNHANES dataset: 10,000 rows, resampled from NHANESraw to undo oversampling effects\n\nTreat it as a simple random sample from the US population (for pedagogical purposes)\n\n\nDepressed\n\nSelf-reported number of days where participant felt down, depressed or hopeless.\nOne of None, Several, or Most (more than half the days).\nReported for participants aged 18 years or older.\n\nPhysActive\n\nParticipant does moderate or vigorous-intensity sports, fitness or recreational activities (Yes or No).\nReported for participants 12 years or older."
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#hypotheses-for-a-chi-squared-test-of-association-independence",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#hypotheses-for-a-chi-squared-test-of-association-independence",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Hypotheses for a Chi-squared test of association (independence)",
    "text": "Hypotheses for a Chi-squared test of association (independence)\n\n\nGeneric wording:\nTest of “association” wording\n\n\\(H_0\\): There is no association between the two variables\n\\(H_A\\): There is an association between the two variables\n\nTest of “independence” wording\n\n\\(H_0\\): The variables are independent\n\\(H_A\\): The variables are not independent\n\n\n\n\nFor our example:\nTest of “association” wording\n\n\\(H_0\\): There is no association between depression and physical activity\n\\(H_A\\): There is an association between depression and physical activity\n\nTest of “independence” wording\n\n\\(H_0\\): The variables depression and physical activity are independent\n\\(H_A\\): The variables depression and physical activity are not independent\n\n\n\n\n\n\n\n\n\nNo symbols\n\n\n\nFor chi-squared test hypotheses we do not have versions using “symbols” like we do with tests of means or proportions."
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#data-from-nhanes",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#data-from-nhanes",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Data from NHANES",
    "text": "Data from NHANES\n\nResults below are from\n\na random sample of 400 adults (≥ 18 yrs old)\nwith data for both the depression Depressed and physically active (PhysActive) variables.\n\n\n\n\n\n\n\n\nWhat does it mean for the variables to be independent?"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#h_0-variables-are-independent",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#h_0-variables-are-independent",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(H_0\\): Variables are Independent",
    "text": "\\(H_0\\): Variables are Independent\n\n\n\nRecall from Chapter 2, that events \\(A\\) and \\(B\\) are independent if and only if\n\n\\[P(A~and~B)=P(A)P(B)\\]\n\nIf depression and being physically active are independent variables, then theoretically this condition needs to hold for every combination of levels, i.e.\n\n\n\\[\\begin{align}\nP(None~and~Yes) &= P(None)P(Yes)\\\\\nP(None~and~No) &= P(None)P(No)\\\\\nP(Several~and~Yes) &= P(Several)P(Yes)\\\\\nP(Several~and~No) &= P(Several)P(No)\\\\\nP(Most~and~Yes) &= P(Most)P(Yes)\\\\\nP(Most~and~No) &= P(Most)P(No)\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\nP(None~and~Yes) &= \\frac{314}{400}\\cdot\\frac{226}{400}\\\\\n& ...\\\\\nP(Most~and~No) &= \\frac{28}{400}\\cdot\\frac{174}{400}\n\\end{align}\\]\n\nWith these probabilities, for each cell of the table we calculate the expected counts for each cell under the \\(H_0\\) hypothesis that the variables are independent"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#expected-counts-if-variables-are-independent",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#expected-counts-if-variables-are-independent",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Expected counts (if variables are independent)",
    "text": "Expected counts (if variables are independent)\n\n\n\nThe expected counts (if \\(H_0\\) is true & the variables are independent) for each cell are\n\n\\(np\\) = total table size \\(\\cdot\\) probability of cell\n\n\nExpected count of Yes & None:\n\\[\\begin{align}\n400 \\cdot & P(None~and~Yes)\\\\\n&= 400 \\cdot P(None)P(Yes)\\\\\n&= 400 \\cdot\\frac{314}{400}\\cdot\\frac{226}{400}\\\\\n&= \\frac{314\\cdot 226}{400} \\\\\n&=  177.41\\\\\n&= \\frac{\\text{column total}\\cdot \\text{row total}}{\\text{table total}}\n\\end{align}\\]\n\n\n\n\n\n\n\nIf depression and being physically active are independent variables\n\n(as assumed by \\(H_0\\)),\n\nthen the observed counts should be close to the expected counts for each cell of the table"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#observed-vs.-expected-counts",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#observed-vs.-expected-counts",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Observed vs. Expected counts",
    "text": "Observed vs. Expected counts\n\n\n\nThe observed counts are the counts in the 2-way table summarizing the data\n\n\n\n\n\n\n\nExpected count for cell \\(i,j\\) :\n\n\nThe expected counts are the counts the we would expect to see in the 2-way table if there was no association between depression and being physically activity\n\n\n\n\n\n\n\n\n\\[\\textrm{Expected Count}_{\\textrm{row } i,\\textrm{ col }j}=\\frac{(\\textrm{row}~i~ \\textrm{total})\\cdot(\\textrm{column}~j~ \\textrm{total})}{\\textrm{table total}}\\]"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#the-chi2-test-statistic",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#the-chi2-test-statistic",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "The \\(\\chi^2\\) test statistic",
    "text": "The \\(\\chi^2\\) test statistic\n\n\nTest statistic for a test of association (independence):\n\\[\\chi^2 = \\sum_{\\textrm{all cells}} \\frac{(\\textrm{observed} - \\text{expected})^2}{\\text{expected}}\\]\n\nWhen the variables are independent, the observed and expected counts should be close to each other\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n\\chi^2 &=  \\sum\\frac{(O-E)^2}{E} \\\\\n&= \\frac{(199-177.41)^2}{177.41} + \\frac{(26-32.77)^2}{32.77} + \\ldots + \\frac{(27-12.18)^2}{12.18} \\\\\n&=  41.2\n\\end{align}\\]\n\nIs this value big? Big enough to reject \\(H_0\\)?"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#the-chi2-distribution-calculating-the-p-value",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#the-chi2-distribution-calculating-the-p-value",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "The \\(\\chi^2\\) distribution & calculating the p-value",
    "text": "The \\(\\chi^2\\) distribution & calculating the p-value\n\n\nThe \\(\\chi^2\\) distribution shape depends on its degrees of freedom\n\nIt’s skewed right for smaller df,\n\ngets more symmetric for larger df\n\ndf = (# rows-1) x (# columns-1)\n\n\n\n\n\n\n\n\nThe p-value is always the area to the right of the test statistic for a \\(\\chi^2\\) test.\nWe can use the pchisq function in R to calculate the probability of being at least as big as the \\(\\chi^2\\) test statistic:\n\n\npv &lt;- pchisq(41.2, df = 2, \n       lower.tail = FALSE)\npv\n\n[1] 1.131185e-09\n\n\nWhat’s the conclusion to the \\(\\chi^2\\) test?"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#conclusion",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#conclusion",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Conclusion",
    "text": "Conclusion\nRecall the hypotheses to our \\(\\chi^2\\) test:\n\n\\(H_0\\): There is no association between depression and being physically activity\n\\(H_A\\): There is an association between depression and being physically activity\n\n\n\n\n\n\n\n\n\nConclusion:\nBased a random sample of 400 US adults from 2009-2012, there is sufficient evidence that there is an association between depression and being physically activity (p-value &lt; 0.001).\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf we fail to reject, we DO NOT have evidence of no association."
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#technical-conditions",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#technical-conditions",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Technical conditions",
    "text": "Technical conditions\n\nIndependence\n\nEach case (person) that contributes a count to the table must be independent of all the other cases in the table\n\nIn particular, observational units cannot be represented in more than one cell.\nFor example, someone cannot choose both “Several” and “Most” for depression status. They have to choose exactly one option for each variable.\n\n\n\n\n\n\n\nSample size\n\nIn order for the distribution of the test statistic to be appropriately modeled by a chi-squared distribution we need\n2 \\(\\times\\) 2 table:\n\nexpected counts are at least 10 for each cell\n\nlarger tables:\n\nno more than 1/5 of the expected counts are less than 5, and\nall expected counts are greater than 1"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#depression-vs.-physical-activity-dataset",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#depression-vs.-physical-activity-dataset",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Depression vs. physical activity dataset",
    "text": "Depression vs. physical activity dataset\nCreate dataset based on results table:\n\n\n\nDepPA &lt;- tibble(\n  Depression = c(rep(\"None\", 314), \n         rep(\"Several\", 58),\n         rep(\"Most\", 28)),\n  PA = c(rep(\"Yes\", 199),  # None\n          rep(\"No\", 115),\n          rep(\"Yes\", 26), # Several\n          rep(\"No\", 32),\n          rep(\"Yes\", 1), # Most\n          rep(\"No\", 27))\n)\n\n\n\n\n\n\n\n\n\nSummary table of data:\n\n\n\nDepPA %&gt;% \n  tabyl(Depression, PA)\n\n Depression  No Yes\n       Most  27   1\n       None 115 199\n    Several  32  26\n\n\n\n\n# base R:\ntable(DepPA)\n\n          PA\nDepression  No Yes\n   Most     27   1\n   None    115 199\n   Several  32  26"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#chi2-test-in-r-using-dataset",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#chi2-test-in-r-using-dataset",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test in R using dataset",
    "text": "\\(\\chi^2\\) test in R using dataset\n\n\nIf only have 2 columns in the dataset:\n\n(ChisqTest_DepPA &lt;- \n   chisq.test(table(DepPA)))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(DepPA)\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\nIf have &gt;2 columns in the dataset, we need to specify which columns to table:\n\n(ChisqTest_DepPA &lt;- \n   chisq.test(table(\n     DepPA$Depression, DepPA$PA)))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(DepPA$Depression, DepPA$PA)\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\n\nThe tidyverse way (fewer parentheses)\n\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() \n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\ntidy() the output (from broom package):\n\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      parameter\n      method\n    \n  \n  \n    41.17067\n1.147897e-09\n2\nPearson's Chi-squared test\n  \n  \n  \n\n\n\n\nPull p-value\n\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() %&gt;% \n  tidy() %&gt;% pull(p.value)\n\n[1] 1.147897e-09"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#observed-expected-counts-in-r",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#observed-expected-counts-in-r",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Observed & expected counts in R",
    "text": "Observed & expected counts in R\n\n\nYou can see what the observed and expected counts are from the saved chi-squared test results:\n\nChisqTest_DepPA$observed\n\n         \n           No Yes\n  Most     27   1\n  None    115 199\n  Several  32  26\n\nChisqTest_DepPA$expected\n\n         \n              No    Yes\n  Most     12.18  15.82\n  None    136.59 177.41\n  Several  25.23  32.77\n\n\n\n\n\n\n\n\n\nWhy is it important to look at the expected counts?\nWhat are we looking for in the expected counts?"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#chi2-test-in-r-with-2-way-table",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#chi2-test-in-r-with-2-way-table",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test in R with 2-way table",
    "text": "\\(\\chi^2\\) test in R with 2-way table\nCreate a base R table of the results:\n\n(DepPA_table &lt;- matrix(c(199, 26, 1, 115, 32, 27), nrow = 2, ncol = 3, byrow = T))\n\n     [,1] [,2] [,3]\n[1,]  199   26    1\n[2,]  115   32   27\n\ndimnames(DepPA_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA_table\n\n     Depression\nPA    None Several Most\n  Yes  199      26    1\n  No   115      32   27\n\n\nRun \\(\\chi^2\\) test with 2-way table:\n\nchisq.test(DepPA_table) \n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA_table\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\nchisq.test(DepPA_table)$expected\n\n     Depression\nPA      None Several  Most\n  Yes 177.41   32.77 15.82\n  No  136.59   25.23 12.18"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#yates-continuity-correction",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#yates-continuity-correction",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "(Yates’) Continuity correction",
    "text": "(Yates’) Continuity correction\n\nFor a 2x2 contingency table,\n\nthe \\(\\chi^2\\) test has the option of including a continuity correction\njust like with the proportions test\n\nThe default includes a continuity correction\nThere is no CC for bigger tables\n\n\n(DepPA_table2x2 &lt;- matrix(c(199, 27, 115, 59), nrow = 2, ncol = 2, byrow = T))\n\n     [,1] [,2]\n[1,]  199   27\n[2,]  115   59\n\ndimnames(DepPA_table2x2) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several/Most\"))  # column names\nDepPA_table2x2\n\n     Depression\nPA    None Several/Most\n  Yes  199           27\n  No   115           59\n\n\n\n\nOutput without a CC\n\nchisq.test(DepPA_table2x2, correct = FALSE) \n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA_table2x2\nX-squared = 28.093, df = 1, p-value = 1.156e-07\n\n\n\nCompare to output with CC:\n\nchisq.test(DepPA_table2x2) \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  DepPA_table2x2\nX-squared = 26.807, df = 1, p-value = 2.248e-07"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#example-with-smaller-sample-size",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#example-with-smaller-sample-size",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Example with smaller sample size",
    "text": "Example with smaller sample size\n\nSuppose that instead of taking a random sample of 400 adults (from the NHANES data), a study takes a random sample of 100 such that\n\n50 people that are physically active and\n50 people that are not physically active\n\n\n\n(DepPA100_table &lt;- matrix(c(43, 5, 2, 40, 4, 6), nrow = 2, ncol = 3, byrow = T))\n\n     [,1] [,2] [,3]\n[1,]   43    5    2\n[2,]   40    4    6\n\ndimnames(DepPA100_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\n\nDepPA100_table\n\n     Depression\nPA    None Several Most\n  Yes   43       5    2\n  No    40       4    6"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#chi-squared-test-warning",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#chi-squared-test-warning",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Chi-squared test warning",
    "text": "Chi-squared test warning\n\nchisq.test(DepPA100_table) \n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA100_table\nX-squared = 2.2195, df = 2, p-value = 0.3296\n\nchisq.test(DepPA100_table)$expected\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n     Depression\nPA    None Several Most\n  Yes 41.5     4.5    4\n  No  41.5     4.5    4\n\n\n\nRecall the sample size condition\n\nIn order for the test statistic to be modeled by a chi-squared distribution we need\n2 \\(\\times\\) 2 table: expected counts are at least 10 for each cell\nlarger tables:\n\nno more than 1/5 of the expected counts are less than 5, and\nall expected counts are greater than 1"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#fishers-exact-test",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#fishers-exact-test",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\n\nCalled an exact test since it\n\ncalculates an exact probability for the p-value\n\ninstead of using an asymptotic approximation, such as the normal, t, or chi-squared distributions\n\nFor 2x2 tables the p-value is calculated using the hypergeometric probability distribution (see book for details)\n\n\n\nfisher.test(DepPA100_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  DepPA100_table\np-value = 0.3844\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nComments\n\n\n\n\nNote that there is no test statistic\nThere is also no CI\nThis is always a two-sided test\nThere is no continuity correction since the hypergeometric distribution is discrete"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#simulate-p-values-another-option-for-small-expected-counts",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#simulate-p-values-another-option-for-small-expected-counts",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Simulate p-values: another option for small expected counts",
    "text": "Simulate p-values: another option for small expected counts\nFrom the chisq.test help file:\n\nSimulation is done by random sampling from the set of all contingency tables with the same margin totals\n\nworks only if the margin totals are strictly positive.\n\nFor each simulation, a \\(\\chi^2\\) test statistic is calculated\nP-value is the proportion of simulations that have a test statistic at least as big as the observed one.\nNo continuity correction\n\n\nset.seed(567)\nchisq.test(DepPA100_table, simulate.p.value = TRUE) \n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  DepPA100_table\nX-squared = 2.2195, df = NA, p-value = 0.3893"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#chi2-test-vs.-testing-differences-in-proportions",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#chi2-test-vs.-testing-differences-in-proportions",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test vs. testing differences in proportions",
    "text": "\\(\\chi^2\\) test vs. testing differences in proportions\nIf there are only 2 levels in both of the categorical variables being tested, then the p-value from the \\(\\chi^2\\) test is equal to the p-value from the differences in proportions test.\n\n\nExample: Previously we tested whether the proportion who had participated in sports betting was the same for college and noncollege young adults:\n\\[\\begin{align}\nH_0:& ~p_{coll} - p_{noncoll} = 0\\\\\nH_A:& ~p_{coll} - p_{noncoll} \\neq 0\n\\end{align}\\]\n\n\n\nSportsBet_table &lt;- matrix(\n  c(175, 94, 137, 77), \n  nrow = 2, ncol = 2, byrow = T)\n\ndimnames(SportsBet_table) &lt;- list(\n  \"Group\" = c(\"College\", \"NonCollege\"), # row names\n  \"Bet\" = c(\"No\", \"Yes\"))  # column names\n\nSportsBet_table\n\n            Bet\nGroup         No Yes\n  College    175  94\n  NonCollege 137  77\n\n\n\n\n\n\nchisq.test(SportsBet_table) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      parameter\n      method\n    \n  \n  \n    0.01987511\n0.8878864\n1\nPearson's Chi-squared test with Yates' continuity correction\n  \n  \n  \n\n\n\nprop.test(SportsBet_table) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.6505576\n0.6401869\n0.01987511\n0.8878864\n1\n-0.07973918\n0.1004806\n2-sample test for equality of proportions with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n2*pnorm(sqrt(0.0199), lower.tail=F) # p-value\n\n[1] 0.8878167"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#test-of-homogeneity",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#test-of-homogeneity",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Test of Homogeneity",
    "text": "Test of Homogeneity\n\nRunning the sports betting example as a chi-squared test is actually an example of a test of homogeneity\nIn a test of homogeneity, proportions can be compared between many groups\n\n\\[\\begin{align}\nH_0:&~ p_1 = p_2 = p_2 = \\ldots = p_n\\\\\nH_A:&~ p_i \\neq p_j \\textrm{for at least one pair of } i, j\n\\end{align}\\]\n\nIt’s an extension of a two proportions test.\nThe test statistic & p-value are calculated the same was as a chi-squared test of association (independence)\nWhen we fix the margins (whether row or columns) of one of the “variables” (such as in a cohort or case-control study)\n\nthe chi-squared test is called a Test of Homogeneity"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#overview-of-tests-with-categorical-outcome",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#overview-of-tests-with-categorical-outcome",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Overview of tests with categorical outcome",
    "text": "Overview of tests with categorical outcome"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#chi-squared-tests-of-independence-vs.-homogeneity-vs.-goodness-of-fit",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#chi-squared-tests-of-independence-vs.-homogeneity-vs.-goodness-of-fit",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Chi-squared Tests of Independence vs. Homogeneity vs. Goodness-of-fit",
    "text": "Chi-squared Tests of Independence vs. Homogeneity vs. Goodness-of-fit\n\n\n\n\n\n\nSee YouTube video from TileStats for a good explanation of how these three tests are different: https://www.youtube.com/watch?v=TyD-_1JUhxw\nUCLA’s INSPIRE website has a good summary too: http://inspire.stat.ucla.edu/unit_13/"
  },
  {
    "objectID": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#whats-next",
    "href": "lessons/13_Chi_sq_Fisher/13_Chi_sq_Fisher.html#whats-next",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "What’s next?",
    "text": "What’s next?"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#goals-for-today-part-1",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#goals-for-today-part-1",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Goals for today: Part 1",
    "text": "Goals for today: Part 1\n\n(4.3, 5.1) Hypothesis testing for mean from one sample\n\nIntroduce hypothesis testing using the case of analyzing a mean from one sample (group)\n\n\n\n\nSteps of a hypothesis test:\n\nlevel of significance\nnull ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\ntest statistic\np-value\nconclusion\n\n\n\n\nRun a hypothesis test in R\n\nLoad a dataset - need to specify location of dataset\nR projects\nRun a t-test in R\ntidy() the test output using broom package\n\n\n\n\n\n\n(4.3.3) Confidence intervals (CIs) vs. hypothesis tests"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#goals-for-today-part-2---class-discussion",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#goals-for-today-part-2---class-discussion",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Goals for today: Part 2 - Class discussion",
    "text": "Goals for today: Part 2 - Class discussion\n\n(5.2) Inference for mean difference from dependent/paired 2 samples\n\nInference: CIs and hypothesis testing\nExploratory data analysis (EDA) to visualize data\nRun paired t-test in R\n\n\n\nOne-sided CIs\n\n\nClass discussion\n\nInference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.\nThus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.\nIn class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#moritzs-tip-of-the-day-use-r-projects-to-organize-analyses",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#moritzs-tip-of-the-day-use-r-projects-to-organize-analyses",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "MoRitz’s tip of the day: use R projects to organize analyses",
    "text": "MoRitz’s tip of the day: use R projects to organize analyses\n\n\n\n\n\n\n\n\nMoRitz loves using R projects to\n\norganize analyses and\nmake it easier to load data files\nand also save output\n\nOther bonuses include\n\nmaking to it easier to collaborate with others,\nincluding yourself when accessing files from different computers.\n\n\nWe will discuss how to use projects later in today’s slides when loading a dataset.\nSee file Projects in RStudio for more information."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#is-98.6f-really-the-mean-healthy-body-temperature",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#is-98.6f-really-the-mean-healthy-body-temperature",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Is 98.6°F really the mean “healthy” body temperature?",
    "text": "Is 98.6°F really the mean “healthy” body temperature?\n\nWhere did the 98.6°F value come from?\n\nGerman physician Carl Reinhold August Wunderlich determined 98.6°F (or 37°C) based on temperatures from 25,000 patients in Leipzig in 1851.\n\n1992 JAMA article by Mackowiak, Wasserman, & Levine\n\nThey claim that 98.2°F (36.8°C) is a more accurate average body temp\nSample: n = 148 healthy men and women aged 18 - 40 years\n\nIn January 2020, a group from Stanford published Decreasing human body temperature in the United States since the Industrial Revolution in eLIFE.\n\n“determined that mean body temperature in men and women, after adjusting for age, height, weight and, in some models date and time of day, has decreased monotonically by 0.03°C (0.05°F) per birth decade”\nSeptember 2023 update: Defining Usual Oral Temperature Ranges in Outpatients Using an Unsupervised Learning Algorithm in JAMA Internal Medicine\n\nAverage is 36.64 °C (97.95 °F); “range of mean temperatures for the coolest to the warmest individuals was 36.24 °C to 36.89 °C” (97.23 to 98.40 °F); based 2008-2017 data\n“findings suggest that age, sex, height, weight, and time of day are factors that contribute to variations in individualized normal temperature ranges.”\n\n\nNYT article The Average Human Body Temperature Is Not 98.6 Degrees, Oct 12, 2023, by Dana G. Smith\n\nQuestion: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#question-based-on-the-1992-jama-data-is-there-evidence-to-support-that-the-population-mean-body-temperature-is-different-from-98.6f",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#question-based-on-the-1992-jama-data-is-there-evidence-to-support-that-the-population-mean-body-temperature-is-different-from-98.6f",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?",
    "text": "Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?\nTwo approaches to answer this question:\n\nCreate a confidence interval (CI) for the population mean \\(\\mu\\) and determine whether 98.6°F is inside the CI or not.\n\nis 98.6°F a plausible value?\n\nRun a hypothesis test to see if there is evidence that the population mean \\(\\mu\\) is significantly different from 98.6°F or not.\n\nThis does not give us a range of plausible values for the population mean \\(\\mu\\).\nInstead, we calculate a test statistic and p-value\n\nto see how likely we are to observe the sample mean \\(\\bar{x}\\)\nor a more extreme sample mean\nassuming that the population mean \\(\\mu\\) is 98.6°F."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#approach-1-create-a-95-c-i-for-the-population-mean-body-temperature",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#approach-1-create-a-95-c-i-for-the-population-mean-body-temperature",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Approach 1: Create a 95% C I for the population mean body temperature",
    "text": "Approach 1: Create a 95% C I for the population mean body temperature\n\nUse data based on the results from the 1992 JAMA study\n\nThe original dataset used in the JAMA article is not available\nHowever, Allen Shoemaker from Calvin College created a dataset with the same summary statistics as in the JAMA article, which we will use:\n\n\n\\[\\bar{x} = 98.25,~s=0.733,~n=130\\] CI for \\(\\mu\\):\n\n\n\\[\\begin{align}\n\\bar{x} &\\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\\\\n98.25 &\\pm 1.979\\cdot\\frac{0.733}{\\sqrt{130}}\\\\\n98.25 &\\pm 0.127\\\\\n(98.123&, 98.377)\n\\end{align}\\]\n\nUsed \\(t^*\\) = qt(.975, df=129)\nConclusion:\nWe are 95% confident that the (population) mean body temperature is between 98.123°F and 98.377°F.\n\nHow does the CI compare to 98.6°F?"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#approach-2-hypothesis-test",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#approach-2-hypothesis-test",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Approach 2: Hypothesis Test",
    "text": "Approach 2: Hypothesis Test\nFrom before:\n\nRun a hypothesis test to see if there is evidence that the population mean \\(\\mu\\) is significantly different from 98.6°F or not.\n\nThis does not give us a range of plausible values for the population mean \\(\\mu\\).\nInstead, we calculate a test statistic and p-value\n\nto see how likely we are to observe the sample mean \\(\\bar{x}\\)\nor a more extreme sample mean\nassuming that the population mean \\(\\mu\\) is 98.6°F.\n\n\n\nHow do we calculate a test statistic and p-value?"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#recall-the-sampling-distribution-of-the-mean",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#recall-the-sampling-distribution-of-the-mean",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Recall the sampling distribution of the mean",
    "text": "Recall the sampling distribution of the mean\nFrom the Central Limit Theorem (CLT), we know that\n\nFor “large” sample sizes ( \\(n\\geq 30\\) ),\n\nthe sampling distribution of the sample mean\ncan be approximated by a normal distribution,with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\\[\\bar{X}\\sim N\\Big(\\mu_{\\bar{X}} = \\mu, \\sigma_{\\bar{X}}= \\frac{\\sigma}{\\sqrt{n}}\\Big)\\]\n\nFor small sample sizes, if the population is known to be normally distributed, then\n\nthe same result holds"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#case-1-suppose-we-know-the-population-sd-sigma",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#case-1-suppose-we-know-the-population-sd-sigma",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Case 1: suppose we know the population sd \\(\\sigma\\)",
    "text": "Case 1: suppose we know the population sd \\(\\sigma\\)\n\nHow likely we are to observe the sample mean \\(\\bar{x}\\) ,\n\nor a more extreme sample mean,\nassuming that the population mean \\(\\mu\\) is 98.6°F?\n\nUse \\(\\bar{x} = 98.25\\), \\(\\sigma=0.733\\), and \\(n=130\\)"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#case-2-we-dont-know-the-population-sd-sigma",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#case-2-we-dont-know-the-population-sd-sigma",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Case 2: we don’t know the population sd \\(\\sigma\\)",
    "text": "Case 2: we don’t know the population sd \\(\\sigma\\)\n\n\n\nThis is usually the case in real life\nWe estimate \\(\\sigma\\) with the sample standard deviation \\(s\\)\nFrom last time, we know that in this case we need to use the t-distribution with d.f. = n-1, instead of the normal distribution\nQuestion: How likely we are to observe the sample mean \\(\\bar{x}\\) or a more extreme sample mean, assuming that the population mean \\(\\mu\\) is 98.6°F?\nUse \\(\\bar{x} = 98.25\\), \\(s=0.733\\), and \\(n=130\\)"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#steps-in-a-hypothesis-test",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#steps-in-a-hypothesis-test",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-2-null-alternative-hypotheses-12",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-2-null-alternative-hypotheses-12",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 2: Null & Alternative Hypotheses (1/2)",
    "text": "Step 2: Null & Alternative Hypotheses (1/2)\nIn statistics, a hypothesis is a statement about the value of an unknown population parameter.\nA hypothesis test consists of a test between two competing hypotheses:\n\na null hypothesis \\(H_0\\) (pronounced “H-naught”) vs. \nan alternative hypothesis \\(H_A\\) (also denoted \\(H_1\\))\n\nExample of hypotheses in words:\n\\[\\begin{aligned}\nH_0 &: \\text{The population mean body temperature is 98.6°F}\\\\\n\\text{vs. } H_A &: \\text{The population mean body temperature is not 98.6°F}\n\\end{aligned}\\]\n\n\\(H_0\\) is a claim that there is “no effect” or “no difference of interest.”\n\\(H_A\\) is the claim a researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis \\(H_0\\)"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-2-null-alternative-hypotheses-22",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-2-null-alternative-hypotheses-22",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 2: Null & Alternative Hypotheses (2/2)",
    "text": "Step 2: Null & Alternative Hypotheses (2/2)\nNotation for hypotheses:\n\\[\\begin{aligned}\nH_0 &: \\mu = \\mu_0\\\\\n\\text{vs. } H_A&: \\mu \\neq, &lt;, \\textrm{or}, &gt; \\mu_0\n\\end{aligned}\\]\nWe call \\(\\mu_0\\) the null value\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\n\nnot choosing a priori whether we believe the population mean is greater or less than the null value \\(\\mu_0\\)\n\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\n\nbelieve the population mean is less than the null value \\(\\mu_0\\)\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\n\nbelieve the population mean is greater than the null value \\(\\mu_0\\)\n\n\n\n\n\n\\(H_A: \\mu \\neq \\mu_0\\) is the most common option, since it’s the most conservative\n\nExample:\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-3-test-statistic-its-distribution",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-3-test-statistic-its-distribution",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 3: Test statistic (& its distribution)",
    "text": "Step 3: Test statistic (& its distribution)\n\n\nCase 1: know population sd \\(\\sigma\\)\n\\[\n\\text{test statistic} = z_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\n\nStatistical theory tells us that \\(z_{\\bar{x}}\\) follows a Standard Normal distribution \\(N(0,1)\\)\n\n\nCase 2: don’t know population sd \\(\\sigma\\)\n\\[\n\\text{test statistic} = t_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\n\nStatistical theory tells us that \\(t_{\\bar{x}}\\) follows a Student’s t distribution with degrees of freedom (df) = \\(n-1\\)\n\n\n\n\n\\(\\bar{x}\\) = sample mean, \\(\\mu_0\\) = hypothesized population mean from \\(H_0\\),\n\\(\\sigma\\) = population standard deviation, \\(s\\) = sample standard deviation,\n\\(n\\) = sample size\n\nAssumptions: same as CLT\n\nIndependent observations: the observations were collected independently.\nApproximately normal sample or big n: the distribution of the sample should be approximately normal, or the sample size should be at least 30."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-3-test-statistic-calculation",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-3-test-statistic-calculation",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 3: Test statistic calculation",
    "text": "Step 3: Test statistic calculation\nRecall that \\(\\bar{x} = 98.25\\), \\(s=0.733\\), and \\(n=130.\\)\nThe test statistic is:\n\\[t_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n= \\frac{98.25 - 98.6}{\\frac{0.73}{\\sqrt{130}}}\n= -5.45\\]\n\nStatistical theory tells us that \\(t_{\\bar{x}}\\) follows a Student’s t-distribution with \\(d.f. = n-1 = 129\\).\n\n\n\n\n\n\n\n\n\nAssumptions met?"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-4-p-value",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-4-p-value",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\nThe \\(p\\)-value is a quantification of “surprise”\n\nAssuming \\(H_0\\) is true, how surprised are we with the observed results?\nEx: assuming that the true mean body temperature is 98.6°F, how surprised are we to get a sample mean of 98.25°F (or more extreme)?\n\nIf the \\(p\\)-value is “small,” it means there’s a small probability that we would get the observed statistic (or more extreme) when \\(H_0\\) is true."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-4-p-value-calculation",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-4-p-value-calculation",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value calculation",
    "text": "Step 4: p-value calculation\nCalculate the p-value using the Student’s t-distribution with \\(d.f. = n-1 = 129\\):\n\\[p-value=P(T \\leq -5.45) + P(T \\geq 5.45) = 2.410889 \\times 10^{-07}\\]\n\n# use pt() instead of pnorm()\n# need to specify df\n2*pt(-5.4548, df = 130-1, lower.tail = TRUE)\n\n[1] 2.410889e-07"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-4-p-value-estimation-using-t-table",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-4-p-value-estimation-using-t-table",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value estimation using \\(t\\)-table",
    "text": "Step 4: p-value estimation using \\(t\\)-table\n\n\\(t\\)-table only gives us bounds on the p-value\nRecall from using the \\(t\\)-table for CIs, that the table gives us the cutoff values for varying tail probabilities (1-tail & 2-tail)\nFind the row with the appropriate degrees of freedom\n\nUse next smallest df in table if actual df not shown\nI.e., for df = 129, use df = 100 in table\n\nFigure out where the test statistic’s absolute value is in relation to the values in the columns, i.e. between which columns is the test statistic?\nThe header rows for those columns gives the lower & upper bounds for the p-value\n\nChoosing one-tail vs. two-tail test, depends on the alternative hypothesis \\(H_A\\).\nFor a 2-sided test ( \\(H_A: \\mu \\neq \\mu_0\\) ), use two-tails\nFor a 1-sided test ( \\(H_A: \\mu &lt; \\textrm{or} &gt; \\mu_0\\) ), use one-tail"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#using-a-t-table-to-estimate-p-value",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#using-a-t-table-to-estimate-p-value",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Using a \\(t\\)-table to estimate p-value",
    "text": "Using a \\(t\\)-table to estimate p-value"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-1-significance-level-alpha",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-1-significance-level-alpha",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 1: Significance Level \\(\\alpha\\)",
    "text": "Step 1: Significance Level \\(\\alpha\\)\n\nBefore doing a hypothesis test, we set a cut-off for how small the \\(p\\)-value should be in order to reject \\(H_0\\).\nWe call this the significance level, denoted by the Greek symbol alpha ( \\(\\alpha\\) )\nTypical \\(\\alpha\\) values are\n\n0.05 - most common by far!!\n0.01 and 0.1\n\nDecision rule:\n\nWhen \\(p\\)-value &lt; \\(\\alpha\\), we “reject the null hypothesis \\(H_0\\).”\nWhen \\(p\\)-value \\(\\geq \\alpha\\), we “fail to reject the null hypothesis \\(H_0\\).”\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n“Failing to reject” \\(H_0\\) is NOT the same as “accepting” \\(H_0\\)!\nBy failing to reject \\(H_0\\) we are just saying that we don’t have sufficient evidence to support the alternative \\(H_A\\).\nThis does not imply that \\(H_0\\) is true!!"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-5-conclusion-to-hypothesis-test",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]\n\nRecall the \\(p\\)-value = \\(2.410889 \\times 10^{-07}\\)\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nBasic: (“stats class” conclusion)\n\nThere is sufficient evidence that the (population) mean body temperature is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001).\n\nBetter: (“manuscript style” conclusion)\n\nThe average body temperature in the sample was 98.25°F (95% CI 98.12, 98.38°F), which is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#confidence-intervals-vs.-hypothesis-testing",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#confidence-intervals-vs.-hypothesis-testing",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Confidence Intervals vs. Hypothesis Testing",
    "text": "Confidence Intervals vs. Hypothesis Testing\n\nSee also V&H Section 4.3.3"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#working-directory",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#working-directory",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Working directory",
    "text": "Working directory\n\nIn order to load a dataset from a file, you need to tell R where the dataset is located\nTo do this you also need to know the location from which R is working, i.e. your working directory\nYou can figure out your working directory by running the getwd() function.\n\n\ngetwd()\n\n[1] \"/Users/wakim/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/Teaching/Classes/F24_EPI_525/F24_EPI_525_site\"\n\n\n\nAbove is the working directory of this slides file\n\nIn this case, this is NOT the location of the actual qmd file though!\n\nTo make it easier to juggle the working directory, the location of your qmd file, and the location of the data,\n\nI highly recommend using R Projects!"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#r-projects",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#r-projects",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "R projects",
    "text": "R projects\n\nI highly, highly, HIGHLY recommend using R Projects to organize your analyses and make it easier to load data files and also save output.\nWhen you create an R Project on your computer, the Project is associated with the folder (directory) you created it in.\n\nThis folder becomes the “root” of your working directory, and RStudio’s point of reference from where to load files from and to.\n\nI create separate Projects for every analysis project and every class I teach.\nYou can run multiple sessions of RStudio by opening different Projects, and the environments (or working directory) of each are working independently of each other.\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlthough we are using Quarto files,\n\nI will show how to set up and use a “regular” R Project\ninstead of “Quarto Project”\n\nQuarto Projects include extra features and thus complexity. Once you are used to how regular R Projects work, you can try out a Quarto Project."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#how-to-create-an-r-project",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#how-to-create-an-r-project",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "How to create an R Project",
    "text": "How to create an R Project\n\nDemonstration in class recording\nPosit’s (RStudio’s) directions for creating Projects\n\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects\n\nSee file Projects in RStudio for more information on R Projects."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#load-the-dataset",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#load-the-dataset",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Load the dataset",
    "text": "Load the dataset\n\nThe data are in a csv file called BodyTemperatures.csv\nYou need to tell R where the dataset is located!\nI recommend saving all datasets in a folder called data.\n\nThe code I will be providing you will be set up this way.\n\nTo make it easier to specify where the dataset is located, I recommend using the here() function from the here package: here::here().\n\n\n# read_csv() is a function from the readr package that is a part of the tidyverse\nlibrary(here)   # first install this package\n\nhere() starts at /Users/wakim/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/Teaching/Classes/F24_EPI_525/F24_EPI_525_site\n\nBodyTemps &lt;- read_csv(here::here(\"data\", \"BodyTemperatures.csv\"))\n\nRows: 130 Columns: 3\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): Temperature, Gender, HeartRate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#                     location: look in \"data\" folder\n#                               for the file \"BodyTemperatures.csv\"\n\nglimpse(BodyTemps)\n\nRows: 130\nColumns: 3\n$ Temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ Gender      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HeartRate   &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#herehere",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#herehere",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "here::here()",
    "text": "here::here()\nGeneral use of here::here()\nhere::here(\"folder_name\", \"filename\")\n\n\nResources for here::here():\n\nhow to use the here package (Jenny Richmond)\nOde to the here package (Jenny Bryan)\n\nProject-oriented workflow (Jenny Bryan)\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#t.test-base-rs-function-for-testing-one-mean",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#t.test-base-rs-function-for-testing-one-mean",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "t.test: base R’s function for testing one mean",
    "text": "t.test: base R’s function for testing one mean\n\nUse the body temperature example with \\(H_A: \\mu \\neq 98.6\\)\nWe called the dataset BodyTemps when we loaded it\n\n\nglimpse(BodyTemps)\n\nRows: 130\nColumns: 3\n$ Temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ Gender      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HeartRate   &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…\n\n(temps_ttest &lt;- t.test(x = BodyTemps$Temperature,\n       # alternative = \"two.sided\",  # default\n       mu = 98.6))\n\n\n    One Sample t-test\n\ndata:  BodyTemps$Temperature\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923 \n\n\nNote that the test output also gives the 95% CI using the t-distribution."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#tidy-the-t.test-output",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#tidy-the-t.test-output",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "tidy() the t.test output",
    "text": "tidy() the t.test output\n\nUse the tidy() function from the broom package for briefer output in table format that’s stored as a tibble\nCombined with the gt() function from the gt package, we get a nice table\n\n\ntidy(temps_ttest) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    98.24923\n-5.454823\n2.410632e-07\n129\n98.122\n98.37646\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n\n\nSince the tidy() output is a tibble, we can easily pull() specific values from it:\n\n\n\nUsing base R’s $\n\ntidy(temps_ttest)$p.value  \n\n[1] 2.410632e-07\n\n\nAdvantage: quick and easy\n\nOr the tidyverse way: using pull() from dplyr package\n\ntidy(temps_ttest) %&gt;% pull(p.value)\n\n[1] 2.410632e-07\n\n\nAdvantage: can use together with piping (%&gt;%) other functions"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#whats-next",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01.html#whats-next",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "What’s next?",
    "text": "What’s next?\nCI’s and hypothesis testing for different scenarios:\n\n\n\n\n\n\n\n\n\n\n\nDay\nSection\nPopulation parameter\nSymbol\nPoint estimate\nSymbol\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\n\n11\n5.3\nDiff in pop means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n12\n8.2\nDiff in pop prop’s\n\\(p_1-p_2\\)\nDiff in sample prop’s\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "lessons/17_Non_parametric/17_Non_parametric.html",
    "href": "lessons/17_Non_parametric/17_Non_parametric.html",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "",
    "text": "Download pdf of slides\n12/2/23: See updates made to code file for creating ranks using R’s rank() function that has an option ties.method to specify how to calculate ties.\n\nSearch “New:” for updates made."
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R_key_info.html",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Include organization\n\nHave them do a data management script!!\n\nself-contained not a proper yaml command now\ncitations!! in zotero!!\nGit and GitHub!!"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R_key_info.html#to-nicky",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R_key_info.html#to-nicky",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Include organization\n\nHave them do a data management script!!\n\nself-contained not a proper yaml command now\ncitations!! in zotero!!\nGit and GitHub!!"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R_key_info.html#announcements",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "Announcements",
    "text": "Announcements"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R_key_info.html#key-dates",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "homework/HW_07.html",
    "href": "homework/HW_07.html",
    "title": "Homework 7",
    "section": "",
    "text": "Due 11/18/23\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F23/blob/main/homework/HW_7_F23_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_07.html#directions",
    "href": "homework/HW_07.html#directions",
    "title": "Homework 7",
    "section": "Directions",
    "text": "Directions\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\n\n\nR & LaTeX code\n\nSee the .qmd files with the code from class notes for LaTeX and R code.\nThe LaTeX code will make it easier to show your work in computations.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_07.html#young-americans-part-i",
    "href": "homework/HW_07.html#young-americans-part-i",
    "title": "Homework 7",
    "section": "8.2 Young Americans, Part I",
    "text": "8.2 Young Americans, Part I"
  },
  {
    "objectID": "homework/HW_07.html#legalization-of-marijuana-part-i",
    "href": "homework/HW_07.html#legalization-of-marijuana-part-i",
    "title": "Homework 7",
    "section": "8.8 Legalization of marijuana, Part I",
    "text": "8.8 Legalization of marijuana, Part I\nAdditional instructions:\n\n(b): Calculate the CI both using the formula and using the appropriate R statistical test.\nAdd parts (e) & (f) as instructed below.\n\n\n(a)\n\n\n(b)\n\n\n(c)\n\n\n(d)\n\n\n(e)\nTest whether the proportion of US residents who think marijuana should be made legal is different than 0.586. Do the test using both the formulas and running it in R (without a continuity correction).\n\n\n(f)\nAre the results from CI and hypothesis test consistent? Why or why not?"
  },
  {
    "objectID": "homework/HW_07.html#legalize-marijuana-part-ii",
    "href": "homework/HW_07.html#legalize-marijuana-part-ii",
    "title": "Homework 7",
    "section": "8.10 Legalize Marijuana, Part II",
    "text": "8.10 Legalize Marijuana, Part II"
  },
  {
    "objectID": "homework/HW_07.html#healthcare-law",
    "href": "homework/HW_07.html#healthcare-law",
    "title": "Homework 7",
    "section": "8.14 2010 Healthcare Law",
    "text": "8.14 2010 Healthcare Law"
  },
  {
    "objectID": "homework/HW_07.html#prenatal-vitamins-and-autism",
    "href": "homework/HW_07.html#prenatal-vitamins-and-autism",
    "title": "Homework 7",
    "section": "8.24 Prenatal vitamins and Autism",
    "text": "8.24 Prenatal vitamins and Autism\nAdditional instructions:\n\n(b): Do the hypothesis test both using the formula and using the appropriate R statistical test.\nAdd part (d) as instructed below.\n\n\n(a)\n\n\n(b)\n\n\n(c)\n\n\n(d)\nCalculate and interpret the 95% confidence interval for the difference in proportions using the formula. Is it consistent with CI from the R output of the hypothesis test?"
  },
  {
    "objectID": "homework/HW_07.html#an-apple-a-day-keeps-the-doctor-away",
    "href": "homework/HW_07.html#an-apple-a-day-keeps-the-doctor-away",
    "title": "Homework 7",
    "section": "8.26 An apple a day keeps the doctor away",
    "text": "8.26 An apple a day keeps the doctor away"
  },
  {
    "objectID": "homework/HW_07.html#true-or-false-part-ii",
    "href": "homework/HW_07.html#true-or-false-part-ii",
    "title": "Homework 7",
    "section": "8.28 True or false, Part II",
    "text": "8.28 True or false, Part II"
  },
  {
    "objectID": "homework/HW_07.html#diabetes-and-unemployment",
    "href": "homework/HW_07.html#diabetes-and-unemployment",
    "title": "Homework 7",
    "section": "8.32 Diabetes and unemployment",
    "text": "8.32 Diabetes and unemployment\n\n(a)\n\n\n(b)\n\n\n(c)\nIn addition to answering the question in (c), run the 2 proportions test in R and calculated the “z” test statistic and p-value using the normal distribution based on the \\(\\chi^2\\) output in R’s test.\n\n\n(d) extra part\nRun the test as a \\(\\chi^2\\) as well, and compare your results to those in (c)."
  },
  {
    "objectID": "homework/HW_07.html#coffee-and-depression",
    "href": "homework/HW_07.html#coffee-and-depression",
    "title": "Homework 7",
    "section": "8.34 Coffee and Depression",
    "text": "8.34 Coffee and Depression\n\nDo not create a dataset and run the test in R. Use just the information given in the problem.\nFor (e), calculate the p-value “directly” (not using \\(\\chi^2\\) test command in R). Also comment on whether you think the sample size condition would be met without computing the expected cell counts."
  },
  {
    "objectID": "homework/HW_07.html#a-extra-salt-intake-and-cvd",
    "href": "homework/HW_07.html#a-extra-salt-intake-and-cvd",
    "title": "Homework 7",
    "section": "8.38 (a) & (extra) Salt intake and CVD",
    "text": "8.38 (a) & (extra) Salt intake and CVD\nDo not do parts (b)-(c) in the book\n\n(a)\n\nYou can use the expected cell counts from R’s chi-squared test (you do not need to compute them using the formula).\nComment on whether the sample size condition is met or not for these data.\n\n\n\n(extra)\nRun a Fisher’s Exact test. Include the hypotheses and a conclusion in the context of the problem."
  },
  {
    "objectID": "homework/HW_07.html#pss1-legalize-marijuana-part-iii.",
    "href": "homework/HW_07.html#pss1-legalize-marijuana-part-iii.",
    "title": "Homework 7",
    "section": "1.1 PSS1: Legalize Marijuana, Part III.",
    "text": "1.1 PSS1: Legalize Marijuana, Part III.\n\n1.1.1 Power\nIn exercise 8.8 (e), we tested whether the proportion of US residents who think marijuana should be made legal is different than 0.586. Calculate the power for the hypothesis test and include an interpretation of the power in the context of the research question. Was it sufficiently powered?\n\n\n1.1.2 Sample size\nFor the same test, what sample size would be needed for 80% power? How about 90% power?\n\n\n1.1.3 Effect size\nIf we increase the sample size, and keep the power and significance level the same, does the effect size increase or decrease? Why?"
  },
  {
    "objectID": "homework/HW_04.html",
    "href": "homework/HW_04.html",
    "title": "Homework 4",
    "section": "",
    "text": "Due 10/28/23\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F23/blob/main/homework/HW_4_F23_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_04.html#directions",
    "href": "homework/HW_04.html#directions",
    "title": "Homework 4",
    "section": "Directions",
    "text": "Directions\n\n\n\n\n\n\nImportant\n\n\n\n\nThe non-R exercises (see sections Book exercises and Non-book exercise) may be completed not using Quarto. I especially recommend writing out by hand the chapter 3 probability questions, whether on paper or a tablet.\n\nSome problems involve R code to calculate a probability, but the code is brief and you can write out the code and the answer by hand.\n\nThe R exercises are to be completed using Quarto.\nIf you complete part of the assignment not using Quarto, you will be uploading 3 files on Sakai for HW 4: qmd & html files for your R work, and a pdf with your written work.\nIf you are completing the homework on paper, you can use a scanning app, such as Adobe Scan, to create a pdf of your assignment.\n\n\n\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_04.html#instructions-for-normal-probability-exercises",
    "href": "homework/HW_04.html#instructions-for-normal-probability-exercises",
    "title": "Homework 4",
    "section": "!!! Instructions for Normal probability exercises !!!",
    "text": "!!! Instructions for Normal probability exercises !!!\n\n\n\n\n\n\nImportant\n\n\n\nAdditional Instructions - IMPORTANT!!!\n\nFor ALL normal distribution exercises:\n\nmake a sketch of the normal distribution curve with the mean and 1 sd away from the mean clearly labeled, and the area representing probability of interest shaded in.\ncalculate probabilities using both\n\nz-table\nR"
  },
  {
    "objectID": "homework/HW_04.html#area-under-the-curve-part-ii",
    "href": "homework/HW_04.html#area-under-the-curve-part-ii",
    "title": "Homework 4",
    "section": "3.20 Area under the curve, Part II",
    "text": "3.20 Area under the curve, Part II"
  },
  {
    "objectID": "homework/HW_04.html#triathlon-times",
    "href": "homework/HW_04.html#triathlon-times",
    "title": "Homework 4",
    "section": "3.22 Triathlon times",
    "text": "3.22 Triathlon times"
  },
  {
    "objectID": "homework/HW_04.html#arsenic-poisoning",
    "href": "homework/HW_04.html#arsenic-poisoning",
    "title": "Homework 4",
    "section": "3.28 Arsenic poisoning",
    "text": "3.28 Arsenic poisoning"
  },
  {
    "objectID": "homework/HW_04.html#find-the-sd",
    "href": "homework/HW_04.html#find-the-sd",
    "title": "Homework 4",
    "section": "3.30 Find the SD",
    "text": "3.30 Find the SD"
  },
  {
    "objectID": "homework/HW_04.html#chickenpox-part-iii",
    "href": "homework/HW_04.html#chickenpox-part-iii",
    "title": "Homework 4",
    "section": "3.32 Chickenpox, Part III",
    "text": "3.32 Chickenpox, Part III"
  },
  {
    "objectID": "homework/HW_04.html#stenographers-typos",
    "href": "homework/HW_04.html#stenographers-typos",
    "title": "Homework 4",
    "section": "3.38 Stenographer’s typos",
    "text": "3.38 Stenographer’s typos"
  },
  {
    "objectID": "homework/HW_04.html#osteosarcoma-in-nyc",
    "href": "homework/HW_04.html#osteosarcoma-in-nyc",
    "title": "Homework 4",
    "section": "3.40 Osteosarcoma in NYC",
    "text": "3.40 Osteosarcoma in NYC"
  },
  {
    "objectID": "homework/HW_04.html#heights-of-adults",
    "href": "homework/HW_04.html#heights-of-adults",
    "title": "Homework 4",
    "section": "4.2 Heights of adults",
    "text": "4.2 Heights of adults"
  },
  {
    "objectID": "homework/HW_04.html#the-ethan-allen",
    "href": "homework/HW_04.html#the-ethan-allen",
    "title": "Homework 4",
    "section": "1.1 The Ethan Allen",
    "text": "1.1 The Ethan Allen\nOn October 5, 2005, a tour boat named the Ethan Allen capsized on Lake George in New York with 47 passengers aboard. In the inquiries that followed, it was suggested that the tour operators should have realized that the combined weight of so many passengers was likely to exceed the weight capacity of the boat. Could they have predicted this?\n\nThe maximum weight capacity of passengers that the Ethan Allen could accommodate was estimated to be 7500 pounds.\nData from the Centers for Disease Control and Prevention indicate that weights of American adults in 2005 had a mean of 167 pounds and a standard deviation of 35 pounds.\n\nIf the tour boat company consistently accepted 47 passengers, what we want to know is the probability that the combined weight of the 47 passengers would exceed this capacity.\n\n1.1.1 Maximum average weight\nWith 47 passengers on board, what is the maximum average weight that the Ethan Allen could accommodate?\n\n\n1.1.2 Probability an individual weighs more than the maximum average weight\nAssuming that the weights of American adults in 2005 can be modeled with a normal distribution, find the probability that an individual weighs more than the maximum average weight the Ethan Allen can accommodate.\n\n\n1.1.3 Probability a random sample of 47 American adults has an average weight greater than the maximum average weight\nCalculate the probability that a random sample of 47 American adults has an average weight greater than the maximum average weight the Ethan Allen can accommodate.\n\n\n1.1.4 What theorem did you use in the previous part, and why were you able to apply it to this problem?\n\n\n1.1.5 Could the tour operators have predicted realized that the combined weight of so many passengers was likely to exceed the weight capacity of the Ethan Allen?"
  },
  {
    "objectID": "homework/HW_04.html#load-all-the-packages-you-need-below-here.",
    "href": "homework/HW_04.html#load-all-the-packages-you-need-below-here.",
    "title": "Homework 4",
    "section": "2.1 Load all the packages you need below here.",
    "text": "2.1 Load all the packages you need below here."
  },
  {
    "objectID": "homework/HW_04.html#r1-youth-weights",
    "href": "homework/HW_04.html#r1-youth-weights",
    "title": "Homework 4",
    "section": "2.2 R1: Youth weights",
    "text": "2.2 R1: Youth weights\nIn this exercise you will use the YRBSS dataset we used in class on Day 8, to simulate the distribution of mean weights from repeated samples. Use the code from class where we simulated mean heights, and apply it to the weights (in pounds) as directed below.\n\n\n\n\n\n\nImportant\n\n\n\nYou will need to install and load the moderndive R package to use the rep_sample_n() command from the class notes.\n\n\n\n2.2.1 Use the set.seed() command to set a randomization seed. Use whatever number you want for the seed.\n\n\n2.2.2 Take 1000 random samples of size 10 and save the tibble with the random samples. Show the first 20 lines of this tibble.\n\n\n2.2.3 Create a tibble with mean weights from the 1000 random samples. Show the first 10 rows of this tibble.\n\n\n2.2.4 Make a histogram of the 1000 mean weights. What do we call this distribution? Describe the shape of the distribution.\n\n\n2.2.5 Calculate the mean and standard deviation of the 1000 sample mean weights. What is another name for this standard deviation?\n\n\n2.2.6 What are the theoretical values for mean and standard deviation of the sampling distribution from the CLT, and how do your simulated values compare to the theoretical values?"
  },
  {
    "objectID": "homework/HW_02.html",
    "href": "homework/HW_02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Due 10/14/23\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F23/blob/main/homework/HW_2_F23_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_02.html#directions",
    "href": "homework/HW_02.html#directions",
    "title": "Homework 2",
    "section": "Directions",
    "text": "Directions\n\n\n\n\n\n\nImportant\n\n\n\n\nThe non-R exercises (see sections Book exercises and Non-book exercise) may be completed not using Quarto. I especially recommend writing out by hand the chapter 2 probability questions, whether on paper or a tablet.\nIf you complete part of the assignment not using Quarto, you will be uploading 3 files on Sakai for HW 2: qmd & html files for your R work, and a pdf with your written work.\nIf you are completing the homework on paper, you can use a scanning app, such as Adobe Scan, to create a pdf of your assignment.\n\n\n\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_02.html#income-and-education-in-us-counties",
    "href": "homework/HW_02.html#income-and-education-in-us-counties",
    "title": "Homework 2",
    "section": "1.24 Income and education in US counties",
    "text": "1.24 Income and education in US counties"
  },
  {
    "objectID": "homework/HW_02.html#mix-and-match",
    "href": "homework/HW_02.html#mix-and-match",
    "title": "Homework 2",
    "section": "1.28 Mix-and-match",
    "text": "1.28 Mix-and-match"
  },
  {
    "objectID": "homework/HW_02.html#associations",
    "href": "homework/HW_02.html#associations",
    "title": "Homework 2",
    "section": "1.36 Associations",
    "text": "1.36 Associations"
  },
  {
    "objectID": "homework/HW_02.html#smoking-and-stenosis",
    "href": "homework/HW_02.html#smoking-and-stenosis",
    "title": "Homework 2",
    "section": "1.38 Smoking and stenosis",
    "text": "1.38 Smoking and stenosis\nSee Section1.6.2 for more on how the relative risk is calculated."
  },
  {
    "objectID": "homework/HW_02.html#poverty-and-language",
    "href": "homework/HW_02.html#poverty-and-language",
    "title": "Homework 2",
    "section": "2.6 Poverty and language",
    "text": "2.6 Poverty and language\nPart (b) asks you to create a Venn Diagram. If you are submitting this question in R, you do not need to turn this part in. If you want an R challenge though, you can use the VennDiagram or other package to create one. See https://www.geeksforgeeks.org/how-to-create-a-venn-diagram-in-r/ for some examples."
  },
  {
    "objectID": "homework/HW_02.html#school-absences",
    "href": "homework/HW_02.html#school-absences",
    "title": "Homework 2",
    "section": "2.8 School absences",
    "text": "2.8 School absences\nPart (b) asks you to create a Venn Diagram. If you are submitting this question in R, you do not need to turn this part in. If you want an R challenge though, you can use the VennDiagram or other package to create one. See https://www.geeksforgeeks.org/how-to-create-a-venn-diagram-in-r/ for some examples."
  },
  {
    "objectID": "homework/HW_02.html#health-coverage-frequencies",
    "href": "homework/HW_02.html#health-coverage-frequencies",
    "title": "Homework 2",
    "section": "2.10 Health coverage, frequencies",
    "text": "2.10 Health coverage, frequencies"
  },
  {
    "objectID": "homework/HW_02.html#health-coverage-relative-frequencies",
    "href": "homework/HW_02.html#health-coverage-relative-frequencies",
    "title": "Homework 2",
    "section": "2.14 Health coverage, relative frequencies",
    "text": "2.14 Health coverage, relative frequencies"
  },
  {
    "objectID": "homework/HW_02.html#predisposition-for-thrombosis",
    "href": "homework/HW_02.html#predisposition-for-thrombosis",
    "title": "Homework 2",
    "section": "2.18 Predisposition for thrombosis",
    "text": "2.18 Predisposition for thrombosis"
  },
  {
    "objectID": "homework/HW_02.html#breast-cancer-and-age",
    "href": "homework/HW_02.html#breast-cancer-and-age",
    "title": "Homework 2",
    "section": "2.24 Breast cancer and age",
    "text": "2.24 Breast cancer and age"
  },
  {
    "objectID": "homework/HW_02.html#load-all-the-packages-you-need-below-here.",
    "href": "homework/HW_02.html#load-all-the-packages-you-need-below-here.",
    "title": "Homework 2",
    "section": "2.1 Load all the packages you need below here.",
    "text": "2.1 Load all the packages you need below here."
  },
  {
    "objectID": "homework/HW_02.html#what-are-the-dimensions-and-column-names-of-the-dataset",
    "href": "homework/HW_02.html#what-are-the-dimensions-and-column-names-of-the-dataset",
    "title": "Homework 2",
    "section": "3.1 What are the dimensions and column names of the dataset?",
    "text": "3.1 What are the dimensions and column names of the dataset?"
  },
  {
    "objectID": "homework/HW_02.html#how-many-unique-id-identifiers-are-in-the-dataset-compare-this-to-the-number-of-rows-in-the-dataset.-what-is-the-explanation-for-these-two-different-numbers",
    "href": "homework/HW_02.html#how-many-unique-id-identifiers-are-in-the-dataset-compare-this-to-the-number-of-rows-in-the-dataset.-what-is-the-explanation-for-these-two-different-numbers",
    "title": "Homework 2",
    "section": "3.2 How many unique ID identifiers are in the dataset? Compare this to the number of rows in the dataset. What is the explanation for these two different numbers?",
    "text": "3.2 How many unique ID identifiers are in the dataset? Compare this to the number of rows in the dataset. What is the explanation for these two different numbers?"
  },
  {
    "objectID": "homework/HW_02.html#using-numerical-and-graphical-summaries-describe-the-distribution-of-ages-among-study-participants.",
    "href": "homework/HW_02.html#using-numerical-and-graphical-summaries-describe-the-distribution-of-ages-among-study-participants.",
    "title": "Homework 2",
    "section": "3.3 Using numerical and graphical summaries, describe the distribution of ages among study participants.",
    "text": "3.3 Using numerical and graphical summaries, describe the distribution of ages among study participants."
  },
  {
    "objectID": "homework/HW_02.html#using-numerical-and-graphical-summaries-describe-the-distribution-of-heights-among-study-participants.",
    "href": "homework/HW_02.html#using-numerical-and-graphical-summaries-describe-the-distribution-of-heights-among-study-participants.",
    "title": "Homework 2",
    "section": "3.4 Using numerical and graphical summaries, describe the distribution of heights among study participants.",
    "text": "3.4 Using numerical and graphical summaries, describe the distribution of heights among study participants."
  },
  {
    "objectID": "homework/HW_02.html#investigate-at-which-age-people-generally-reach-their-adult-height.-is-it-possible-to-do-the-same-for-weight-why-or-why-not",
    "href": "homework/HW_02.html#investigate-at-which-age-people-generally-reach-their-adult-height.-is-it-possible-to-do-the-same-for-weight-why-or-why-not",
    "title": "Homework 2",
    "section": "3.5 Investigate at which age people generally reach their adult height. Is it possible to do the same for weight; why or why not?",
    "text": "3.5 Investigate at which age people generally reach their adult height. Is it possible to do the same for weight; why or why not?\nUse whatever EDA tools you think are appropriate to answer this question."
  },
  {
    "objectID": "homework/HW_02.html#calculate-the-median-and-interquartile-range-of-the-distribution-of-the-variable-poverty.-write-a-sentence-explaining-the-median-and-iqr-in-the-context-of-these-data.",
    "href": "homework/HW_02.html#calculate-the-median-and-interquartile-range-of-the-distribution-of-the-variable-poverty.-write-a-sentence-explaining-the-median-and-iqr-in-the-context-of-these-data.",
    "title": "Homework 2",
    "section": "3.6 Calculate the median and interquartile range of the distribution of the variable Poverty. Write a sentence explaining the median and IQR in the context of these data.",
    "text": "3.6 Calculate the median and interquartile range of the distribution of the variable Poverty. Write a sentence explaining the median and IQR in the context of these data."
  },
  {
    "objectID": "homework/HW_02.html#use-the-mutate-command-explained-on-slide-23-pdf-pg-22-of-the-day-3-part-3-notes-to-create-a-new-column-variable-in-the-nhanes-dataset-for-the-heights-in-inches.-then-find-the-mean-and-standard-deviation-of-the-heights-in-inches.-note-that-1-centimeter-is-approximately-0.3937-inches.",
    "href": "homework/HW_02.html#use-the-mutate-command-explained-on-slide-23-pdf-pg-22-of-the-day-3-part-3-notes-to-create-a-new-column-variable-in-the-nhanes-dataset-for-the-heights-in-inches.-then-find-the-mean-and-standard-deviation-of-the-heights-in-inches.-note-that-1-centimeter-is-approximately-0.3937-inches.",
    "title": "Homework 2",
    "section": "4.1 Use the mutate() command explained on Slide 23 (pdf pg 22) of the Day 3 Part 3 notes to create a new column (variable) in the NHANES dataset for the heights in inches. Then find the mean and standard deviation of the heights in inches. Note that 1 centimeter is approximately 0.3937 inches.",
    "text": "4.1 Use the mutate() command explained on Slide 23 (pdf pg 22) of the Day 3 Part 3 notes to create a new column (variable) in the NHANES dataset for the heights in inches. Then find the mean and standard deviation of the heights in inches. Note that 1 centimeter is approximately 0.3937 inches.\n\n\n\n\n\n\nTips\n\n\n\n\nColumn names cannot start with a number or symbol\nNames are case sensitive\nIt’s easier to work with column names that don’t have spaces in them. I usually use _ or . instead of spaces."
  },
  {
    "objectID": "homework/HW_02.html#what-proportion-of-americans-at-least-25-years-of-age-are-college-graduates",
    "href": "homework/HW_02.html#what-proportion-of-americans-at-least-25-years-of-age-are-college-graduates",
    "title": "Homework 2",
    "section": "4.2 What proportion of Americans at least 25 years of age are college graduates?",
    "text": "4.2 What proportion of Americans at least 25 years of age are college graduates?\nHint: First create a new dataset that is restricted to Americans at least 25 years old."
  },
  {
    "objectID": "homework/HW_02.html#what-proportion-of-americans-at-least-25-years-of-age-with-a-high-school-degree-are-college-graduates",
    "href": "homework/HW_02.html#what-proportion-of-americans-at-least-25-years-of-age-with-a-high-school-degree-are-college-graduates",
    "title": "Homework 2",
    "section": "4.3 What proportion of Americans at least 25 years of age with a high school degree are college graduates?",
    "text": "4.3 What proportion of Americans at least 25 years of age with a high school degree are college graduates?"
  },
  {
    "objectID": "homework/HW_02.html#two-way-table",
    "href": "homework/HW_02.html#two-way-table",
    "title": "Homework 2",
    "section": "4.4 Two-way table",
    "text": "4.4 Two-way table\nConstruct a two-way table (contingency table), with PhysActive as the row variable and Diabetes as the column variable. Among participants who are not physically active, what proportion have diabetes? What proportion of physically active participants have diabetes?"
  },
  {
    "objectID": "homework/HW_02.html#relative-risk",
    "href": "homework/HW_02.html#relative-risk",
    "title": "Homework 2",
    "section": "4.5 Relative Risk",
    "text": "4.5 Relative Risk\nIn this context, relative risk is the ratio of the proportion of participants who have diabetes among those who are not physically active to the proportion of participants with diabetes among those physically active. Relative risks greater than 1 indicate that people who are not physically active seem to be at a higher risk for diabetes than physically active people. Calculate the relative risk of diabetes for the participants. From these calculations, is it possible to conclude that being physically active reduces one’s chance of becoming diabetic?"
  },
  {
    "objectID": "homework/HW_08.html",
    "href": "homework/HW_08.html",
    "title": "Homework 8",
    "section": "",
    "text": "Due Monday 11/27/23\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F23/blob/main/homework/HW_8_F23_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_08.html#directions",
    "href": "homework/HW_08.html#directions",
    "title": "Homework 8",
    "section": "Directions",
    "text": "Directions\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\n\n\nR & LaTeX code\n\nSee the .qmd files with the code from class notes for LaTeX and R code.\nThe LaTeX code will make it easier to show your work in computations.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_08.html#work-hours-and-education",
    "href": "homework/HW_08.html#work-hours-and-education",
    "title": "Homework 8",
    "section": "5.44 Work hours and education",
    "text": "5.44 Work hours and education"
  },
  {
    "objectID": "homework/HW_08.html#child-care-hours",
    "href": "homework/HW_08.html#child-care-hours",
    "title": "Homework 8",
    "section": "5.46 Child care hours",
    "text": "5.46 Child care hours"
  },
  {
    "objectID": "homework/HW_08.html#truefalse-anova-part-ii",
    "href": "homework/HW_08.html#truefalse-anova-part-ii",
    "title": "Homework 8",
    "section": "5.48 True/False: ANOVA, Part II",
    "text": "5.48 True/False: ANOVA, Part II"
  },
  {
    "objectID": "homework/HW_08.html#identify-relationships-part-ii",
    "href": "homework/HW_08.html#identify-relationships-part-ii",
    "title": "Homework 8",
    "section": "6.2 Identify relationships, Part II",
    "text": "6.2 Identify relationships, Part II"
  },
  {
    "objectID": "homework/HW_08.html#over-under-part-ii",
    "href": "homework/HW_08.html#over-under-part-ii",
    "title": "Homework 8",
    "section": "6.6 Over-under, Part II",
    "text": "6.6 Over-under, Part II"
  },
  {
    "objectID": "homework/HW_08.html#guppies-part-i",
    "href": "homework/HW_08.html#guppies-part-i",
    "title": "Homework 8",
    "section": "6.10 Guppies, Part I",
    "text": "6.10 Guppies, Part I"
  },
  {
    "objectID": "homework/HW_08.html#load-all-the-packages-you-need-below-here.",
    "href": "homework/HW_08.html#load-all-the-packages-you-need-below-here.",
    "title": "Homework 8",
    "section": "1.1 Load all the packages you need below here.",
    "text": "1.1 Load all the packages you need below here."
  },
  {
    "objectID": "homework/HW_08.html#r1-palmer-penguins-anova",
    "href": "homework/HW_08.html#r1-palmer-penguins-anova",
    "title": "Homework 8",
    "section": "1.2 R1: Palmer Penguins ANOVA",
    "text": "1.2 R1: Palmer Penguins ANOVA\n\nUse the penguins data from the palmerpenguins package.\n\nDon’t forget to first install the palmerpenguins package\n\nYou can learn more about the Palmer penguins data at https://allisonhorst.github.io/palmerpenguins/\nWe will test whether there are differences in penguins’ mean bill depths when comparing different species.\n\n\nlibrary(palmerpenguins)\ndata(penguins)\n\n\n1.2.1 Dotplots\nMake a dotplot of the penguins’ bill depths stratified by species type. Include points for the mean of each species type as well as a horizontal dashed line for the overall mean. See example from class for the plot I’m describing.\n\n\n1.2.2 Which groups significantly different?\nBased on the figure, which pairs of species look like they have significantly different mean bill depths?\n\n\n1.2.3 Hypotheses in words\nWrite out in words the null and alternative hypotheses.\n\n\n1.2.4 Hypotheses in symbols\nWrite out in symbols the null and alternative hypotheses.\n\n\n1.2.5 Run ANOVA in R\nUsing R, run the hypothesis test and display the output.\n\n\n1.2.6 SST\nUsing the values from the ANOVA table, calculate the value of the SST (total sum of squares).\n\n\n1.2.7 MSG & MSE\nUsing the values from the ANOVA table, verify (calculate) the values of the MSG (mean square groups) and MSE (mean square error).\n\n\n1.2.8 F statistic\nUsing the values from the ANOVA table, verify (calculate) the value of the F statistic.\n\n\n1.2.9 p-value\nUsing the values from the ANOVA table, verify (calculate) the p-value.\n\n\n1.2.10 Decision?\nBased on the p-value, will we reject or fail to reject the null hypothesis? Why?\n\n\n1.2.11 Conclusion\nWrite a conclusion to the hypothesis test in the context of the problem.\n\n\n1.2.12 Technical conditions\nInvestigate whether the technical conditions for using an ANOVA been satisfied. \n\n\n1.2.13 Post-hoc pairwise t-tests: no correction\nRun post-hoc pairwise t-tests using NO p-value correction. Which pairs of species have significantly different bill depths?\n\n\n1.2.14 Post-hoc pairwise t-tests: Bonferroni correction\nRun post-hoc pairwise t-tests using a Bonferroni correction. Which pairs of species have significantly different bill depths?\n\n\n1.2.15 Hypothetical Bonferroni correction\nIf hypothetically the p-value comparing the mean bill depths of the Adelie and Chinstrap species were 0.03 without any p-value adjustment, what would the p-value be after running the post-hoc pairwise t-tests using a Bonferroni correction?\n\n\n1.2.16 Post-hoc pairwise t-tests: Tukey’s Honest Significance Test correction\nRun post-hoc pairwise t-tests using Tukey’s Honest Significance Test correction. Which pairs of species have significantly different bill depths?\n\n\n1.2.17 Tukey confidence intervals\nMake a plot showing the 95% family-wise Tukey confidence intervals. How does this plot visually confirm the which pairs of species have significantly different bill depths?"
  },
  {
    "objectID": "homework/HW_08.html#r2-palmer-penguins-slr",
    "href": "homework/HW_08.html#r2-palmer-penguins-slr",
    "title": "Homework 8",
    "section": "1.3 R2: Palmer Penguins SLR",
    "text": "1.3 R2: Palmer Penguins SLR\n\n\n\n\n\n\nImportant\n\n\n\nBelow I frequently use the terminology variable1 vs. variable2. When we write this, the first variable is \\(y\\) (vertical axis) and the second is \\(x\\) (horizontal axis). Thus it’s always \\(y\\) vs. \\(x\\) (NOT \\(x\\) vs. \\(y\\)).\n\n\n\n1.3.1 Scatterplots\n\nFor each of the following pairs of variables, make a scatterplot showing the best fit line and describe the relationship between the variables.\nIn particular address\n\nwhether the association is linear,\nhow strong it is (based purely on the plot), and\nwhat direction (positive, negative, or neither).\n\n\n\nbody mass vs. flipper length\nbill depth vs. flipper length\nbill depth vs. bill length\n\n\n\n1.3.2 Correlations\n\nFor each of the following pairs of variables, find the correlation coefficient \\(r\\).\n\n\nbody mass vs. flipper length\nbill depth vs. flipper length\nbill depth vs. bill length\n\n\n\n1.3.3 Compare associations\nWhich pair of variables has the strongest association? Which has the weakest? Explain how you determined this.\n\n\n1.3.4 Body mass vs. flipper length SLR\nRun the simple linear regression model for body mass vs. flipper length, and display the regression table output.\n\n\n1.3.5 Regression equation\nWrite out the regression equation for this model, using the variable names instead of the generic \\(x\\) and \\(y\\), and inserting the regression coefficient values.\n\n\n1.3.6 \\(b_1\\) calculation\nVery that the formula \\(b_1 = r \\cdot \\frac{s_y}{s_x}\\) holds for this example using the values of the statistics.\n\n\n1.3.7 Interpret intercept\nWrite a sentence interpreting the intercept for this example. Is it meaningful in this example?\n\n\n1.3.8 Interpret slope\nWrite a sentence interpreting the slope for this example.\n\n\n1.3.9 Prediction\nWhat is the expected body mass of a penguin with flipper length 210 mm based on the model?"
  },
  {
    "objectID": "homework/HW_09.html",
    "href": "homework/HW_09.html",
    "title": "Homework 9",
    "section": "",
    "text": "Due Monday, 12/4/23\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F23/blob/main/homework/HW_9_F23_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_09.html#directions",
    "href": "homework/HW_09.html#directions",
    "title": "Homework 9",
    "section": "Directions",
    "text": "Directions\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\n\n\nR & LaTeX code\n\nSee the .qmd files with the code from class notes for LaTeX and R code.\nThe LaTeX code will make it easier to show your work in computations.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_09.html#trends-in-the-residuals",
    "href": "homework/HW_09.html#trends-in-the-residuals",
    "title": "Homework 9",
    "section": "6.12 Trends in the residuals",
    "text": "6.12 Trends in the residuals"
  },
  {
    "objectID": "homework/HW_09.html#guppies-part-iv",
    "href": "homework/HW_09.html#guppies-part-iv",
    "title": "Homework 9",
    "section": "6.20 Guppies, Part IV",
    "text": "6.20 Guppies, Part IV"
  },
  {
    "objectID": "homework/HW_09.html#a-e-helmets-and-lunches",
    "href": "homework/HW_09.html#a-e-helmets-and-lunches",
    "title": "Homework 9",
    "section": "6.26 (a, e) Helmets and lunches",
    "text": "6.26 (a, e) Helmets and lunches\nSkip parts (b)-(d). To complete (e), use that the slope from part (b) is −0.537, and the intercept is 55.34.\nNote: if you have time, it would be good practice to calculate the regression line as well. This was covered on the previous assignment."
  },
  {
    "objectID": "homework/HW_09.html#guppies-part-v",
    "href": "homework/HW_09.html#guppies-part-v",
    "title": "Homework 9",
    "section": "6.28 Guppies, Part V",
    "text": "6.28 Guppies, Part V"
  },
  {
    "objectID": "homework/HW_09.html#a-b-guppies-part-vi",
    "href": "homework/HW_09.html#a-b-guppies-part-vi",
    "title": "Homework 9",
    "section": "6.32 (a, b) Guppies, Part VI",
    "text": "6.32 (a, b) Guppies, Part VI\nSkip part (c)."
  },
  {
    "objectID": "homework/HW_09.html#load-all-the-packages-you-need-below-here.",
    "href": "homework/HW_09.html#load-all-the-packages-you-need-below-here.",
    "title": "Homework 9",
    "section": "1.1 Load all the packages you need below here.",
    "text": "1.1 Load all the packages you need below here."
  },
  {
    "objectID": "homework/HW_09.html#r1-life-expectancy-vs.-co2-emissions",
    "href": "homework/HW_09.html#r1-life-expectancy-vs.-co2-emissions",
    "title": "Homework 9",
    "section": "1.2 R1: Life expectancy vs. CO2 emissions",
    "text": "1.2 R1: Life expectancy vs. CO2 emissions\nUse the dataset Gapminder_2011_LifeExp_CO2.csv\nData were downloaded from https://www.gapminder.org/data/.\n\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same. Source: https://www.gminder.org/data/documentation/gd004/\nCO2 emissions (tons per person) = Carbon dioxide emissions from the burning of fossil fuels (metric tons of CO2 per person). Source: https://cdiac.ess-dive.lbl.gov/\n2011 is the most recent year with the most complete data\n\n\n1.2.1 Load data\nLoad the dataset Gapminder_2011_LifeExp_CO2.csv and do a quick inspection of it. What are the dimensions? Variable names?\n\n\n1.2.2 Linear association?\nMake a scatterplot of life expectancy vs. CO2 emissions per person showing the best fit line and describe the relationship between the variables. In addition comment on whether the relationship looks linear or not.\n\n\n1.2.3 SLR\nRun the simple linear regression of life expectancy vs.CO2 emissions per person, and write out the corresponding regression equation. \n\n\n1.2.4 Prediction\nUsing the regression equation, what is the expected life expectancy for a country with \\(CO_2\\) emissions 20 metric tons per person?\n\n\n1.2.5 Independent data points?\nExplain whether you think the data point are independent of each other or not.\n\n\n1.2.6 Normality of residuals?\nMake a histogram, density plot, and boxplot of the model’s residuals. What is the distribution shape of the residuals? What shape do we want them to have?\n\n\n1.2.7 QQ plot\nMake a QQ plot of the model’s residuals.Explain whether or not the distribution of the residuals deviates from normality and how you made that conclusion based on the QQ plot.\n\n\n1.2.8 Random Normal QQ plots\nMake 5 QQ plots with points randomly generated from a normal distribution, where the number of points matches the sample size used in the linear model.\n\n\n1.2.9 QQ plot comparison\nCompare the QQ plot of the model’s residuals to the randomly generated QQ plots. Is the QQ plot of the residuals similar to the randomly generated plots or different? Based on the these QQ plots, do you think it’s possible that the residuals could be normally distributed?\n\n\n1.2.10 Equality of variance of the residuals?\nMake a residual plot. Describe what the plot looks like and whether there are any patterns in the residuals, and whether the equality of variance assumption the residuals seems to be satisfied or not.\n\n\n1.2.11 Transformation: log(x)\nAdd a new variable to the dataset for the natural logarithm (log()) of the CO2 emissions per person values.\n\n\n1.2.12 Linear association (with transformation)?\nMake a scatterplot of life expectancy vs. log of CO2 emissions per person showing the best fit line and describe the relationship between the variables. In addition comment on whether the relationship looks linear or not.\n\n\n1.2.13 SLR (with transformation)\nRun the simple linear regression of life expectancy vs.CO2 emissions per person, and write out the corresponding regression equation.\n\n\n1.2.14 Prediction (with transformation)\nUsing the regression equation, what is the expected life expectancy for a country with \\(CO_2\\) emissions 20 metric tons per person?\n\n\n1.2.15 Compare predictions from without and with transformation\nCompare the predicted values from the models with and without the transformation. Which is bigger and why? Explain in terms of visually comparing the respective regression lines on the scatterplots.\n\n\n1.2.16 Normality of residuals (with transformation)?\nMake a histogram, density plot, and boxplot of the model’s residuals. What is the distribution shape of the residuals? What shape do we want them to have?\n\n\n1.2.17 QQ plot (with transformation)\nMake a QQ plot of the model’s residuals.Explain whether or not the distribution of the residuals deviates from normality and how you made that conclusion based on the QQ plot.\n\n\n1.2.18 Random Normal QQ plots (with transformation)\nCompare the QQ plot of the model’s residuals to the randomly generated QQ plots (use the ones you generated above). Is the QQ plot of the residuals similar to the randomly generated plots or different? Based on the these QQ plots, do you think it’s possible that the residuals could be normally distributed?\n\n\n1.2.19 Equality of variance of the residuals (with transformation)?\nMake a residual plot. Describe what the plot looks like and whether there are any patterns in the residuals, and whether the equality of variance assumption the residuals seems to be satisfied or not.\n\n\n1.2.20 Comparison of models without and with transformation\nWhich of the models do you think has a better fit? Make sure your explanation comments on each of the LINE assumptions, and also compare the \\(R^2\\) values from the models."
  },
  {
    "objectID": "homework/HW_09.html#npt-1-sign-test",
    "href": "homework/HW_09.html#npt-1-sign-test",
    "title": "Homework 9",
    "section": "2.1 NPT 1: Sign test",
    "text": "2.1 NPT 1: Sign test\nVegetarian diet and cholesterol levels\nWhen covering paired t-tests on Day 10 Part 2, the class notes used the example of testing whether a vegetarian diet changed cholesterol levels. The data are in the file chol213.csv at https://niederhausen.github.io/BSTA_511_F23/data/chol213.csv. In this exercise we will use non-parametric tests to test for a change and compare the results to the paired t-test.\n\n2.1.1 Hypotheses\nWhat are the hypotheses for the (Wilcoxon) Signed-rank test (2-sided) in the context of the problem?\n\n\n2.1.2 \\(D^+\\) and \\(D^-\\)\nCalculate \\(D^+\\) and \\(D^-\\), the number of positive and negative differences when the differences are calculated as After - Before.\n\n\n2.1.3 Probability distribution\nWhat probability distribution can be used to model the number of positive differences? Make sure to specify its parameters.\n\n\n2.1.4 Exact probability\nFind the exact probability that there were at most 3 positive differences.\n\n\n2.1.5 Sign test in R\nRun the sign test in R. What is the sign test p-value and how does it compare to the p-value of the paired t-test (check the class notes for this)? \n\n\n2.1.6 S\nThe sign test output includes the value for S. What is S and what does it measure?\n\n\n2.1.7 p-value\nDoes the probability that there were at most 3 positive differences match the p-value from the R output? Why or why not?\n\n\n2.1.8 Normal approximation\nWould it be appropriate to use a normal approximation to calculate the p-value for this test? Why or why not?"
  },
  {
    "objectID": "homework/HW_09.html#npt-2-wilcoxon-signed-rank-test",
    "href": "homework/HW_09.html#npt-2-wilcoxon-signed-rank-test",
    "title": "Homework 9",
    "section": "2.2 NPT 2: (Wilcoxon) Signed-rank test",
    "text": "2.2 NPT 2: (Wilcoxon) Signed-rank test\nVegetarian diet and cholesterol levels\nWhen covering paired t-tests on Day 10 Part 2, the class notes used the example of testing whether a vegetarian diet changed cholesterol levels. The data are in the file chol213.csv at https://niederhausen.github.io/BSTA_511_F23/data/chol213.csv. In this exercise we will use non-parametric tests to test for a change and compare the results to the paired t-test.\n\n2.2.1 Hypotheses\nWhat are the hypotheses for the sign test (2-sided) in the context of the problem?\n\n\n2.2.2 Signed ranks\nFind the signed ranks. Make sure to account for ties when doing so.\n\n\n2.2.3 \\(T^+\\)\nCalculate the sum of the positive ranks ( \\(T^+\\) ) \n\n\n2.2.4 Exact p-value\nCan an exact p-value for the (Wilcoxon) Signed-rank test be calculated? Why or why not?\n\n\n2.2.5 Normal approximation\nIs it appropriate to use the normal approximation method in this case?\n\n\n2.2.6 Test in R\nRun the (Wilcoxon) Signed-rank test in R. What is the p-value and how does it compare to the p-value of the sign test and the paired t-test (check the class notes for this)?\n\n\n2.2.7 Condition\nThere’s one more condition that should be satisfied to use the (Wilcoxon) Signed-rank test that has not been asked about yet in these questions. What is it and do you think it’s satisfied?"
  },
  {
    "objectID": "homework/HW_09.html#npt-3-wilcoxon-rank-sum-test",
    "href": "homework/HW_09.html#npt-3-wilcoxon-rank-sum-test",
    "title": "Homework 9",
    "section": "2.3 NPT 3: Wilcoxon rank-sum test",
    "text": "2.3 NPT 3: Wilcoxon rank-sum test\nDoes caffeine increase finger taps/min?\nWhen covering 2-sample t-tests on Day 11, the class notes used the example of testing whether caffeine increases finger taps/min. The data are in the file CaffeineTaps.csv at https://niederhausen.github.io/BSTA_511_F23/data/CaffeineTaps.csv. In this exercise we will use a non-parametric test and compare the results to the paired t-test.\n\n2.3.1 Condition\nWhat condition needs to be satisfied to apply the Wilcoxon rank-sum test and is it satisfied for these data?\nAnswer the following questions using the Wilcoxon rank-sum test whether you think the condition has been satisfied or not.\n\n\n2.3.2 Why Wilcoxon rank-sum test?\nHow would we know to use the Wilcoxon rank-sum test instead of the sign test or (Wilcoxon) Signed-rank test?\n\n\n2.3.3 Hypotheses\nWhat are the hypotheses for the Wilcoxon rank-sum test (1-sided) in the context of the problem? \n\n\n2.3.4 Exact test in R\nRun the exact Wilcoxon rank-sum test in R. What warning(s) does it give you?\n\n\n2.3.5 Normal approximation test in R\nRun the Wilcoxon rank-sum test in R with the normal approximation. Comment on whether it is appropriate to use the normal approximation or not in this case.\n\n\n2.3.6 p-value\nWhat is the p-value and how does it compare to the p-value of the 2-sample t-test (check the class notes for this)?\n\n\n2.3.7 Conclusion\nWrite a conclusion to the test in the context of the problem."
  },
  {
    "objectID": "homework/HW_03.html",
    "href": "homework/HW_03.html",
    "title": "Homework 3",
    "section": "",
    "text": "Updated 10/18/23: Moved questions on Day 7 material to HW 4.\nDue 10/21/23\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F23/blob/main/homework/HW_3_F23_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_03.html#directions",
    "href": "homework/HW_03.html#directions",
    "title": "Homework 3",
    "section": "Directions",
    "text": "Directions\n\n\n\n\n\n\nImportant\n\n\n\n\nThe non-R exercises (see sections Book exercises and Non-book exercise) may be completed not using Quarto. I especially recommend writing out by hand the chapter 3 probability questions, whether on paper or a tablet.\n\nSome problems involve R code to calculate a probability, but the code is brief and you can write out the code and the answer by hand.\n\nIf you are completing the homework on paper, you can use a scanning app, such as Adobe Scan, to create a pdf of your assignment.\n\n\n\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_03.html#baggage-fees",
    "href": "homework/HW_03.html#baggage-fees",
    "title": "Homework 3",
    "section": "3.4 Baggage fees",
    "text": "3.4 Baggage fees"
  },
  {
    "objectID": "homework/HW_03.html#gull-clutch-size",
    "href": "homework/HW_03.html#gull-clutch-size",
    "title": "Homework 3",
    "section": "3.5 Gull clutch size",
    "text": "3.5 Gull clutch size\nReview the solution in the back of the book for exercise 3.5.\n\n(a)\nFor part (a), the answer is correct, but there is an error in the work and the notation is sloppy. Correct the error and rewrite the solution with proper notation.\n\n\n(b)\nFor part (b), the answer is not correct, as a result of the error in the work with sloppy notation. Give a correct solution with proper notation."
  },
  {
    "objectID": "homework/HW_03.html#scooping-ice-cream",
    "href": "homework/HW_03.html#scooping-ice-cream",
    "title": "Homework 3",
    "section": "3.6 Scooping ice cream",
    "text": "3.6 Scooping ice cream"
  },
  {
    "objectID": "homework/HW_03.html#chickenpox-part-i.",
    "href": "homework/HW_03.html#chickenpox-part-i.",
    "title": "Homework 3",
    "section": "3.8 Chickenpox, Part I.",
    "text": "3.8 Chickenpox, Part I.\nFor #3.8, calculate the binomial probabilities two ways:\n(1) using the formula (can use R to calculate factorials or the choose function) and\n(2) using R functions for binomial distribution probabilities."
  },
  {
    "objectID": "homework/HW_03.html#chickenpox-part-ii.",
    "href": "homework/HW_03.html#chickenpox-part-ii.",
    "title": "Homework 3",
    "section": "3.10 Chickenpox, Part II.",
    "text": "3.10 Chickenpox, Part II.\nFor #3.10, you can use R functions to calculate the binomial probabilities instead of directly using the formula. However, include the mathematical formulas that would be used to calculate the probabilities."
  },
  {
    "objectID": "homework/HW_03.html#instructions-for-normal-probability-exercises",
    "href": "homework/HW_03.html#instructions-for-normal-probability-exercises",
    "title": "Homework 3",
    "section": "!!! Instructions for Normal probability exercises !!!",
    "text": "!!! Instructions for Normal probability exercises !!!\n\n\n\n\n\n\nImportant\n\n\n\nAdditional Instructions - IMPORTANT!!!\n\nFor ALL normal distribution exercises:\n\nmake a sketch of the normal distribution curve with the mean and 1 sd away from the mean clearly labeled, and the area representing probability of interest shaded in.\ncalculate probabilities using both\n\nz-table\nR"
  },
  {
    "objectID": "homework/HW_03.html#clinican-time-with-patients",
    "href": "homework/HW_03.html#clinican-time-with-patients",
    "title": "Homework 3",
    "section": "1.1 Clinican time with patients",
    "text": "1.1 Clinican time with patients\nSuppose a clinician schedules 20 minutes to spend with each of their patients. However, they sometimes run over or end earlier. Based on past data, the mean “extra” time they spend with a patient is 3 minutes with a standard deviation of 2 minutes. Suppose they see 13 patients today and the extra times they spend with patients are independent from patient to patient.\n\n1.1.1 Expected total time\nFind the expected total time they will spend with all of their patients today.\n\n\n1.1.2 SD of total time\nFind the standard deviation of the total time they will spend with all of their patients today."
  },
  {
    "objectID": "homework/HW_05.html",
    "href": "homework/HW_05.html",
    "title": "Homework 5",
    "section": "",
    "text": "Due 11/4/23\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F23/blob/main/homework/HW_5_F23_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_05.html#directions",
    "href": "homework/HW_05.html#directions",
    "title": "Homework 5",
    "section": "Directions",
    "text": "Directions\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\n\n\nR & LaTeX code for probability distributions\n\nSee the .qmd files with the code from class notes for LaTeX and R code.\nThe LaTeX code will make it easier to show your work in computations.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_05.html#mental-health-part-i",
    "href": "homework/HW_05.html#mental-health-part-i",
    "title": "Homework 5",
    "section": "4.4 Mental health, Part I",
    "text": "4.4 Mental health, Part I"
  },
  {
    "objectID": "homework/HW_05.html#thanksgiving-spending-part-i",
    "href": "homework/HW_05.html#thanksgiving-spending-part-i",
    "title": "Homework 5",
    "section": "4.6 Thanksgiving spending, Part I",
    "text": "4.6 Thanksgiving spending, Part I"
  },
  {
    "objectID": "homework/HW_05.html#age-at-first-marriage-part-i",
    "href": "homework/HW_05.html#age-at-first-marriage-part-i",
    "title": "Homework 5",
    "section": "4.8 Age at first marriage, Part I",
    "text": "4.8 Age at first marriage, Part I"
  },
  {
    "objectID": "homework/HW_05.html#working-backwards-part-ii",
    "href": "homework/HW_05.html#working-backwards-part-ii",
    "title": "Homework 5",
    "section": "5.6 Working backwards, Part II",
    "text": "5.6 Working backwards, Part II"
  },
  {
    "objectID": "homework/HW_05.html#t-vs.-z",
    "href": "homework/HW_05.html#t-vs.-z",
    "title": "Homework 5",
    "section": "5.10 t⋆ vs. z⋆",
    "text": "5.10 t⋆ vs. z⋆"
  },
  {
    "objectID": "homework/HW_05.html#auto-exhaust-and-lead-exposure",
    "href": "homework/HW_05.html#auto-exhaust-and-lead-exposure",
    "title": "Homework 5",
    "section": "5.12 Auto exhaust and lead exposure",
    "text": "5.12 Auto exhaust and lead exposure"
  },
  {
    "objectID": "homework/HW_05.html#paired-or-not-part-ii",
    "href": "homework/HW_05.html#paired-or-not-part-ii",
    "title": "Homework 5",
    "section": "5.16 Paired or not, Part II",
    "text": "5.16 Paired or not, Part II"
  },
  {
    "objectID": "homework/HW_05.html#ddt-exposure",
    "href": "homework/HW_05.html#ddt-exposure",
    "title": "Homework 5",
    "section": "5.22 DDT exposure",
    "text": "5.22 DDT exposure"
  },
  {
    "objectID": "homework/HW_05.html#load-all-the-packages-you-need-below-here.",
    "href": "homework/HW_05.html#load-all-the-packages-you-need-below-here.",
    "title": "Homework 5",
    "section": "1.1 Load all the packages you need below here.",
    "text": "1.1 Load all the packages you need below here."
  },
  {
    "objectID": "homework/HW_05.html#r1-youth-weights",
    "href": "homework/HW_05.html#r1-youth-weights",
    "title": "Homework 5",
    "section": "1.2 R1: Youth weights",
    "text": "1.2 R1: Youth weights\nIn this exercise you will use the YRBSS dataset again that we used in class on Days 8-9, to simulate the distribution of mean weights from repeated samples.\n\n1.2.1 Use the set.seed() command to set a randomization seed. Use whatever number you want for the seed.\n\n\n1.2.2 CI\nSuppose you took a random sample of size 50 from the YRBSS data, that has mean weight 130 pounds. Calculate and interpret a 90% confidence interval using the standard deviation of weights from the YRBSS “population.”\n\n\n1.2.3 Another CI\nCalculate and interpret a 90% confidence interval assuming the standard deviation of weights from the random sample is 40."
  },
  {
    "objectID": "homework/HW_05.html#r2-swim-times",
    "href": "homework/HW_05.html#r2-swim-times",
    "title": "Homework 5",
    "section": "1.3 R2: Swim times",
    "text": "1.3 R2: Swim times\n\nIn these exercises you will use R to work through the swim times example from Section 5.2 in the textbook.\nThe data are in the oibiostats package and called swim.\n\n\n1.3.1 Mean & SD of differences\nCalculate the mean and standard deviation for the differences in swim times, and compare them to the ones in the book. Which order were the differences calculated, wet suit - swim suit or the opposite? Were all the differences positive?\n\n\n1.3.2 Dot plot of differences\nCreate a dot plot of the differences in swim times and comment on the distribution shape.\n\n\n1.3.3 Hypothesis test\nRun the appropriate statistical test in R as both a one-sample t-test and a paired t-test to verify the test statistic, p-value, and CI in the text. Use inline R code to pull these values from the test output when writing up your comparison of these values to the book’s values."
  },
  {
    "objectID": "homework/HW_06.html",
    "href": "homework/HW_06.html",
    "title": "Homework 6",
    "section": "",
    "text": "Updated 11/6/23: moved Day 12 questions to HW 7\nDue 11/11/23\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F23/blob/main/homework/HW_6_F23_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_06.html#directions",
    "href": "homework/HW_06.html#directions",
    "title": "Homework 6",
    "section": "Directions",
    "text": "Directions\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\n\n\nR & LaTeX code\n\nSee the .qmd files with the code from class notes for LaTeX and R code.\nThe LaTeX code will make it easier to show your work in computations.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly.\n\n\n\n\nHypothesis test & CI instructions\n\n\n\n\n\n\nImportant\n\n\n\n\nFor book exercises, make sure to include all steps in a hypothesis test (where applicable) as outlined in the class notes.\nDo not forget to include a discussion on whether you think the test (or CI) conditions have been satisfied. Are there assumptions you need to make in order for them to be satisfied? Whether you believe they are satisfied or not, continue to run the hypothesis test (or CI) as instructed."
  },
  {
    "objectID": "homework/HW_06.html#egg-volume",
    "href": "homework/HW_06.html#egg-volume",
    "title": "Homework 6",
    "section": "5.26 Egg volume",
    "text": "5.26 Egg volume"
  },
  {
    "objectID": "homework/HW_06.html#placebos-without-deception",
    "href": "homework/HW_06.html#placebos-without-deception",
    "title": "Homework 6",
    "section": "5.34 Placebos without deception",
    "text": "5.34 Placebos without deception"
  },
  {
    "objectID": "homework/HW_06.html#pss1-4.22-testing-for-food-safety.",
    "href": "homework/HW_06.html#pss1-4.22-testing-for-food-safety.",
    "title": "Homework 6",
    "section": "1.1 PSS1: 4.22 Testing for food safety.",
    "text": "1.1 PSS1: 4.22 Testing for food safety.\nDo exercise 4.22 from textbook."
  },
  {
    "objectID": "homework/HW_06.html#pss2-auto-exhaust-and-lead-exposure-revisited.",
    "href": "homework/HW_06.html#pss2-auto-exhaust-and-lead-exposure-revisited.",
    "title": "Homework 6",
    "section": "1.2 PSS2: Auto exhaust and lead exposure revisited.",
    "text": "1.2 PSS2: Auto exhaust and lead exposure revisited.\n\n1.2.1 Power\nIn exercise 5.12, we tested whether police officers appear to have been exposed to a higher concentration of lead than 35. Calculate the power for the hypothesis test and include an interpretation of the power in the context of the research question. Was it sufficiently powered?\n\n\n1.2.2 Sample size\nFor the same test, what sample size would be needed for 80% power? How about 90% power? Would it be reasonable to conduct the study with these sample sizes? Why or why not?\n\n\n1.2.3 Effect size\nSuppose the study has resources to include 30 people. What minimum effect size would they be able to detect with 85% power assuming the same sample mean and standard deviation. Use \\(\\alpha\\) = 0.05.\n\n\n1.2.4 2-sided vs. 1-sided\nContinuing with the previous question, what happens to the effect size they can detect if the test is two sided instead of one-sided?"
  },
  {
    "objectID": "homework/HW_06.html#load-all-the-packages-you-need-below-here.",
    "href": "homework/HW_06.html#load-all-the-packages-you-need-below-here.",
    "title": "Homework 6",
    "section": "2.1 Load all the packages you need below here.",
    "text": "2.1 Load all the packages you need below here."
  },
  {
    "objectID": "homework/HW_06.html#r1-dds-expenditures-by-ethnicity",
    "href": "homework/HW_06.html#r1-dds-expenditures-by-ethnicity",
    "title": "Homework 6",
    "section": "2.2 R1: DDS expenditures by ethnicity",
    "text": "2.2 R1: DDS expenditures by ethnicity\n\nIn these exercises you will use R to work through the discrimination in developmental disability support example from Section 5.3.4 (pg. 253) in the textbook.\nThe data are in the oibiostats package and called dds.discr.\n\n\n2.2.1 New dataset\nCreate a new dataset that only includes the White (non Hispanic) and Hispanic ethnicities. Use this new dataset for the following questions.\n\n\n2.2.2 Data viz\nCreate density plots and box plots of the expenditures stratified by ethnicity. Comment on the distribution shapes. Are there any outliers?\n\n\n2.2.3 t-test conditions\nAre the conditions for a t-test comparing the mean expenditures of the two ethnicities satisfied?\n\n\n2.2.4 Log-transformation\nThe book recommends log-transforming the expenditure values before testing. Create a new column in the dataset with the transformed values. The R command for the natural logarithm is log(). \n\n\n2.2.5 Data viz: log-transformed expenditures\nCreate density plots and box plots of the log-transformed expenditures stratified by ethnicity. Comment on the distribution shapes. Are there any outliers?\n\n\n2.2.6 t-test conditions: log-transformed expenditures\nAre the conditions for a t-test comparing the mean log-transformed expenditures of the two ethnicities satisfied?\n\n\n2.2.7 Summary stats: log-transformed expenditures\nCalculate the means, standard deviations, and sample sizes for the log-transformed expenditures stratified by ethnicity, and compare them to the ones in the book. Which group had a larger mean?\n\n\n2.2.8 Test\nRun the appropriate statistical test in R to verify the test statistic in the text and get the actual p-value. In which order was the difference in means calculated, and is this same as in the book? Use inline R code to pull these values from the test output when writing up your comparison of these values to the book’s values.\n\n\n2.2.9 df\nHow do the degrees of freedom (df) from the hypothesis test compare to the df used by the book? Why are they different? Which degrees df (book vs. test output) leads to a bigger p-value?\n\n\n2.2.10 CI\nWhat is the 95% CI? Write an interpretation of the CI in the context of the research question. \n\n\n2.2.11 Test original expenditure values\nRun the appropriate statistical test in R using the original expenditure values. What are the test statistic and p-value? Does the conclusion of the test change?\n\n\n2.2.12 CI using original expenditure values\nWhat is the 95% CI? Write an interpretation of the CI in the context of the research question. Which of the CI’s (log-transformed vs not) is easier to interpret?\n\n\n2.2.13 Age groups\nThe book’s example goes on to analyze the data stratified by age groups, since age is a confounder in expenditure amounts. Create two new datasets restricted to the age groups 13-17 and 22-50, respectively.\n\n\n2.2.14 Data viz by age groups\nCreate density plots and box plots of the expenditures stratified by ethnicity for each of the age groups separately. Comment on the distribution shapes. Are there any outliers?\n\n\n2.2.15 t-test conditions: age groups\nAre the conditions for a t-test comparing the mean expenditures of the two ethnicities satisfied for either or both of the age groups?\n\n\n2.2.16 Summary stats: age groups\nCalculate the means, standard deviations, and sample sizes for the expenditures stratified by ethnicity and the age groups, and compare them to the ones in the book. Which group had a larger mean?\n\n\n2.2.17 t-test: age groups\nRun the appropriate statistical tests for both age groups in R to verify the test statistics, df’s, and p-values in the text. In which order were the differences in means calculated, and are they the same as in the book? Use inline R code to pull these values from the test output when writing up your comparison of these values to the book’s values.\n\n\n2.2.18 CI: age groups\nWhat are the 95% CI’s for each of the age groups? Write interpretations of the CI’s in the context of the research question. Does they suggest there are differences in expenditures between the two ethnicities? Why or why not?\n\n\n2.2.19 Discrimination in DDS expenditures?\nEven though the p-values for the age-stratified tests were not significant, is it possible that there was discrimination in DDS expenditures?"
  },
  {
    "objectID": "lessons.html",
    "href": "lessons.html",
    "title": "Lessons",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "",
    "text": "Artwork by @allison_horst\n\n\n\n\n\n\n\nA programming language\nFocus on statistical modeling and data analysis\n\nimport data, manipulate data, run statistics, make plots\n\nUseful for data science\nGreat visualizations\nAlso useful for most anything else you’d want to tell a computer to do\nInterfaces with other languages i.e. python, C++, bash\n\n\n\n\n\nFor the history and details: Wikipedia\n\nan interpreted language (run it through a command line)\nprocedural programming with functions\nWhy “R”?? Scheme inspired S (invented at Bell Labs in 1976) which inspired R since 1st letters of original authors (free and open source! in 2000)\n\n\n\n\n\n\nR is a programming language\n\nRStudio is an integrated development environment (IDE)\n= an interface to use R (with perks!)\n\n\n\n\n\nModern Dive\n\n\n\n\n\n\n\n\nModern Dive\n\n\n\n\n\n\n\n\nEmma Rand\n\n\nRead more about RStudio’s layout in Section 3.4 of “Getting Used to R, RStudio, and R Markdown” (Ismay and Kennedy 2016)"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#what-is-r",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#what-is-r",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "",
    "text": "A programming language\nFocus on statistical modeling and data analysis\n\nimport data, manipulate data, run statistics, make plots\n\nUseful for data science\nGreat visualizations\nAlso useful for most anything else you’d want to tell a computer to do\nInterfaces with other languages i.e. python, C++, bash\n\n\n\n\n\nFor the history and details: Wikipedia\n\nan interpreted language (run it through a command line)\nprocedural programming with functions\nWhy “R”?? Scheme inspired S (invented at Bell Labs in 1976) which inspired R since 1st letters of original authors (free and open source! in 2000)"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#what-is-rstudio",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#what-is-rstudio",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "",
    "text": "R is a programming language\n\nRStudio is an integrated development environment (IDE)\n= an interface to use R (with perks!)\n\n\n\n\n\nModern Dive"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#open-rstudio-on-your-computer-not-r",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#open-rstudio-on-your-computer-not-r",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "",
    "text": "Modern Dive"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#rstudio-anatomy",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#rstudio-anatomy",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "",
    "text": "Emma Rand\n\n\nRead more about RStudio’s layout in Section 3.4 of “Getting Used to R, RStudio, and R Markdown” (Ismay and Kennedy 2016)"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#coding-in-the-console",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#coding-in-the-console",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Coding in the console",
    "text": "Coding in the console\n\n\nWhen you first open R, the console should be empty.\n\n\n\n\n\n\nTyping and executing code in the console \n\nType code in the console (blue text)\nPress return to execute the code\nOutput shown below in black"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#math-calculations-using-r",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#math-calculations-using-r",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Math calculations using R",
    "text": "Math calculations using R\n\nRules for order of operations are followed\nSpaces between numbers and characters are ignored\n\n\n\n\n10^2\n\n[1] 100\n\n3 ^ 7\n\n[1] 2187\n\n6/9\n\n[1] 0.6666667\n\n9-43\n\n[1] -34\n\n\n\n\n4^3-2* 7+9 /2\n\n[1] 54.5\n\n\nThe equation above is computed as \\[4^3 − (2 \\cdot 7) + \\frac{9}{2}\\]"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#variables-saved-r-objects",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#variables-saved-r-objects",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Variables (saved R objects)",
    "text": "Variables (saved R objects)\nVariables are used to store data, figures, model output, etc.\n\n\n\nCan assign a variable using either = or &lt;-\n\nUsing &lt;- is preferable\n\n\nAssign just one value:\n\nx = 5\nx\n\n[1] 5\n\nx &lt;- 5\nx\n\n[1] 5\n\n\n\n\n\nAssign a vector of values\n\nConsecutive integers using :\n\n\na &lt;- 3:10\na\n\n[1]  3  4  5  6  7  8  9 10\n\n\n\nConcatenate a string of numbers\n\n\nb &lt;- c(5, 12, 2, 100, 8)\nb\n\n[1]   5  12   2 100   8"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#doing-math-with-variables",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#doing-math-with-variables",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Doing math with variables",
    "text": "Doing math with variables\n\n\nMath using variables with just one value\n\nx &lt;- 5\nx\n\n[1] 5\n\nx + 3\n\n[1] 8\n\ny &lt;- x^2\ny\n\n[1] 25\n\n\n\nMath on vectors of values:\nelement-wise computation\n\na &lt;- 3:6\na\n\n[1] 3 4 5 6\n\na+2; a*3\n\n[1] 5 6 7 8\n\n\n[1]  9 12 15 18\n\na*a\n\n[1]  9 16 25 36"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#variables-can-include-text-characters",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#variables-can-include-text-characters",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Variables can include text (characters)",
    "text": "Variables can include text (characters)\n\nhi &lt;- \"hello\"\nhi\n\n[1] \"hello\"\n\ngreetings &lt;- c(\"Guten Tag\", \"Hola\", hi)\ngreetings\n\n[1] \"Guten Tag\" \"Hola\"      \"hello\""
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#using-functions",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#using-functions",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Using functions",
    "text": "Using functions\n\nmean() is an example of a function\nfunctions have “arguments” that can be specified within the ()\n?mean in console will show help file for mean()\n\n\n\nFunction arguments specified by name:\n\nmean(x = 1:4)\n\n[1] 2.5\n\n\n\nseq(from = 1, to = 12, by = 3)\n\n[1]  1  4  7 10\n\nseq(by = 3, to = 12, from = 1)\n\n[1]  1  4  7 10\n\n\n\nFunction arguments not specified, but listed in order:\n\nmean(1:4)\n\n[1] 2.5\n\n\n\nseq(1, 12, 3)\n\n[1]  1  4  7 10"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#common-console-errors-12",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#common-console-errors-12",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Common console errors (1/2)",
    "text": "Common console errors (1/2)\nIncomplete commands\n\n\n\nWhen the console is waiting for a new command, the prompt line begins with &gt;\n\nIf the console prompt is +, then a previous command is incomplete\nYou can finish typing the command in the console window\n\n\n\nExample:\n\n&gt; 3 + (2*6\n+ )\n\n[1] 15"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#common-console-errors-22",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#common-console-errors-22",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Common console errors (2/2)",
    "text": "Common console errors (2/2)\nObject is not found\n\nThis happens when text is entered for a non-existent variable (object)\n\nExample:\n\nhello\n\nError in eval(expr, envir, enclos): object 'hello' not found\n\n\n\nCan be due to missing quotes\n\n\ninstall.packages(dplyr) \n\nError in eval(expr, envir, enclos): object 'dplyr' not found\n\n# correct code is: install.packages(\"dplyr\")"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#example-creating-an-html-file",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#example-creating-an-html-file",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Example: creating an html file",
    "text": "Example: creating an html file\n\n\n.qmd file \n\nhtml output"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#quarto-.qmd-file-code-text",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#quarto-.qmd-file-code-text",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Quarto = .qmd file = Code + text",
    "text": "Quarto = .qmd file = Code + text\nknitr is a package that converts .qmd files containing code + markdown syntax to a plain text .md markdown file, and then to other formats (html, pdf, Word, etc)\n\n\n\nArtwork from “Hello, Quarto” keynote by Julia Lowndes and Mine Çetinkaya-Rundel, presented at RStudio Conference 2022. Illustrated by Allison Horst."
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#create-a-quarto-file-.qmd",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#create-a-quarto-file-.qmd",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "1. Create a Quarto file (.qmd)",
    "text": "1. Create a Quarto file (.qmd)\nTwo options:\n\nclick on File \\(\\rightarrow\\) New File \\(\\rightarrow\\) Quarto Document…\\(\\rightarrow\\) OK,\nor in upper left corner of RStudio click on  \\(\\rightarrow\\) \n\n\n\nPop-up window selections:\n\nEnter a title and your name\nSelect HTML output format (default)\nEngine: select Knitr\nEditor: Select Use visual markdown editor\nClick Create"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#create-a-quarto-file-.qmd-1",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#create-a-quarto-file-.qmd-1",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "2. Create a Quarto file (.qmd)",
    "text": "2. Create a Quarto file (.qmd)\n\nAfter clicking on Create, you should then see the following in your editor window:"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#save-the-quarto-file-.qmd",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#save-the-quarto-file-.qmd",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "3. Save the Quarto file (.qmd)",
    "text": "3. Save the Quarto file (.qmd)\n\nSave the file by\n\nselecting File -&gt; Save,\nor clicking on  (towards the left above the scripting window),\nor keyboard shortcut\n\nPC: Ctrl + s\nMac: Command + s\n\n\nYou will need to specify\n\na filename to save the file as\n\nALWAYS use .qmd as the filename extension for Quarto files\n\nthe folder to save the file in"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#create-html-file",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#create-html-file",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "4. Create html file",
    "text": "4. Create html file\nWe create the html file by rendering the .qmd file.\nTwo options:\n\nclick on the Render icon  at the top of the editor window,\nor use keyboard shortcuts\n\nMac: Command+Shift+K\nPC: Ctrl+Shift+K\n\n\n\nA new window will open with the html output.\nYou will now see both .qmd and .html files in the folder where you saved the .qmd file.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe template .qmd file that RStudio creates will render to an html file by default.\nThe output format can be changed to create a Word doc, pdf, slides, etc."
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#qmd-file-vs.-its-html-output",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#qmd-file-vs.-its-html-output",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": ".qmd file vs. its html output",
    "text": ".qmd file vs. its html output\n\n\n.qmd file \n\nhtml output"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#formatting-text",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#formatting-text",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Formatting text",
    "text": "Formatting text\n\nbold, italics, superscripts & subscripts, strikethrough, verbatim, etc.\n\n\nText is formatted through a markup language called Markdown (Wikipedia)\n\nOther markup languages include html (webapges) and LaTeX (math)\nAll text formatting is specified via code\n“Markdown is a plain text format that is designed to be easy to write, and, even more importantly, easy to read” 1\n\nNewer versions of RStudio include a Visual editor as well that makes formatting text similar to using a word processor."
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#formatting-text-visual-editor",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#formatting-text-visual-editor",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Formatting text: Visual editor",
    "text": "Formatting text: Visual editor\n\nUsing the Visual editor is similar to using a wordprocessor, such as Word\nKeyboard shortcuts usually work as well (shown for Mac below)"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#practice",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#practice",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Practice",
    "text": "Practice\n\nPart 1\n\nUsing the visual editor, practice formatting text in your qmd file, such as making text bold, italicized, and in code format.\nAdd 1st, 2nd, and 3rd level headers\nAdd a list with a\n\nsub-list (bullet and/or numbered)\n\nAdd a table\nAdd whatever else you are interested in!\n\nPart 2\n\nSwitch back to the Source editor and examine the markdown code that was used for the formatting.\n\n\nQuestions:\n\nWhat went smoothly?\nWhat hurdles did you encounter?"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#formatting-text-markdown",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#formatting-text-markdown",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Formatting text: Markdown",
    "text": "Formatting text: Markdown\n\n\n\n\n\n\n\nMarkdown:\nOutput:\n\n\n\n\n*This text is in italics*, but _so is this text_.\nThis text is in italics, but so is this text.\n\n\n**Bold** also has __2 options__\nBold also has 2 options\n\n\n~~Should this be deleted?~~\nShould this be deleted?\n\n\nNeed^super^ or~sub~ scripts?\nNeedsuper orsub scripts?\n\n\n`Code is often formatted as verbatim`\nCode is often formatted as verbatim\n\n\n&gt;This is a block quote.\n\nThis is a block quote."
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#headers",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#headers",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Headers",
    "text": "Headers\n\nOrganize your documents using headers to create sections and subsections\nUse # at the beginning of the line to create headers\n\n\n\nText in editor:\n\n\n\n\n\n\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure there is no space before the #, and there IS a space after the # in order for the header to work properly."
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#rstudio-tip",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#rstudio-tip",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "RStudio tip",
    "text": "RStudio tip\nYou can easily navigate through your .qmd file if you use headers to outline your text"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#code-chunks",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#code-chunks",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Code chunks",
    "text": "Code chunks\n\n\n.qmd file \n\nhtml output"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#create-a-code-chunk",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#create-a-code-chunk",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Create a code chunk",
    "text": "Create a code chunk\n3 options to create a code chunk\n\nClick on  at top right of the editor window, or\nKeyboard shortcut\n\n\n\n\nMac\nCommand + Option + I\n\n\nPC\nCtrl + Alt + I\n\n\n\n\nVisual editor: Select Insert -&gt; Executable Cell -&gt; R"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#what-does-a-code-chunk-look-like",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#what-does-a-code-chunk-look-like",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "What does a code chunk look like?",
    "text": "What does a code chunk look like?\nAn empty code chunk looks like this:\nVisual editor\n\nSource editor\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that a code chunks start with ```{r} and ends with ```. Make sure there is no space before ```."
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#enter-and-run-code-1n",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#enter-and-run-code-1n",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Enter and run code (1/n)",
    "text": "Enter and run code (1/n)\n\nType R code inside code chunks\nSelect code you want to run, by\n\nplacing the cursor in the line of code you want to run,\nor highlighting the code you want to run\n\n\n\n\n\nRun selected code by\n\nclicking on the  button in the top right corner of the scripting window and choosing Run Selected Line(s),\nor typing one of the following key combinations:\n\n\n\n\n\nMac\nctrl + return\n\n\nPC\ncommand + return\n\n\n\n\nWhere does the output appear?"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#enter-and-run-code-2n",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#enter-and-run-code-2n",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Enter and run code (2/n)",
    "text": "Enter and run code (2/n)\n\n\nRun all code in a chunk by\n\nby clicking the play button in the top right corner of the chunk\n\nThe code output appears below the code chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe output should also appear in the Console.\nSettings can be changed so that the output appears only in the Console and not below the code chunk:\n\nSelect  (to right of Render button) and then Chunk Output in Console."
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#useful-keyboard-shortcuts",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#useful-keyboard-shortcuts",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Useful keyboard shortcuts",
    "text": "Useful keyboard shortcuts\nFull list of keyboard shortcuts\n \n\n\n\n\n\n\n\n\naction\nmac\nwindows/linux\n\n\n\n\nRun code in qmd (or script)\ncmd + enter\nctrl + enter\n\n\n&lt;-\noption + -\nalt + -\n\n\ninterrupt currently running command\nesc\nesc\n\n\nin console, retrieve previously run code\nup/down\nup/down\n\n\nkeyboard shortcut help\noption + shift + k\nalt + shift + k\n\n\n\n\n\n\nPractice\nTry typing code below in your qmd (with shortcut) and evaluating it:\n\ny &lt;- 5\ny"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#yaml-metadata",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#yaml-metadata",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "YAML metadata",
    "text": "YAML metadata\nMany output options can be set in the YAML metadata, which is the first set of code in the file starting and ending with ---.\n\nIt sets the configuration specifications for the output file\nYAML is an acronym for\n\nyet another markup language, or\nYAML ain’t markup language"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#simple-yaml-example",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#simple-yaml-example",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Simple YAML example",
    "text": "Simple YAML example\n\n\nThe default YAML includes a title and author that appear at the top of the output file. In the example below, I also added in a date option\n\n\n\n\nYAML:\n\n---\ntitle: \"My first Quarto file\"\nauthor: \"Meike\"\ndate: \"9/25/2023\"\nformat: html\neditor: visual\n---\n\n\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nThe YAML must start and end with 3 dashes ---.\nThe first set of --- must be on the very first line."
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#change-the-output-file-type",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#change-the-output-file-type",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Change the output file type",
    "text": "Change the output file type\n\n\n\nThe YAML specifies the format of the output file:\n\nhtml, Word, pdf, slides, website, book, etc.\n\nThis is done by changing the format: option\n\n\n\n\n\nIllustration by Alison Hill and Allison Horst, for RStudio.\n\n\n\n\n\n\n\n---\ntitle: \"My first Quarto file\"\nauthor: \"Meike\"\ndate: \"9/25/2023\"\nformat: html\neditor: visual\n---\n\n\n\n\n\n\n\n\n\nOutput format\nYAML\n\n\n\n\nhtml\nformat: html\n\n\nWord\nformat: docx\n\n\npdf2\nformat: pdf\n\n\nhtml slides\nformat: revealjs\n\n\nPPT slides\nformat: pptx"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#section",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#section",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#you-will-get-frustrated-while-learning-r",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#you-will-get-frustrated-while-learning-r",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "You WILL get frustrated while learning R!",
    "text": "You WILL get frustrated while learning R!\nFrom Garrett Grolemund’s Prologue of his book Hands-On Programming with R3:\n\nAs you learn to program, you are going to get frustrated. You are learning a new language, and it will take time to become fluent. But frustration is not just natural, it’s actually a positive sign that you should watch for. Frustration is your brain’s way of being lazy; it’s trying to get you to quit and go do something easy or fun. If you want to get physically fitter, you need to push your body even though it complains. If you want to get better at programming, you’ll need to push your brain. Recognize when you get frustrated and see it as a good thing: you’re now stretching yourself. Push yourself a little further every day, and you’ll soon be a confident programmer."
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#resources",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#resources",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Resources",
    "text": "Resources\n\nOfficial Quarto guide: https://quarto.org/docs/guide/\n\nMarkdown basics: https://quarto.org/docs/authoring/markdown-basics.html\n\nText formatting, headings, linnks, images, lists, tables, equations, diagrams, page breaks, keyboard shortcuts, and more!\n\nCode blocks: https://quarto.org/docs/computations/r.html#code-blocks\n\nChunk options: https://quarto.org/docs/computations/r.html#chunk-options\n\n\nMine Çetinkaya-Rundel’s Quarto tip a day: https://mine-cetinkaya-rundel.github.io/quarto-tip-a-day/\n\nHadley Wickham’s R for Data Science: https://r4ds.hadley.nz/ _ See Chapter 29 for Quarto"
  },
  {
    "objectID": "lessons/01_Intro_to_R/01_Intro_to_R.html#footnotes",
    "href": "lessons/01_Intro_to_R/01_Intro_to_R.html#footnotes",
    "title": "Lesson 1: Intro to R & Rstudio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom Quarto’s Markdown Basics webpage, https://quarto.org/docs/authoring/markdown-basics.html↩︎\nrequires LaTeX installation↩︎\nGrolemund, Garrett. 2014. Hands-on Programming with R. O’Reilly. https://rstudio-education.github.io/hopr/↩︎"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "lessons/14_ANOVA/14_ANOVA.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#goals-for-today-section-5.5",
    "href": "lessons/14_ANOVA/14_ANOVA.html#goals-for-today-section-5.5",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Goals for today (Section 5.5)",
    "text": "Goals for today (Section 5.5)\n\nAnalysis of Variance (ANOVA)\nWhen to use an ANOVA\nHypotheses\nANOVA table\nDifferent sources of variation in ANOVA\nANOVA conditions\nF-distribution\nPost-hoc testing of differences in means\nRunning an ANOVA in R"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#disability-discrimination-example",
    "href": "lessons/14_ANOVA/14_ANOVA.html#disability-discrimination-example",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Disability Discrimination Example",
    "text": "Disability Discrimination Example\n\n\n\nThe U.S. Rehabilitation Act of 1973 prohibited discrimination against people with physical disabilities.\n\nThe act defined a disabled person as any individual who has a physical or mental impairment that limits the person’s major life activities.\n\nA 1980’s study examined whether physical disabilities affect people’s perceptions of employment qualifications\n\n(Cesare, Tannenbaum, & Dalessio, 1990).\n\n\n\n\nResearchers prepared recorded job interviews, using same actors and script each time.\nOnly difference: job applicant appeared with different disabilities.\n\nNo disability\nLeg amputation\nCrutches\nHearing impairment\nWheelchair confinement\n\n70 undergrad students were randomly assigned to view one of the videotapes,\n\nthen rated the candidate’s qualifications on a 1-10 scale.\n\n\n\n\n\nThe research question: are qualifications evaluated differently depending on the applicant’s presented disability?"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#load-interview-data-from-.txt-file",
    "href": "lessons/14_ANOVA/14_ANOVA.html#load-interview-data-from-.txt-file",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Load interview data from .txt file",
    "text": "Load interview data from .txt file\n\n.txt (text) files are usually tab-deliminated files\n\n.csv files are comma-separated files\n\nread_delim is from the readr package, just like read_csv, and loads with other tidyverse packages\n\n\nemploy &lt;- read_delim(\n  file = here::here(\"data\", \"DisabilityEmployment.txt\"), \n  delim = \"\\t\",   # tab delimited\n  trim_ws = TRUE)\n\nRows: 70 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): disability\ndbl (1): score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\ntrim_ws: specify whether leading and trailing white space should be trimmed from each field before parsing it\n\nglimpse(employ)\n\nRows: 70\nColumns: 2\n$ disability &lt;chr&gt; \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"no…\n$ score      &lt;dbl&gt; 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n\n\n\n\n\nsummary(employ)\n\n  disability            score      \n Length:70          Min.   :1.400  \n Class :character   1st Qu.:3.700  \n Mode  :character   Median :5.050  \n                    Mean   :4.929  \n                    3rd Qu.:6.100  \n                    Max.   :8.500  \n\n\n\n\nemploy %&gt;% tabyl(disability)\n\n disability  n percent\n    amputee 14     0.2\n   crutches 14     0.2\n    hearing 14     0.2\n       none 14     0.2\n wheelchair 14     0.2"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#moritzs-tip-of-the-day",
    "href": "lessons/14_ANOVA/14_ANOVA.html#moritzs-tip-of-the-day",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "MoRitz’s tip of the day",
    "text": "MoRitz’s tip of the day\nRead OHSU’s Inclusive Language Guide (below is from pgs. 22-25)\n“… an evolving tool to help OHSU members learn about and use inclusive language…”\nSections on: Race and ethnicity, Immigration status, Gender and sexual orientation, and Ability (including physical, mental and chronological attributes)"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#factor-variable-make-disability-a-factor-variable",
    "href": "lessons/14_ANOVA/14_ANOVA.html#factor-variable-make-disability-a-factor-variable",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Factor variable: Make disability a factor variable",
    "text": "Factor variable: Make disability a factor variable\n\n\n\nglimpse(employ)\n\nRows: 70\nColumns: 2\n$ disability &lt;chr&gt; \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"no…\n$ score      &lt;dbl&gt; 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n\n\n\n\nsummary(employ)\n\n  disability            score      \n Length:70          Min.   :1.400  \n Class :character   1st Qu.:3.700  \n Mode  :character   Median :5.050  \n                    Mean   :4.929  \n                    3rd Qu.:6.100  \n                    Max.   :8.500  \n\n\n\n\nMake disability a factor variable:\n\nemploy &lt;- employ %&gt;% \n  mutate(disability = factor(disability))\n\n\nWhat’s different now?\n\n\n\nglimpse(employ)\n\nRows: 70\nColumns: 2\n$ disability &lt;fct&gt; none, none, none, none, none, none, none, none, none, none,…\n$ score      &lt;dbl&gt; 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n\n\n\n\nsummary(employ)\n\n      disability     score      \n amputee   :14   Min.   :1.400  \n crutches  :14   1st Qu.:3.700  \n hearing   :14   Median :5.050  \n none      :14   Mean   :4.929  \n wheelchair:14   3rd Qu.:6.100  \n                 Max.   :8.500"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#factor-variable-change-order-name-of-disability-levels",
    "href": "lessons/14_ANOVA/14_ANOVA.html#factor-variable-change-order-name-of-disability-levels",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Factor variable: Change order & name of disability levels",
    "text": "Factor variable: Change order & name of disability levels\nWhat are the current level names and order?\n\nlevels(employ$disability)\n\n[1] \"amputee\"    \"crutches\"   \"hearing\"    \"none\"       \"wheelchair\"\n\n\nWhat changes are being made below?\n\nemploy &lt;- employ %&gt;% \n  mutate(\n    # make \"none\" the first level\n    # by only listing the level none, all other levels will be in original order\n    disability = fct_relevel(disability, \"none\"),\n    # change the level name amputee to amputation\n    disability = fct_recode(disability, amputation = \"amputee\")\n    )\n\n\nfct_relevel() and fct_recode() are from the forcats package: https://forcats.tidyverse.org/index.html.\nforcats is loaded with library(tidyverse).\n\nNew order & names:\n\nlevels(employ$disability) # note the new order and new name\n\n[1] \"none\"       \"amputation\" \"crutches\"   \"hearing\"    \"wheelchair\""
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#data-viz-12",
    "href": "lessons/14_ANOVA/14_ANOVA.html#data-viz-12",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Data viz (1/2)",
    "text": "Data viz (1/2)\n\nWhat are the score distribution shapes within each group?\nAny unusual values?\n\n\n\n\nggplot(employ, aes(x=score)) +\n  geom_density() +\n  facet_wrap(~ disability)\n\n\n\n\n\n\nlibrary(ggridges) \nggplot(employ, \n       aes(x=score,\n           y = disability,\n           fill = disability)) + \n  geom_density_ridges(alpha = 0.4) +\n  theme(legend.position=\"none\")\n\nPicking joint bandwidth of 0.801"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#data-viz-22",
    "href": "lessons/14_ANOVA/14_ANOVA.html#data-viz-22",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Data viz (2/2)",
    "text": "Data viz (2/2)\n\nCompare the score measures of center and spread between the groups\n\n\n\n\nggplot(employ, \n       aes(y=score, \n           x = disability,\n           fill = disability)) +\n  geom_boxplot(alpha = 0.3) +\n  coord_flip() +\n  geom_jitter(width = 0.1, \n              alpha = 0.3) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nggplot(employ, \n       aes(x = disability, \n           y=score, \n           fill=disability, \n           color=disability)) +\n  geom_dotplot(binaxis = \"y\", alpha = 0.5) +\n  geom_hline(aes(yintercept = mean(score)), \n             lty = \"dashed\") +\n  stat_summary(fun =\"mean\", geom=\"point\", \n    size = 3, color = \"grey33\", alpha = 1) +\n  theme(legend.position = \"none\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`."
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#hypotheses",
    "href": "lessons/14_ANOVA/14_ANOVA.html#hypotheses",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Hypotheses",
    "text": "Hypotheses\nTo test for a difference in means across k groups:\n\\[\\begin{align}\nH_0 &: \\mu_1 = \\mu_2 = ... = \\mu_k\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nHypothetical examples:\nIn which set (A or B) do you believe the evidence will be stronger that at least one population differs from the others?"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#comparing-means",
    "href": "lessons/14_ANOVA/14_ANOVA.html#comparing-means",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Comparing means",
    "text": "Comparing means\nWhether or not two means are significantly different depends on:\n\nHow far apart the means are\nHow much variability there is within each group\n\nQuestions:\n\nHow to measure variability between groups?\nHow to measure variability within groups?\nHow to compare the two measures of variability?\nHow to determine significance?"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#anova-in-base-r",
    "href": "lessons/14_ANOVA/14_ANOVA.html#anova-in-base-r",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA in base R",
    "text": "ANOVA in base R\n\nThere are several options to run an ANOVA model in R\nTwo most common are lm and aov\n\nlm = linear model; will be using frequently in BSTA 512\n\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naov(score ~ disability, data = employ) %&gt;% summary()\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ndisability   4  30.52   7.630   2.862 0.0301 *\nResiduals   65 173.32   2.666                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHypotheses:\n\\[\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nDo we reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#anova-tables",
    "href": "lessons/14_ANOVA/14_ANOVA.html#anova-tables",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA tables",
    "text": "ANOVA tables\nDisability example ANOVA table from R:\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nGeneric ANOVA table:"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#anova-analysis-of-variance",
    "href": "lessons/14_ANOVA/14_ANOVA.html#anova-analysis-of-variance",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nANOVA compares the variability between groups to the variability within groups"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#anova-analysis-of-variance-1",
    "href": "lessons/14_ANOVA/14_ANOVA.html#anova-analysis-of-variance-1",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nAnalysis of Variance (ANOVA) compares the variability between groups to the variability within groups\n\n\n\n\n\n\\[\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 \\ \\\n= \\ \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2\\]"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#notation",
    "href": "lessons/14_ANOVA/14_ANOVA.html#notation",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Notation",
    "text": "Notation\n\n\n\nk groups\n\\(n_i\\) observations in each of the k groups\nTotal sample size is \\(N=\\sum_{i=1}^{k}n_i\\)\n\\(\\bar{x}_{i}\\) = mean of observations in group i\n\\(\\bar{x}\\) = mean of all observations\n\\(s_{i}\\) = sd of observations in group i\n\\(s\\) = sd of all observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\ni = 1\ni = 2\ni = 3\n\\(\\ldots\\)\ni = k\noverall\n\n\n\n\nj = 1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(x_{31}\\)\n\\(\\ldots\\)\n\\(x_{k1}\\)\n\n\n\nj = 2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(x_{32}\\)\n\\(\\ldots\\)\n\\(x_{k2}\\)\n\n\n\nj = 3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(x_{33}\\)\n\\(\\ldots\\)\n\\(x_{k3}\\)\n\n\n\nj = 4\n\\(x_{14}\\)\n\\(x_{24}\\)\n\\(x_{34}\\)\n\\(\\ldots\\)\n\\(x_{k4}\\)\n\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\nj = \\(n_i\\)\n\\(x_{1n_1}\\)\n\\(x_{2n_2}\\)\n\\(x_{3n_3}\\)\n\\(\\ldots\\)\n\\(x_{kn_k}\\)\n\n\n\nMeans\n\\(\\bar{x}_{1}\\)\n\\(\\bar{x}_{2}\\)\n\\(\\bar{x}_{3}\\)\n\\(\\ldots\\)\n\\(\\bar{x}_{k}\\)\n\\(\\bar{x}\\)\n\n\nVariance\n\\({s}^2_{1}\\)\n\\({s}^2_{2}\\)\n\\({s}^2_{3}\\)\n\\(\\ldots\\)\n\\({s}^2_{k}\\)\n\\({s}^2\\)"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#total-sums-of-squares-visually",
    "href": "lessons/14_ANOVA/14_ANOVA.html#total-sums-of-squares-visually",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Total Sums of Squares Visually",
    "text": "Total Sums of Squares Visually\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nTotal Sums of Squares:\n\\[SST = \\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 = (N-1)s^2\\]\n\nwhere\n\n\\(N=\\sum_{i=1}^{k}n_i\\) is the total sample size and\n\\(s^2\\) is the grand standard deviation of all the observations\n\nThis is the sum of the squared differences between each observed \\(x_{ij}\\) value and the grand mean, \\(\\bar{x}\\).\nThat is, it is the total deviation of the \\(x_{ij}\\)’s from the grand mean."
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#calculate-total-sums-of-squares",
    "href": "lessons/14_ANOVA/14_ANOVA.html#calculate-total-sums-of-squares",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Total Sums of Squares",
    "text": "Calculate Total Sums of Squares\nTotal Sums of Squares:\n\\[SST = \\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 = (N-1)s^2\\]\n\n\n\nwhere\n\n\\(N=\\sum_{i=1}^{k}n_i\\) is the total sample size and\n\\(s^2\\) is the grand standard deviation of all the observations\n\n\n\nTotal sample size \\(N\\):\n\n(Ns &lt;- employ %&gt;% group_by(disability) %&gt;% count())\n\n# A tibble: 5 × 2\n# Groups:   disability [5]\n  disability     n\n  &lt;fct&gt;      &lt;int&gt;\n1 none          14\n2 amputation    14\n3 crutches      14\n4 hearing       14\n5 wheelchair    14\n\n\n\\(SST\\):\n\n(SST &lt;- (sum(Ns$n) - 1) * sd(employ$score)^2)\n\n[1] 203.8429"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#anova-analysis-of-variance-2",
    "href": "lessons/14_ANOVA/14_ANOVA.html#anova-analysis-of-variance-2",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nANOVA compares the variability between groups to the variability within groups"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#sums-of-squares-due-to-groups-visually-between-groups",
    "href": "lessons/14_ANOVA/14_ANOVA.html#sums-of-squares-due-to-groups-visually-between-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Sums of Squares due to Groups Visually (“between” groups)",
    "text": "Sums of Squares due to Groups Visually (“between” groups)\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nSums of Squares due to Groups:\n\\[SSG = \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2\\]\n\nThis is the sum of the squared differences between each group mean, \\(\\bar{x}_{i}\\), and the grand mean, \\(\\bar{x}\\).\nThat is, it is the deviation of the group means from the grand mean.\nAlso called the Model SS, or \\(SS_{model}.\\)"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#calculate-sums-of-squares-due-to-groups-between-groups",
    "href": "lessons/14_ANOVA/14_ANOVA.html#calculate-sums-of-squares-due-to-groups-between-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Sums of Squares due to Groups (“between” groups)",
    "text": "Calculate Sums of Squares due to Groups (“between” groups)\n\\[SSG = \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2\\]\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nCalculate means \\(\\bar{x}_i\\) for each group:\n\nxbar_groups &lt;- employ %&gt;% \n  group_by(disability) %&gt;% \n  summarise(mean = mean(score))\nxbar_groups\n\n# A tibble: 5 × 2\n  disability  mean\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        4.9 \n2 amputation  4.43\n3 crutches    5.92\n4 hearing     4.05\n5 wheelchair  5.34\n\n\nCalculate \\(SSG\\):\n\n(SSG &lt;- sum(Ns$n *\n  (xbar_groups$mean - mean(employ$score))^2))\n\n[1] 30.52143"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#anova-analysis-of-variance-3",
    "href": "lessons/14_ANOVA/14_ANOVA.html#anova-analysis-of-variance-3",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nANOVA compares the variability between groups to the variability within groups"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#sums-of-squares-error-visually-within-groups",
    "href": "lessons/14_ANOVA/14_ANOVA.html#sums-of-squares-error-visually-within-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Sums of Squares Error Visually (within groups)",
    "text": "Sums of Squares Error Visually (within groups)\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nSums of Squares Error:\n\\[SSE = \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2 = \\sum_{i = 1}^k(n_i-1)s_{i}^2\\] where \\(s_{i}\\) is the standard deviation of the \\(i^{th}\\) group\n\nThis is the sum of the squared differences between each observed \\(x_{ij}\\) value and its group mean \\(\\bar{x}_{i}\\).\nThat is, it is the deviation of the \\(x_{ij}\\)’s from the predicted score by group.\nAlso called the residual sums of squares, or \\(SS_{residual}.\\)"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#calculate-sums-of-squares-error-within-groups",
    "href": "lessons/14_ANOVA/14_ANOVA.html#calculate-sums-of-squares-error-within-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Sums of Squares Error (within groups)",
    "text": "Calculate Sums of Squares Error (within groups)\n\\[SSE = \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2 = \\sum_{i = 1}^k(n_i-1)s_{i}^2\\] where \\(s_{i}\\) is the standard deviation of the \\(i^{th}\\) group\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nCalculate sd’s \\(s_i\\) for each group:\n\nsd_groups &lt;- employ %&gt;% \n  group_by(disability) %&gt;% \n  summarise(SD = sd(score))\nsd_groups\n\n# A tibble: 5 × 2\n  disability    SD\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        1.79\n2 amputation  1.59\n3 crutches    1.48\n4 hearing     1.53\n5 wheelchair  1.75\n\n\nCalculate \\(SSE\\):\n\n(SSE &lt;- sum(\n  (Ns$n-1)*sd_groups$SD^2))\n\n[1] 173.3214"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#verify-sst-ssg-sse",
    "href": "lessons/14_ANOVA/14_ANOVA.html#verify-sst-ssg-sse",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Verify SST = SSG + SSE",
    "text": "Verify SST = SSG + SSE\nANOVA compares the variability between groups to the variability within groups\n\n\n\n\n\n\\[\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 \\ \\ = \\ \\ n_i\\sum_{i = 1}^k(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2\\]\n\\[(N-1)s^2 \\ \\\n= \\ \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k(n_i-1)s_{i}^2\\]\n\n\n\n\n\n\n\n\nSST\n\n[1] 203.8429\n\n\n\n\nSSG + SSE\n\n[1] 203.8429"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#anova-table",
    "href": "lessons/14_ANOVA/14_ANOVA.html#anova-table",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA table",
    "text": "ANOVA table"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#thinking-about-the-f-statistic",
    "href": "lessons/14_ANOVA/14_ANOVA.html#thinking-about-the-f-statistic",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Thinking about the F-statistic",
    "text": "Thinking about the F-statistic\n\n\nIf the groups are actually different, then which of these is more accurate?\n\nThe variability between groups should be higher than the variability within groups\nThe variability within groups should be higher than the variability between groups\n\n\nIf there really is a difference between the groups, we would expect the F-statistic to be which of these:\n\nHigher than we would observe by random chance\nLower than we would observe by random chance\n\n\n\n\n\n\\[F = \\frac{MSG}{MSE}\\]"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#anova-in-base-r-1",
    "href": "lessons/14_ANOVA/14_ANOVA.html#anova-in-base-r-1",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA in base R",
    "text": "ANOVA in base R\n\n# Note that I'm saving the tidy anova table\n# Will be pulling p-value from this on future slide\n\nempl_lm &lt;- lm(score ~ disability, data = employ) %&gt;% \n  anova() %&gt;% \n  tidy()\n\nempl_lm %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    disability\n4\n30.52143\n7.630357\n2.86158\n0.03012686\n    Residuals\n65\n173.32143\n2.666484\nNA\nNA\n  \n  \n  \n\n\n\n\nHypotheses:\n\\[\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nDo we reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#conclusion-to-hypothesis-test",
    "href": "lessons/14_ANOVA/14_ANOVA.html#conclusion-to-hypothesis-test",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Conclusion to hypothesis test",
    "text": "Conclusion to hypothesis test\n\\[\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\n\n\n\nempl_lm  # tidy anova output\n\n# A tibble: 2 × 6\n  term          df sumsq meansq statistic p.value\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 disability     4  30.5   7.63      2.86  0.0301\n2 Residuals     65 173.    2.67     NA    NA     \n\n# Note that this is a vector:\nempl_lm$p.value\n\n[1] 0.03012686         NA\n\n\nPull the p-value using base R:\n\nround(empl_lm$p.value[1],2)\n\n[1] 0.03\n\n\nPull the p-value using tidyverse:\n\nempl_lm %&gt;% \n  filter(term == \"disability\") %&gt;% \n  pull(p.value) %&gt;% \n  round(2)\n\n[1] 0.03\n\n\n\n\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nThere is sufficient evidence that at least one of the disability groups has a mean employment score statistically different from the other groups. ( \\(p\\)-value = 0.03)."
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#conditions-for-anova",
    "href": "lessons/14_ANOVA/14_ANOVA.html#conditions-for-anova",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Conditions for ANOVA",
    "text": "Conditions for ANOVA\nIF ALL of the following conditions hold:\n\nThe null hypothesis is true\nSample sizes in each group group are large (each \\(n \\ge 30\\))\n\nOR the data are relatively normally distributed in each group\n\n\n\n\n\nVariability is “similar” in all group groups:\n\nIs the within group variability about the same for each group?\nAs a rough rule of thumb, this condition is violated if the standard deviation of one group is more than double the standard deviation of another group\n\n\n\n\n\nChecking the equal variance condition:\n\nsd_groups # previously defined\n\n# A tibble: 5 × 2\n  disability    SD\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        1.79\n2 amputation  1.59\n3 crutches    1.48\n4 hearing     1.53\n5 wheelchair  1.75\n\nmax(sd_groups$SD) / min(sd_groups$SD)\n\n[1] 1.210425\n\n\n\n\nTHEN the sampling distribution of the F-statistic is an F-distribution"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#testing-variances-condition-3",
    "href": "lessons/14_ANOVA/14_ANOVA.html#testing-variances-condition-3",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Testing variances (Condition 3)",
    "text": "Testing variances (Condition 3)\nBartlett’s test for equal variances\n\n\\(H_0:\\) population variances of group levels are equal\n\\(H_A:\\) population variances of group levels are NOT equal\n\nNote: \\(H_A\\) is same as saying that at least one of the group levels has a different variance\n\n\n\n\n\n\nCaution\n\n\n\n\nBartlett’s test assumes the data in each group are normally distributed.\nDo not use if data do not satisfy the normality condition.\n\n\n\n\nbartlett.test(score ~ disability, data = employ)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  score by disability\nBartlett's K-squared = 0.7016, df = 4, p-value = 0.9511\n\n\n\n\n\n\n\n\nTip\n\n\n\nLevene’s test for equality of variances is not as restrictive: see https://www.statology.org/levenes-test-r/"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#the-f-distribution",
    "href": "lessons/14_ANOVA/14_ANOVA.html#the-f-distribution",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The F-distribution",
    "text": "The F-distribution\n\nThe F-distribution is skewed right.\nThe F-distribution has two different degrees of freedom:\n\none for the numerator of the ratio (k – 1) and\none for the denominator (N – k)\n\n\\(p\\)-value\n\nis always the upper tail\n(the area as extreme or more extreme)\n\n\n\n\n\n\n\n\n\n\n\nempl_lm %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    disability\n4\n30.52143\n7.630357\n2.86158\n0.03012686\n    Residuals\n65\n173.32143\n2.666484\nNA\nNA\n  \n  \n  \n\n\n\n# p-value using F-distribution\n\npf(2.86158, df1=5-1, df2=70-5, \n   lower.tail = FALSE)\n\n[1] 0.03012688"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#which-groups-are-statistically-different",
    "href": "lessons/14_ANOVA/14_ANOVA.html#which-groups-are-statistically-different",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Which groups are statistically different?",
    "text": "Which groups are statistically different?\n\n\n\n\nSo far we’ve only determined that at least one of the groups is different from the others,\n\nbut we don’t know which.\n\n\n\n\nWhat’s your guess?\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`."
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#post-hoc-testing-pairwise-t-tests",
    "href": "lessons/14_ANOVA/14_ANOVA.html#post-hoc-testing-pairwise-t-tests",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Post-hoc testing: pairwise t-tests",
    "text": "Post-hoc testing: pairwise t-tests\n\n\n\nIn post-hoc testing we run all the pairwise t-tests comparing the means from each pair of groups.\nWith 5 groups, this involves doing \\({5 \\choose 2} = \\frac{5!}{2!3!} = \\frac{5\\cdot 4}{2}= 10\\) different pairwise tests.\n\nProblem:\n\nAlthough the ANOVA test has an \\(\\alpha\\) chance of a Type I error (finding a difference between a pair that aren’t different),\nthe overall Type I error rate will be much higher when running many tests simultaneously.\n\n\n\n\n\n\\[\\begin{align}\nP(\\text{making an error}) = & \\alpha\\\\\nP(\\text{not making an error}) = & 1-\\alpha\\\\\nP(\\text{not making an error in m tests}) = & (1-\\alpha)^m\\\\\nP(\\text{making at least 1 error in m tests}) = & 1-(1-\\alpha)^m\n\\end{align}\\]"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#the-bonferroni-correction-12",
    "href": "lessons/14_ANOVA/14_ANOVA.html#the-bonferroni-correction-12",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The Bonferroni Correction (1/2)",
    "text": "The Bonferroni Correction (1/2)\n\n\nA very conservative (but very popular) approach is to divide the \\(\\alpha\\) level by how many tests \\(m\\) are being done:\n\\[\\alpha_{Bonf} = \\frac{\\alpha}{m}\\]\n\nThis is equivalent to multiplying the\np-values by m:\n\n\\[p\\textrm{-value} &lt; \\alpha_{Bonf} = \\frac{\\alpha}{m}\\] is the same as \\[m \\cdot (p\\textrm{-value}) &lt; \\alpha\\] The Bonferroni correction is popular since it’s very easy to implement.\n\n\nThe plot below shows the likelihood of making at least one Type I error depending on how may tests are done.\nNotice the likelihood decreases very quickly\n\nUnfortunately the likelihood of a Type II error is increasing as well\nIt becomes “harder” and harder to reject \\(H_0\\) if doing many tests."
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#the-bonferroni-correction-22",
    "href": "lessons/14_ANOVA/14_ANOVA.html#the-bonferroni-correction-22",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The Bonferroni Correction (2/2)",
    "text": "The Bonferroni Correction (2/2)\n\n\nPairwise t-tests without any p-value adjustments:\n\npairwise.t.test(employ$score, \n                employ$disability, \n                p.adj=\"none\") \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none   amputation crutches hearing\namputation 0.4477 -          -        -      \ncrutches   0.1028 0.0184     -        -      \nhearing    0.1732 0.5418     0.0035   -      \nwheelchair 0.4756 0.1433     0.3520   0.0401 \n\nP value adjustment method: none \n\n\n\nPairwise t-tests with Bonferroni p-value adjustments:\n\npairwise.t.test(employ$score,  \n                employ$disability, \n                p.adj=\"bonferroni\")  \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 1.000 -          -        -      \ncrutches   1.000 0.184      -        -      \nhearing    1.000 1.000      0.035    -      \nwheelchair 1.000 1.000      1.000    0.401  \n\nP value adjustment method: bonferroni \n\n\n\nSince there were 10 tests, all the p-values were multiplied by 10.\nAre there any significant pairwise differences?"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#tukeys-honest-significance-test-hsd",
    "href": "lessons/14_ANOVA/14_ANOVA.html#tukeys-honest-significance-test-hsd",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Tukey’s Honest Significance Test (HSD)",
    "text": "Tukey’s Honest Significance Test (HSD)\n\nTukey’s Honest Significance Test (HSD) controls the “family-wise probability” of making a Type I error using a much less conservative method than Bonferroni\n\nIt is specific to ANOVA\n\nIn addition to adjusted p-values, it also calculates Tukey adjusted CI’s for all pairwise differences\nThe function TukeyHSD() creates a set of confidence intervals of the differences between means with the specified family-wise probability of coverage.\n\n\n\n\n# need to run the model using `aov` instead of `lm`\nempl_aov &lt;- aov(score ~ disability, data = employ) \n\nTukeyHSD(x=empl_aov, conf.level = 0.95) \n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score ~ disability, data = employ)\n\n$disability\n                            diff        lwr        upr     p adj\namputation-none       -0.4714286 -2.2031613  1.2603042 0.9399911\ncrutches-none          1.0214286 -0.7103042  2.7531613 0.4686233\nhearing-none          -0.8500000 -2.5817328  0.8817328 0.6442517\nwheelchair-none        0.4428571 -1.2888756  2.1745899 0.9517374\ncrutches-amputation    1.4928571 -0.2388756  3.2245899 0.1232819\nhearing-amputation    -0.3785714 -2.1103042  1.3531613 0.9724743\nwheelchair-amputation  0.9142857 -0.8174470  2.6460185 0.5781165\nhearing-crutches      -1.8714286 -3.6031613 -0.1396958 0.0277842\nwheelchair-crutches   -0.5785714 -2.3103042  1.1531613 0.8812293\nwheelchair-hearing     1.2928571 -0.4388756  3.0245899 0.2348141\n\n\n\n\nplot(TukeyHSD(x=empl_aov, \n        conf.level = 0.95))"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#there-are-many-more-multiple-testing-adjustment-procedures",
    "href": "lessons/14_ANOVA/14_ANOVA.html#there-are-many-more-multiple-testing-adjustment-procedures",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "There are many more multiple testing adjustment procedures",
    "text": "There are many more multiple testing adjustment procedures\n\n\n\nBonferroni is popular because it’s so easy to apply\nTukey’s HSD is usually used for ANOVA\nCode below used Holm’s adjustment\n\n\n# default is Holm's adjustments\npairwise.t.test(employ$score, \n                employ$disability) \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 1.000 -          -        -      \ncrutches   0.719 0.165      -        -      \nhearing    0.866 1.000      0.035    -      \nwheelchair 1.000 0.860      1.000    0.321  \n\nP value adjustment method: holm \n\n\n\n\n\n\nFalse discovery rate (fdr) p-value adjustments are popular in omics, or whenever there are many tests being run:\n\n\npairwise.t.test(employ$score, \n                employ$disability, \n                p.adj=\"fdr\") \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 0.528 -          -        -      \ncrutches   0.257 0.092      -        -      \nhearing    0.289 0.542      0.035    -      \nwheelchair 0.528 0.287      0.503    0.134  \n\nP value adjustment method: fdr"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#multiple-testing-controlling-the-type-i-error-rate",
    "href": "lessons/14_ANOVA/14_ANOVA.html#multiple-testing-controlling-the-type-i-error-rate",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Multiple testing: controlling the Type I error rate",
    "text": "Multiple testing: controlling the Type I error rate\n\n\n\nThe multiple testing issue is not unique to ANOVA post-hoc testing.\nIt is also a concern when running separate tests for many related outcomes.\nBeware of p-hacking!\n\nProblem:\n\nAlthough one test has an \\(\\alpha\\) chance of a Type I error (finding a difference between a pair that aren’t different),\nthe overall Type I error rate will be much higher when running many tests simultaneously.\n\n\n\n\n\n\\[\\begin{align}\nP(\\text{making an error}) = & \\alpha\\\\\nP(\\text{not making an error}) = & 1-\\alpha\\\\\nP(\\text{not making an error in m tests}) = & (1-\\alpha)^m\\\\\nP(\\text{making at least 1 error in m tests}) = & 1-(1-\\alpha)^m\n\\end{align}\\]"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#anova-summary",
    "href": "lessons/14_ANOVA/14_ANOVA.html#anova-summary",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA Summary",
    "text": "ANOVA Summary\n\n\\[\\begin{align}\nH_0 &: \\mu_1 = \\mu_2 = ... = \\mu_k\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\n\nANOVA table in R:\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nANOVA table\n\n\n\n\n\nPost-hoc testing\n\nF-distribution & p-value"
  },
  {
    "objectID": "lessons/14_ANOVA/14_ANOVA.html#whats-next",
    "href": "lessons/14_ANOVA/14_ANOVA.html#whats-next",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "What’s next?",
    "text": "What’s next?"
  },
  {
    "objectID": "lessons/05_Bayes_thm/05_Bayes_thm.html",
    "href": "lessons/05_Bayes_thm/05_Bayes_thm.html",
    "title": "Day 5: Probability Part 2 - Bayes’ Rule",
    "section": "",
    "text": "Download slides\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#what-we-covered-in-day-10-part-1",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#what-we-covered-in-day-10-part-1",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "What we covered in Day 10 Part 1",
    "text": "What we covered in Day 10 Part 1\n\n(4.3, 5.1) Hypothesis testing for mean from one sample\n\nIntroduce hypothesis testing using the case of analyzing a mean from one sample (group)\n\n\n\n\nSteps of a hypothesis test:\n\nlevel of significance\nnull ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\ntest statistic\np-value\nconclusion\n\n\n\n\nRun a hypothesis test in R\n\nLoad a dataset - need to specify location of dataset\nR projects\nRun a t-test in R\ntidy() the test output using broom package\n\n\n\n\n\n\n(4.3.3) Confidence intervals (CIs) vs. hypothesis tests"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#goals-for-today-part-2---class-discussion",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#goals-for-today-part-2---class-discussion",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Goals for today: Part 2 - Class discussion",
    "text": "Goals for today: Part 2 - Class discussion\n\n(5.2) Inference for mean difference from dependent/paired 2 samples\n\nInference: CIs and hypothesis testing\nExploratory data analysis (EDA) to visualize data\nRun paired t-test in R\n\n\n\nOne-sided CIs\n\n\nClass discussion\n\nInference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.\nThus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.\nIn class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#cis-and-hypothesis-tests-for-different-scenarios",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#cis-and-hypothesis-tests-for-different-scenarios",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "CI’s and hypothesis tests for different scenarios:",
    "text": "CI’s and hypothesis tests for different scenarios:\n\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n???\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#steps-in-a-hypothesis-test",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#steps-in-a-hypothesis-test",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#examples-of-paired-designs-two-samples",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#examples-of-paired-designs-two-samples",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Examples of paired designs (two samples)",
    "text": "Examples of paired designs (two samples)\n\nEnroll pairs of identical twins to study a disease\nEnroll father & son pairs to study cholesterol levels\nStudying pairs of eyes\nEnroll people and collect data before & after an intervention (longitudinal data)\nTextbook example: Compare maximal speed of competitive swimmers wearing a wetsuit vs. wearing a regular swimsuit\n\nWIll use these data on homework\n\n\nCome up with 2 more examples of paired study designs."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#can-a-vegetarian-diet-change-cholesterol-levels",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#can-a-vegetarian-diet-change-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Can a vegetarian diet change cholesterol levels?",
    "text": "Can a vegetarian diet change cholesterol levels?\n\nScenario:\n\n24 non-vegetarian people were enrolled in a study\nThey were instructed to adopt a vegetarian diet\nCholesterol levels were measured before and after the diet\n\nQuestion: Is there evidence to support that cholesterol levels changed after the vegetarian diet?\nHow to answer the question?\n\nFirst, calculate changes (differences) in cholesterol levels\n\nWe usually do after - before if the data are longitudinal\n\n\n\n\n\nCalculate CI for the\nmean difference \\(\\delta\\):\n\\[\\bar{x}_d \\pm t^*\\cdot\\frac{s_d}{\\sqrt{n}}\\]\n\nRun a hypothesis test\nHypotheses\n\\[\\begin{align}\nH_0:& \\delta = \\delta_0 \\\\\nH_A:& \\delta \\neq \\delta_0 \\\\\n(or&~ &lt;, &gt;)\n\\end{align}\\]\n\n Test statistic\n\\[\nt_{\\bar{x}_d} = \\frac{\\bar{x}_d - \\delta_0}{\\frac{s_d}{\\sqrt{n}}}\n\\]"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#eda-explore-the-cholesterol-data",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#eda-explore-the-cholesterol-data",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Explore the cholesterol data",
    "text": "EDA: Explore the cholesterol data\n\nScenario:\n\n24 non-vegetarian people were enrolled in a study\nThey were instructed to adopt a vegetarian diet\nCholesterol levels were measured before and after the diet\n\n\n\nchol &lt;- read_csv(here::here(\"data\", \"chol213.csv\"))\n\nRows: 24 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): Before, After\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(chol)\n\nRows: 24\nColumns: 2\n$ Before &lt;dbl&gt; 195, 145, 205, 159, 244, 166, 250, 236, 192, 224, 238, 197, 169…\n$ After  &lt;dbl&gt; 146, 155, 178, 146, 208, 147, 202, 215, 184, 208, 206, 169, 182…\n\nchol %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    Before\n24\n137\n250\n179\n44.5\n187.792\n33.160\n6.769\n14.002\n    After\n24\n125\n215\n165\n38.0\n168.250\n26.796\n5.470\n11.315\n  \n  \n  \n\n\n\n\nMake sure you are able to load the data on your computer!"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#eda-cholesterol-levels-before-and-after-vegetarian-diet",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#eda-cholesterol-levels-before-and-after-vegetarian-diet",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Cholesterol levels before and after vegetarian diet",
    "text": "EDA: Cholesterol levels before and after vegetarian diet\nDescribe the distributions of the before & after data.\n\n\n\nggplot(chol, aes(x=Before)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=Before)) +\n  geom_boxplot()\n\n\n\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_boxplot()"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#eda-spaghetti-plot-of-cholesterol-levels-before-after-diet",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#eda-spaghetti-plot-of-cholesterol-levels-before-after-diet",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Spaghetti plot of cholesterol levels before & after diet",
    "text": "EDA: Spaghetti plot of cholesterol levels before & after diet\n\nVisualize the individual before vs. after diet changes in cholesterol levels\n\nWhat does this figure tell us?\n\n\n\n\n\n\nSee code file for how to wrangle the data and create the figure - you will not be expected to do this yourself."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#eda-differences-in-cholesterol-levels-after---before-diet",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#eda-differences-in-cholesterol-levels-after---before-diet",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Differences in cholesterol levels: After - Before diet",
    "text": "EDA: Differences in cholesterol levels: After - Before diet\nWhat is this code doing?\n\nchol &lt;- chol %&gt;% \n  mutate(DiffChol = After-Before) \nhead(chol, 8)\n\n# A tibble: 8 × 3\n  Before After DiffChol\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1    195   146      -49\n2    145   155       10\n3    205   178      -27\n4    159   146      -13\n5    244   208      -36\n6    166   147      -19\n7    250   202      -48\n8    236   215      -21\n\n\nIs the mean of DiffChol the same as the difference in means of After - Before? Should it be? Why or why not?\n\nchol %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    Before\n24\n137\n250\n179\n44.50\n187.792\n33.160\n6.769\n14.002\n    After\n24\n125\n215\n165\n38.00\n168.250\n26.796\n5.470\n11.315\n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.430\n7.096"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#eda-differences-in-cholesterol-levels-after---before-diet-1",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#eda-differences-in-cholesterol-levels-after---before-diet-1",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Differences in cholesterol levels: After - Before diet",
    "text": "EDA: Differences in cholesterol levels: After - Before diet\nCompare and contrast the 3 distributions. Comment on shape, center, and spread.\n\n\nBefore & After\n\nggplot(chol, aes(x=Before)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_density()\n\n\n\n\n\nDiffChol\n\nggplot(chol, aes(x=DiffChol)) + \n  geom_density()\n\n\n\nggplot(chol, aes(x=DiffChol)) + \n  geom_boxplot()"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#steps-in-a-hypothesis-test-1",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#steps-in-a-hypothesis-test-1",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#step-2-null-alternative-hypotheses",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#step-2-null-alternative-hypotheses",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nQuestion: Is there evidence to support that cholesterol levels changed after the vegetarian diet?\n\n\n\nNull and alternative hypotheses in words Include as much context as possible\n\n\n\\(H_0\\): The population mean difference in cholesterol levels after a vegetarian diet is fill in\n\\(H_A\\): The population mean difference in cholesterol levels after a vegetarian diet is fill in\n\n\nNull and alternative hypotheses in symbols\nfill in the missing parts of the hypotheses.\n\\[\\begin{align}\nH_0:& \\delta = \\\\\nH_A:& \\delta \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#step-3-test-statistic",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#step-3-test-statistic",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 3: Test statistic",
    "text": "Step 3: Test statistic\n\nchol %&gt;% select(DiffChol) %&gt;% get_summary_stats(type = \"common\") %&gt;% gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.43\n7.096\n  \n  \n  \n\n\n\n\n\n\n\\[\nt_{\\bar{x}_d} = \\frac{\\bar{x}_d - \\delta_0}{\\frac{s_d}{\\sqrt{n}}}\n\\]\n\nCalculate the test statistic.\nBased on the value of the test statistic, do you think we are going to reject or fail to reject \\(H_0\\)?\nWhat probability distribution does the test statistic have?\nAre the assumptions for a paired t-test satisfied so that we can use the probability distribution to calculate the \\(p\\)-value??"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#step-4-p-value",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#step-4-p-value",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value and shade in the area representing the p-value:"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#step-5-conclusion-to-hypothesis-test",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& \\delta = 0 \\\\\nH_A:& \\delta \\neq 0 \\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = \\(8.434775 \\cdot 10 ^{-6}\\)\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is sufficient evidence that the (population) mean difference in cholesterol levels after a vegetarian diet is different from 0 mg/dL ( \\(p\\)-value &lt; 0.001).\n\nMore realistic manuscript conclusion:\n\nAfter a vegetarian diet, cholesterol levels decreased by on average 19.54 mg/dL (SE = 3.43 mg/dL, 2-sided \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "95% CI for the mean difference in cholesterol levels",
    "text": "95% CI for the mean difference in cholesterol levels\n\nchol %&gt;% \n  select(DiffChol) %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.43\n7.096\n  \n  \n  \n\n\n\n\nCI for \\(\\mu_d\\) (or \\(\\delta\\)): How was \\(t^*\\) calculated?\n\n\n\\[\\begin{align}\n\\bar{x}_d &\\pm t^*\\cdot\\frac{s_d}{\\sqrt{n}}\\\\\n-19.542 &\\pm 2.069\\cdot\\frac{16.806}{\\sqrt{24}}\\\\\n-19.542 &\\pm 2.069\\cdot 3.43\\\\\n-19.542 &\\pm 7.096\\\\\n(-26.638&, -12.445)\n\\end{align}\\]\n\nConclusion:\nWe are 95% that the (population) mean difference in cholesterol levels after a vegetarian diet is between -26.638 mg/dL and -12.445 mg/dL.\n\nBased on the CI, is there evidence the diet made a difference in cholesterol levels? Why or why not?"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#r-option-1-run-a-1-sample-t.test-using-the-paired-differences",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#r-option-1-run-a-1-sample-t.test-using-the-paired-differences",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 1: Run a 1-sample t.test using the paired differences",
    "text": "R option 1: Run a 1-sample t.test using the paired differences\n\\(H_A: \\delta \\neq 0\\)\n\nt.test(x = chol$DiffChol, mu = 0)\n\n\n    One Sample t-test\n\ndata:  chol$DiffChol\nt = -5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -26.63811 -12.44522\nsample estimates:\nmean of x \n-19.54167 \n\n\nRun the code without mu = 0. Do the results change? Why or why not?"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#r-option-2-run-a-2-sample-t.test-with-paired-true-option",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#r-option-2-run-a-2-sample-t.test-with-paired-true-option",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 2: Run a 2-sample t.test with paired = TRUE option",
    "text": "R option 2: Run a 2-sample t.test with paired = TRUE option\n\\(H_A: \\delta \\neq 0\\)\n\nFor a 2-sample t-test we specify both x= and y=\nNote: mu = 0 is the default value and doesn’t need to be specified\n\n\nt.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  chol$Before and chol$After\nt = 5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 12.44522 26.63811\nsample estimates:\nmean difference \n       19.54167 \n\n\nWhat is different in the output compared to option 1?"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-12",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-12",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (1/2)",
    "text": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (1/2)\n\n\n\nThe data have to be in a long format for option 3, where each person has 2 rows: one for Before and one for After.\n\nThe long dataset chol_long was created for the slide “EDA: Spaghetti plot of cholesterol levels before & after diet”.\nSee the code to create it there.\n\nWhat information is being stored in each of the columns?\n\n\n\n# first 16 rows of long data:\nhead(chol_long, 16)\n\n# A tibble: 16 × 3\n   ID    Time   Cholesterol\n   &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt;\n 1 1     Before         195\n 2 1     After          146\n 3 2     Before         145\n 4 2     After          155\n 5 3     Before         205\n 6 3     After          178\n 7 4     Before         159\n 8 4     After          146\n 9 5     Before         244\n10 5     After          208\n11 6     Before         166\n12 6     After          147\n13 7     Before         250\n14 7     After          202\n15 8     Before         236\n16 8     After          215"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-22",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-22",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (2/2)",
    "text": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (2/2)\n\n\n\n\nUse the usual t.test\nWhat’s different is that\n\ninstead of specifying the variables with x= and y=,\nwe give a formula of the form y ~ x using just the variable names,\nand then specify the name of the dataset using data =\n\nThis method is often used in practice, and more similar to the coding style of running a regression model (BSTA 512 & 513)\n\n\n\n\n# using long data \n# with columns Cholesterol & Time\n# t.test(Cholesterol ~ Time, \n#        paired = TRUE, \n#        data = chol_long)\n\n\nWhat is different in the output compared to option 1?\nRerun the test using Time ~ Cholesterol (switch the variables). What do you get?"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#compare-the-3-options",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#compare-the-3-options",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Compare the 3 options",
    "text": "Compare the 3 options\n\nHow is the code similar and different for the 3 options?\nGiven a dataset, how would you choose which of the 3 options to use?\n\n\n# option 1\nt.test(x = chol$DiffChol, mu = 0) %&gt;% tidy() %&gt;% gt() # tidy from broom package\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    -19.54167\n-5.696519\n8.434775e-06\n23\n-26.63811\n-12.44522\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n# option 2\nt.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    19.54167\n5.696519\n8.434775e-06\n23\n12.44522\n26.63811\nPaired t-test\ntwo.sided\n  \n  \n  \n\n\n\n# option 3\n#t.test(Cholesterol ~ Time, paired = TRUE, data = chol_long) %&gt;% tidy() %&gt;% gt()"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "What if we wanted to test whether the diet decreased cholesterol levels?",
    "text": "What if we wanted to test whether the diet decreased cholesterol levels?\n\n\nWhat changes in each of the steps?\n\nSet the level of significance \\(\\alpha\\)\nSpecify the hypotheses \\(H_0\\) and \\(H_A\\)\n\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#r-what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#r-what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R: What if we wanted to test whether the diet decreased cholesterol levels?",
    "text": "R: What if we wanted to test whether the diet decreased cholesterol levels?\n\nWhich of the 3 options to run a paired t-test in R is being used below?\nHow did the code change to account for testing a decrease in cholesterol levels?\nWhich values in the output changed compared to testing for a change in cholesterol levels? How did they change?\n\n\n# alternative = c(\"two.sided\", \"less\", \"greater\")\nt.test(x = chol$DiffChol, mu = 0, alternative = \"less\") %&gt;% \n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    -19.54167\n-5.696519\n4.217387e-06\n23\n-Inf\n-13.6623\nOne Sample t-test\nless"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#one-sided-confidence-intervals",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#one-sided-confidence-intervals",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "One-sided confidence intervals",
    "text": "One-sided confidence intervals\n\n\nFormula for a 2-sided (1- \\(\\alpha\\) )% CI:\n\\[\\bar{x} \\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\]\n\n\\(t^*\\) = qt(1-alpha/2, df = n-1)\n\\(\\alpha\\) is split over both tails of the distribution\n\n\n\n\nA one-sided (1- \\(\\alpha\\) )% CI has all (1- \\(\\alpha\\) )% on just the left or the right tail of the distribution:\n\\[\\begin{align}\n(\\bar{x} - t^*\\cdot\\frac{s}{\\sqrt{n}},~\\infty) \\\\\n(\\infty,~\\bar{x} + t^*\\cdot\\frac{s}{\\sqrt{n}})\n\\end{align}\\]\n\n\\(t^*\\) = qt(1-alpha, df = n-1) for a\n1-sided lower (1- \\(\\alpha\\) )% CI\n\\(t^*\\) = qt(alpha, df = n-1) for a 1-sided upper (1- \\(\\alpha\\) )% CI\nA 1-sided CI gives estimates for a lower or upper bound of the population mean.\nSee Section 4.2.3 of the V&H book for more"
  },
  {
    "objectID": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#today-whats-next",
    "href": "lessons/10_Hyp_testing/10_Hyp_testing_01_pt_02.html#today-whats-next",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Today & what’s next?",
    "text": "Today & what’s next?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n???\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "lessons/15_SLR_01/15_SLR_01.html",
    "href": "lessons/15_SLR_01/15_SLR_01.html",
    "title": "Day 15: Simple Linear Regression (Sections 6.1-6.2)",
    "section": "",
    "text": "Download pdf of slides"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html",
    "title": "Lesson 3: Data visualization",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) (Sections 1.4, 1.5, 1.6, 1.7.1)\n\nData visualization with ggplot\n\nnumerical & categorical variables, and relationships between variables\n\nSummarizing numerical data\nFrequency (two-way) tables\n\nSome data wrangling techniques along the way\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#goals-for-today",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#goals-for-today",
    "title": "Lesson 3: Data visualization",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) (Sections 1.4, 1.5, 1.6, 1.7.1)\n\nData visualization with ggplot\n\nnumerical & categorical variables, and relationships between variables\n\nSummarizing numerical data\nFrequency (two-way) tables\n\nSome data wrangling techniques along the way\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#international-day-of-women-in-statistics-and-data-science",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#international-day-of-women-in-statistics-and-data-science",
    "title": "Lesson 3: Data visualization",
    "section": "International Day of Women in Statistics and Data Science",
    "text": "International Day of Women in Statistics and Data Science\nTuesday, October 10, 2022\n12 am - 11:59 pm UTC (5pm 10/9 to 4:59 pm 10/10 here)\n\n\n\nInternational Day of Women in Statistics and Data Science"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#mimis-tip-of-the-day-sending-messages-in-slack",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#mimis-tip-of-the-day-sending-messages-in-slack",
    "title": "Lesson 3: Data visualization",
    "section": "Mimi’s tip of the day: sending messages in Slack",
    "text": "Mimi’s tip of the day: sending messages in Slack\n\n\nAre you frustrated that Slack sends a message when you press Enter? You can change that!"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#recap-of-last-time",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#recap-of-last-time",
    "title": "Lesson 3: Data visualization",
    "section": "Recap of last time",
    "text": "Recap of last time\n\n(1.3) Data collection principles\n\nPopulation vs. sample\nSampling methods\nExperiments vs. Observational studies\n\n(1.2) Intro to Data\n\nData types\n\nNumerical: discrete (integer in R), continuous (double or numeric in R)\nCategorical: ordinal, nominal\n\ncharacter or factor in R\n\n\nHow are data stored in R? data frames, tibbles\nWorking with data in R: dim(), nrow(), ncol(), names(), str(), summary(), head(), tail(), $\n\n(1.4) Summarizing numerical data\n\nmean(), median(), sd(), quantile()"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#from-last-time-install-the-pacakges-listed-below",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#from-last-time-install-the-pacakges-listed-below",
    "title": "Lesson 3: Data visualization",
    "section": "From last time: Install the pacakges listed below",
    "text": "From last time: Install the pacakges listed below\n\n\nknitr\n\nthis might actually already be installed\ncheck your packages list\n\ntidyverse\n\nthis is actually a bundle of packages\nWarning: it will take a while to install!!!\nsee more info at https://tidyverse.tidyverse.org/\n\nrstatix\n\nfor summary statistics of a dataset\n\njanitor\n\nfor cleaning and exploring data\n\nggridges\n\nfor creating ridgeline plots\n\ndevtools\n\nused to create R packages\nfor our purposes, needed to install some packages\n\noi_biostat_data\n\nthis package is on github\nsee the next slide for directions on how to install oi_biostat_data"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#directions-for-installing-package-oibiostat",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#directions-for-installing-package-oibiostat",
    "title": "Lesson 3: Data visualization",
    "section": "Directions for installing package oibiostat",
    "text": "Directions for installing package oibiostat\n\n\nThe textbook’s datasets are in the R package oibiostat\nExplanation of code below\n\nInstallation of oibiostat package requires first installing devtools package\nThe code devtools::install_github() tells R to use the command install_github() from the devtools package without loading the entire package and all of its commands (which library(devtools) would do).\n\n\n\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"OI-Biostat/oi_biostat_data\", force = TRUE)\n\n\n\nAfter running the code above, put # in front of the commands so that RStudio doesn’t evaluate them when rendering.\nNow load the oibiostat package\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\n\n\n\nlibrary(oibiostat)"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#load-packages-with-library-command",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#load-packages-with-library-command",
    "title": "Lesson 3: Data visualization",
    "section": "Load packages with library() command",
    "text": "Load packages with library() command\n\n\nTip: at the top of your Qmd file, create a chunk that loads all of the R packages you want to use in that file.\nUse the library() command to load each required package.\n\nPackages need to be reloaded every time you open Rstudio.\nlibrary() commands to load needed packages must be in the Qmd file\n\n\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(rstatix)\n\n\nAttaching package: 'rstatix'\n\n\nThe following object is masked from 'package:janitor':\n\n    make_clean_names\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(knitr)\nlibrary(gtsummary) # NEW!!\n\n#StandWithUkraine\n\n\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#case-study-description",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#case-study-description",
    "title": "Lesson 3: Data visualization",
    "section": "Case Study Description",
    "text": "Case Study Description\n\n\nIn the US, individuals with developmental disabilities typically receive services and support from state governments.\n\nCalifornia allocates funds to developmentally disabled residents through the Department of Developmental Services (DDS)\nRecipients of DDS funds are referred to as “consumers.”\n\nDataset dds.discr\n\nsample of 1,000 DDS consumers (out of a total of ~ 250,000)\ndata include age, gender, race/ethnicity, and annual DDS financial support per consumer\n\nPrevious research\n\nResearchers examined expenditures on consumers by ethnicity\nFound that the mean annual expenditure on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?\nSee Section 1.7.1 in the textbook for more details"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#load-dds.discr-dataset-from-oibiostat-package",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#load-dds.discr-dataset-from-oibiostat-package",
    "title": "Lesson 3: Data visualization",
    "section": "Load dds.discr dataset from oibiostat package",
    "text": "Load dds.discr dataset from oibiostat package\n\n\nThe textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#getting-to-know-the-dataset",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#getting-to-know-the-dataset",
    "title": "Lesson 3: Data visualization",
    "section": "Getting to know the dataset",
    "text": "Getting to know the dataset\n\ndim(dds.discr)\n\n[1] 1000    6\n\nnames(dds.discr)\n\n[1] \"id\"           \"age.cohort\"   \"age\"          \"gender\"       \"expenditures\"\n[6] \"ethnicity\"   \n\nlength(unique(dds.discr$id)) # How many unique id's are there?\n\n[1] 1000"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#str-structure",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#str-structure",
    "title": "Lesson 3: Data visualization",
    "section": "str() structure",
    "text": "str() structure\n\n\nWe previously used the base R structure command str() to get information about variable types in a dataset.\nNote this dataset is a tibble instead of a data.frame\n\n\n\nstr(dds.discr)      # base R\n\ntibble [1,000 × 6] (S3: tbl_df/tbl/data.frame)\n $ id          : int [1:1000] 10210 10409 10486 10538 10568 10690 10711 10778 10820 10823 ...\n $ age.cohort  : Factor w/ 6 levels \"0-5\",\"6-12\",\"13-17\",..: 3 5 1 4 3 3 3 3 3 3 ...\n $ age         : int [1:1000] 17 37 3 19 13 15 13 17 14 13 ...\n $ gender      : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 1 2 1 1 2 1 2 ...\n $ expenditures: int [1:1000] 2113 41924 1454 6400 4412 4566 3915 3873 5021 2887 ...\n $ ethnicity   : Factor w/ 8 levels \"American Indian\",..: 8 8 4 4 8 4 8 3 8 4 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ID = col_integer(),\n  ..   `Age Cohort` = col_character(),\n  ..   Age = col_integer(),\n  ..   Gender = col_character(),\n  ..   Expenditures = col_integer(),\n  ..   Ethnicity = col_character()\n  .. )"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#glimpse",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#glimpse",
    "title": "Lesson 3: Data visualization",
    "section": "glimpse()",
    "text": "glimpse()\n\nNew: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#summary",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#summary",
    "title": "Lesson 3: Data visualization",
    "section": "summary()",
    "text": "summary()\n\n\nWe previously used the base R structure command summary() to get summary information about variables\n\n\n\nsummary(dds.discr)      # base R\n\n       id        age.cohort       age          gender     expenditures  \n Min.   :10210   0-5  : 82   Min.   : 0.0   Female:503   Min.   :  222  \n 1st Qu.:31809   6-12 :175   1st Qu.:12.0   Male  :497   1st Qu.: 2899  \n Median :55384   13-17:212   Median :18.0                Median : 7026  \n Mean   :54663   18-21:199   Mean   :22.8                Mean   :18066  \n 3rd Qu.:76135   22-50:226   3rd Qu.:26.0                3rd Qu.:37713  \n Max.   :99898   51+  :106   Max.   :95.0                Max.   :75098  \n                                                                        \n              ethnicity  \n White not Hispanic:401  \n Hispanic          :376  \n Asian             :129  \n Black             : 59  \n Multi Race        : 26  \n American Indian   :  4  \n (Other)           :  5"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#tbl_summary-summary-table",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#tbl_summary-summary-table",
    "title": "Lesson 3: Data visualization",
    "section": "tbl_summary(): summary table",
    "text": "tbl_summary(): summary table\n\n\n\n\nNew: Use tbl_summary() from the gtsummary package to get summary information\n\n\n\n# library(gtsummary)\ntbl_summary(dds.discr)"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#what-data-variables-are-included-in-the-plot-below",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#what-data-variables-are-included-in-the-plot-below",
    "title": "Lesson 3: Data visualization",
    "section": "What data (variables) are included in the plot below?",
    "text": "What data (variables) are included in the plot below?"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#basics-of-a-ggplot",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#basics-of-a-ggplot",
    "title": "Lesson 3: Data visualization",
    "section": "Basics of a ggplot",
    "text": "Basics of a ggplot"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#grammar-of-ggplot2",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#grammar-of-ggplot2",
    "title": "Lesson 3: Data visualization",
    "section": "Grammar of ggplot2",
    "text": "Grammar of ggplot2\n\n\n\nKieran Healy"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#histograms",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#histograms",
    "title": "Lesson 3: Data visualization",
    "section": "Histograms",
    "text": "Histograms\nWhat is being measured on the vertical axes?\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = expenditures)) +\n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#histograms-showing-proportions",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#histograms-showing-proportions",
    "title": "Lesson 3: Data visualization",
    "section": "Histograms showing proportions",
    "text": "Histograms showing proportions\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram(\n    aes(y = stat(density)))  \n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram(\n    aes(y = stat(density))) +  \n  scale_y_continuous(labels =   \n      scales::percent_format())  \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#distribution-shapes",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#distribution-shapes",
    "title": "Lesson 3: Data visualization",
    "section": "Distribution shapes",
    "text": "Distribution shapes"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#density-plots",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#density-plots",
    "title": "Lesson 3: Data visualization",
    "section": "Density plots",
    "text": "Density plots\nWhat is being measured on the vertical axes?\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_density() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#dot-plots",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#dot-plots",
    "title": "Lesson 3: Data visualization",
    "section": "Dot plots",
    "text": "Dot plots\n\n\nBetter for smaller samples\nWhat is being measured on the vertical axes?\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_dotplot(binwidth =1) \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram(binwidth =1)"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#boxplots",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#boxplots",
    "title": "Lesson 3: Data visualization",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) + \n  geom_boxplot() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(y = age)) + \n  geom_boxplot()"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#boxplots-5-number-summary-visualization",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#boxplots-5-number-summary-visualization",
    "title": "Lesson 3: Data visualization",
    "section": "Boxplots: 5 number summary visualization",
    "text": "Boxplots: 5 number summary visualization\n\nNo outliers: \nWith outliers:"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#side-by-side-boxplots",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#side-by-side-boxplots",
    "title": "Lesson 3: Data visualization",
    "section": "Side-by-side boxplots",
    "text": "Side-by-side boxplots\n\n\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           y = ethnicity)) + \n  geom_boxplot() + \n  labs(x = \"Annual Expenditures ($)\", \n       y = \"Race and ethnicity\")  \n\n\nCan you determine the following using boxplots?\n\ndistribution shape\nsample size"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#side-by-side-boxplots-with-data-points",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#side-by-side-boxplots-with-data-points",
    "title": "Lesson 3: Data visualization",
    "section": "Side-by-side boxplots with data points",
    "text": "Side-by-side boxplots with data points\n\n\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           y = ethnicity)) + \n  geom_boxplot(color=\"darkgrey\") + \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(     \n      height = 0.4))      \n\n\nCan you determine the following using boxplots?\n\ndistribution shape\nsample size"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#density-plots-by-group",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#density-plots-by-group",
    "title": "Lesson 3: Data visualization",
    "section": "Density plots by group",
    "text": "Density plots by group\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           color = ethnicity)) + \n  geom_density() + \n  labs(x = \"Annual Expenditures ($)\")"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#ridgeline-plot",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#ridgeline-plot",
    "title": "Lesson 3: Data visualization",
    "section": "Ridgeline plot",
    "text": "Ridgeline plot\n\n\n\n# library(ggridges)\nggplot(data = dds.discr,\n       aes(x = expenditures,\n           y = ethnicity,      \n           fill = ethnicity)      \n       ) + \n  geom_density_ridges(      \n    alpha = 0.3,      \n    show.legend = FALSE) +      \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\",\n       title =        \n\"Expenditures by race and \n       \\nethnicity\")       \n\n\n\n\nPicking joint bandwidth of 5520"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#transforming-data-1.4.5",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#transforming-data-1.4.5",
    "title": "Lesson 3: Data visualization",
    "section": "Transforming data (1.4.5)",
    "text": "Transforming data (1.4.5)\n\n\nWe sometimes apply a transformation to highly skewed data to make it more symmetric\nLog transformations are often used for skewed right data\n\n\n\n\nx = expenditures\n\nggplot(data = dds.discr, \n       aes(x = expenditures)) +  \n  geom_density() \n\n\n\n\n\nx = log(expenditures)\n\nggplot(data = dds.discr, \n       aes(x = log(expenditures))) +  \n  geom_density()"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#scatterplots",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#scatterplots",
    "title": "Lesson 3: Data visualization",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\nggplot(data = dds.discr, \n       aes(x = age,\n           y = expenditures)) + \n  geom_point() +       \n  labs(x = \"Age\",\n       y = \"Annual Expenditures ($)\") \n\n\nResponse vs. explanatory variables (Section 1.2.3)\n\nA response variable measures the outcome of interest in a study\nA study will typically examine whether the values of a response variable differ as values of an explanatory variable change\n\n\n\n\n\n\n\n\n\n\n\nDescribe the association between the variables"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#describing-associations-between-2-numerical-variables",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#describing-associations-between-2-numerical-variables",
    "title": "Lesson 3: Data visualization",
    "section": "Describing associations between 2 numerical variables",
    "text": "Describing associations between 2 numerical variables\n\n\n\nTwo variables \\(x\\) and \\(y\\) are\n\npositively associated if \\(y\\) increases as \\(x\\) increases.\nnegatively associated if \\(y\\) decreases as \\(x\\) increases.\nIf there is no association between the variables, then we say they are uncorrelated or independent.\n\n\n\n\n\n\n\n\n\n\n\n\nThe term “association” is a very general term.\n\nCan be used for numerical or categorical variables\nNot specifically referring to linear associations"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#pearson-correlation-coefficient-r",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#pearson-correlation-coefficient-r",
    "title": "Lesson 3: Data visualization",
    "section": "(Pearson) Correlation coefficient \\(r\\)",
    "text": "(Pearson) Correlation coefficient \\(r\\)\n\n\n\\(r = -1\\) indicates a perfect negative linear relationship: As one variable increases, the value of the other variable tends to go down, following a straight line.\n\\(r = 0\\) indicates no linear relationship: The values of both variables go up/down independently of each other.\n\\(r = 1\\) indicates a perfect positive linear relationship: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion.\nThe closer \\(r\\) is to ±1, the stronger the linear association."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#pearson-correlation-coefficient-r-formula",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#pearson-correlation-coefficient-r-formula",
    "title": "Lesson 3: Data visualization",
    "section": "(Pearson) Correlation coefficient (r): formula",
    "text": "(Pearson) Correlation coefficient (r): formula\n\nThe (Peasron) correlation coefficient of variables \\(x\\) and \\(y\\) can be computed using the formula \\[r = \\frac{1}{n-1}\\sum_{i=1}^{n}\\Big(\\frac{x_i - \\bar{x}}{s_x}\\Big)\\Big(\\frac{y_i - \\bar{y}}{s_y}\\Big)\\] where\n\n\\((x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\) are the \\(n\\) paired values of the variables \\(x\\) and \\(y\\)\n\\(s_x\\) and \\(s_y\\) are the sample standard deviations of the variables \\(x\\) and \\(y\\), respectively\n\n\n\ncor(dds.discr$age, dds.discr$expenditures)\n\n[1] 0.8432422"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#guess-the-correlation-game",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#guess-the-correlation-game",
    "title": "Lesson 3: Data visualization",
    "section": "Guess the correlation game!",
    "text": "Guess the correlation game!\n\n\n\nRossman & Chance’s applet\n\n\n\n\n\nTracks performance of guess vs. actual, error vs. actual, and error vs. trial\nhttp://www.rossmanchance.com/applets/GuessCorrelation.html\n\nOr, for the Atari-like experience\n\n\n\nhttp://guessthecorrelation.com/"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#scatterplots-with-color-coded-dots",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#scatterplots-with-color-coded-dots",
    "title": "Lesson 3: Data visualization",
    "section": "Scatterplots with color-coded dots",
    "text": "Scatterplots with color-coded dots\nDescribe the association between the variables\n\nggplot(data = dds.discr, \n       aes(x = age, y = expenditures,\n           color = ethnicity)) +   \n  geom_point(alpha = .5) +       \n  labs(x = \"Age\", y = \"Annual Expenditures ($)\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#barplots",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#barplots",
    "title": "Lesson 3: Data visualization",
    "section": "Barplots",
    "text": "Barplots\n\n\n\nCounts (below) vs.\npercentages (right)\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity)) +\n  geom_bar() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity)) +\n  geom_bar(aes(y = stat(prop),  \n               group = 1)) + \n  scale_y_continuous(labels =  \n      scales::percent_format())"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#barplots-with-2-variables-segmented-bar-plots",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#barplots-with-2-variables-segmented-bar-plots",
    "title": "Lesson 3: Data visualization",
    "section": "Barplots with 2 variables: segmented bar plots",
    "text": "Barplots with 2 variables: segmented bar plots\n\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#barplots-with-2-variables-side-by-side-bar-plots",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#barplots-with-2-variables-side-by-side-bar-plots",
    "title": "Lesson 3: Data visualization",
    "section": "Barplots with 2 variables: side-by-side bar plots",
    "text": "Barplots with 2 variables: side-by-side bar plots\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar(position = \"dodge\")"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#frequency-tables-count",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#frequency-tables-count",
    "title": "Lesson 3: Data visualization",
    "section": "Frequency tables: count()",
    "text": "Frequency tables: count()\n\n\n\n\ncount is from the dplyr package\nthe output is a long tibble, and not a “nice” table\n\n\n\ndds.discr %&gt;% count(ethnicity)\n\n# A tibble: 8 × 2\n  ethnicity              n\n  &lt;fct&gt;              &lt;int&gt;\n1 American Indian        4\n2 Asian                129\n3 Black                 59\n4 Hispanic             376\n5 Multi Race            26\n6 Native Hawaiian        3\n7 Other                  2\n8 White not Hispanic   401\n\n\n\n\ndds.discr %&gt;% \n  count(ethnicity, age.cohort)\n\n# A tibble: 35 × 3\n   ethnicity       age.cohort     n\n   &lt;fct&gt;           &lt;fct&gt;      &lt;int&gt;\n 1 American Indian 13-17          1\n 2 American Indian 22-50          1\n 3 American Indian 51+            2\n 4 Asian           0-5            8\n 5 Asian           6-12          18\n 6 Asian           13-17         20\n 7 Asian           18-21         41\n 8 Asian           22-50         29\n 9 Asian           51+           13\n10 Black           0-5            3\n# ℹ 25 more rows"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#how-to-use-the-pipe",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#how-to-use-the-pipe",
    "title": "Lesson 3: Data visualization",
    "section": "How to use the pipe %>%",
    "text": "How to use the pipe %&gt;%\n\nThe pipe operator %&gt;% strings together commands to be performed sequentially\n\n\ndds.discr %&gt;% head(n=3)      # pronounce %&gt;% as \"then\"\n\n# A tibble: 3 × 6\n     id age.cohort   age gender expenditures ethnicity         \n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt;             \n1 10210 13-17         17 Female         2113 White not Hispanic\n2 10409 22-50         37 Male          41924 White not Hispanic\n3 10486 0-5            3 Male           1454 Hispanic          \n\n\n\n\nAlways first list the tibble that the commands are being applied to\nCan use multiple pipes to run multiple commands in sequence\n\nWhat does the following code do?\n\n\n\n\ndds.discr %&gt;% head(n=3) %&gt;% summary()"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#frequency-tables-janitor-packages-tabyl-function",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#frequency-tables-janitor-packages-tabyl-function",
    "title": "Lesson 3: Data visualization",
    "section": "Frequency tables: janitor package’s tabyl function",
    "text": "Frequency tables: janitor package’s tabyl function\n\n\n\n# default table\ndds.discr %&gt;% \n  tabyl(ethnicity)  \n\n          ethnicity   n percent\n    American Indian   4   0.004\n              Asian 129   0.129\n              Black  59   0.059\n           Hispanic 376   0.376\n         Multi Race  26   0.026\n    Native Hawaiian   3   0.003\n              Other   2   0.002\n White not Hispanic 401   0.401\n\n\n\nadorn_ your table!\n\ndds.discr %&gt;% \n  tabyl(ethnicity) %&gt;%\n  adorn_totals(\"row\") %&gt;% \n  adorn_pct_formatting(digits=2)  \n\n          ethnicity    n percent\n    American Indian    4   0.40%\n              Asian  129  12.90%\n              Black   59   5.90%\n           Hispanic  376  37.60%\n         Multi Race   26   2.60%\n    Native Hawaiian    3   0.30%\n              Other    2   0.20%\n White not Hispanic  401  40.10%\n              Total 1000 100.00%"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#relative-frequency-table",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#relative-frequency-table",
    "title": "Lesson 3: Data visualization",
    "section": "Relative frequency table",
    "text": "Relative frequency table\n\n\n\n\nA relative frequency table shows proportions (or percentages) instead of counts\nTo the right I removed (deselected) the counts column (n) to create a relative frequency table\n\n\n\n\ndds.discr %&gt;% \n  tabyl(ethnicity) %&gt;%\n  adorn_totals(\"row\") %&gt;% \n  adorn_pct_formatting(digits=2) %&gt;%   \n  select(-n) \n\n          ethnicity percent\n    American Indian   0.40%\n              Asian  12.90%\n              Black   5.90%\n           Hispanic  37.60%\n         Multi Race   2.60%\n    Native Hawaiian   0.30%\n              Other   0.20%\n White not Hispanic  40.10%\n              Total 100.00%"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#contingency-tables-two-way-tables",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#contingency-tables-two-way-tables",
    "title": "Lesson 3: Data visualization",
    "section": "Contingency tables (two-way tables)",
    "text": "Contingency tables (two-way tables)\n\n\n\n\nContingency tables summarize data for two categorical variables\n\nwith each value in the table representing the number of times\na particular combination of outcomes occurs\n\nRow & column totals\nare sometimes called marginal totals\n\n\n\n\ndds.discr %&gt;% \n  tabyl(ethnicity, gender) %&gt;%    \n  adorn_totals(c(\"row\", \"col\"))    \n\n          ethnicity Female Male Total\n    American Indian      3    1     4\n              Asian     61   68   129\n              Black     26   33    59\n           Hispanic    192  184   376\n         Multi Race     13   13    26\n    Native Hawaiian      2    1     3\n              Other      1    1     2\n White not Hispanic    205  196   401\n              Total    503  497  1000"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#contingency-tables-with-percentages",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#contingency-tables-with-percentages",
    "title": "Lesson 3: Data visualization",
    "section": "Contingency tables with percentages",
    "text": "Contingency tables with percentages\n\ndds.discr %&gt;% \n  tabyl(ethnicity, age.cohort) %&gt;%\n  adorn_totals(c(\"row\")) %&gt;%\n  adorn_percentages(\"row\") %&gt;%   \n  adorn_pct_formatting(digits=0) %&gt;%    \n  adorn_ns()    \n\n          ethnicity      0-5      6-12      13-17     18-21     22-50       51+\n    American Indian  0%  (0)  0%   (0)  25%   (1)  0%   (0) 25%   (1) 50%   (2)\n              Asian  6%  (8) 14%  (18)  16%  (20) 32%  (41) 22%  (29) 10%  (13)\n              Black  5%  (3) 19%  (11)  20%  (12) 15%   (9) 29%  (17) 12%   (7)\n           Hispanic 12% (44) 24%  (91)  27% (103) 21%  (78) 11%  (43)  5%  (17)\n         Multi Race 27%  (7) 35%   (9)  27%   (7)  8%   (2)  4%   (1)  0%   (0)\n    Native Hawaiian  0%  (0)  0%   (0)   0%   (0)  0%   (0) 67%   (2) 33%   (1)\n              Other  0%  (0)  0%   (0) 100%   (2)  0%   (0)  0%   (0)  0%   (0)\n White not Hispanic  5% (20) 11%  (46)  17%  (67) 17%  (69) 33% (133) 16%  (66)\n              Total  8% (82) 18% (175)  21% (212) 20% (199) 23% (226) 11% (106)"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#mean-annual-dds-expenditures-by-raceethnicity",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#mean-annual-dds-expenditures-by-raceethnicity",
    "title": "Lesson 3: Data visualization",
    "section": "Mean annual DDS expenditures by race/ethnicity",
    "text": "Mean annual DDS expenditures by race/ethnicity\n\n\n\nmean(dds.discr$expenditures)\n\n[1] 18065.79\n\ndds.discr %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 1 × 3\n     ave     SD   med\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18066. 19543.  7026\n\n\n\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 8 × 4\n  ethnicity             ave     SD    med\n  &lt;fct&gt;               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 American Indian    36438. 25694. 41818.\n2 Asian              18392. 19209.  9369 \n3 Black              20885. 20549.  8687 \n4 Hispanic           11066. 15630.  3952 \n5 Multi Race          4457.  7332.  2622 \n6 Native Hawaiian    42782.  6576. 40727 \n7 Other               3316.  1836.  3316.\n8 White not Hispanic 24698. 20604. 15718"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#get_summary_stats-from-rstatix-package",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#get_summary_stats-from-rstatix-package",
    "title": "Lesson 3: Data visualization",
    "section": "get_summary_stats() from rstatix package",
    "text": "get_summary_stats() from rstatix package\n\ndds.discr %&gt;% get_summary_stats()\n\n# A tibble: 3 × 13\n  variable         n   min   max median     q1     q3   iqr    mad   mean     sd\n  &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 id            1000 10210 99898 55384. 31809. 76135. 44326 3.27e4 5.47e4 2.56e4\n2 age           1000     0    95    18     12     26     14 1.04e1 2.28e1 1.85e1\n3 expenditures  1000   222 75098  7026   2899. 37713. 34814 7.76e3 1.81e4 1.95e4\n# ℹ 2 more variables: se &lt;dbl&gt;, ci &lt;dbl&gt;\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;%\n  get_summary_stats(expenditures, type = \"common\")\n\n# A tibble: 8 × 11\n  ethnicity variable     n   min   max median    iqr   mean     sd     se     ci\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 American… expendi…     4  3726 58392 41818. 34085. 36438. 25694. 12847. 40885.\n2 Asian     expendi…   129   374 75098  9369  30892  18392. 19209.  1691.  3346.\n3 Black     expendi…    59   240 60808  8687  37987  20885. 20549.  2675.  5355.\n4 Hispanic  expendi…   376   222 65581  3952   7961. 11066. 15630.   806.  1585.\n5 Multi Ra… expendi…    26   669 38619  2622   2060.  4457.  7332.  1438.  2962.\n6 Native H… expendi…     3 37479 50141 40727   6331  42782.  6576.  3797. 16337.\n7 Other     expendi…     2  2018  4615  3316.  1298.  3316.  1836.  1298. 16499.\n8 White no… expendi…   401   340 68890 15718  39157  24698. 20604.  1029.  2023."
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#how-to-force-all-output-to-be-shown-12",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#how-to-force-all-output-to-be-shown-12",
    "title": "Lesson 3: Data visualization",
    "section": "How to force all output to be shown? (1/2)",
    "text": "How to force all output to be shown? (1/2)\nUse kable() from the knitr package.\n\ndds.discr %&gt;% get_summary_stats() %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nn\nmin\nmax\nmedian\nq1\nq3\niqr\nmad\nmean\nsd\nse\nci\n\n\n\n\nid\n1000\n10210\n99898\n55384.5\n31808.75\n76134.75\n44326\n32734.325\n54662.85\n25643.673\n810.924\n1591.310\n\n\nage\n1000\n0\n95\n18.0\n12.00\n26.00\n14\n10.378\n22.80\n18.462\n0.584\n1.146\n\n\nexpenditures\n1000\n222\n75098\n7026.0\n2898.75\n37712.75\n34814\n7760.670\n18065.79\n19542.831\n617.999\n1212.724"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#how-to-force-all-output-to-be-shown-knitr-22",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#how-to-force-all-output-to-be-shown-knitr-22",
    "title": "Lesson 3: Data visualization",
    "section": "How to force all output to be shown? knitr (2/2)",
    "text": "How to force all output to be shown? knitr (2/2)\nUse kable() from the knitr package.\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;%\n  get_summary_stats(expenditures, type = \"common\") %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nethnicity\nvariable\nn\nmin\nmax\nmedian\niqr\nmean\nsd\nse\nci\n\n\n\n\nAmerican Indian\nexpenditures\n4\n3726\n58392\n41817.5\n34085.25\n36438.250\n25693.912\n12846.956\n40884.748\n\n\nAsian\nexpenditures\n129\n374\n75098\n9369.0\n30892.00\n18392.372\n19209.225\n1691.278\n3346.482\n\n\nBlack\nexpenditures\n59\n240\n60808\n8687.0\n37987.00\n20884.593\n20549.274\n2675.288\n5355.170\n\n\nHispanic\nexpenditures\n376\n222\n65581\n3952.0\n7961.25\n11065.569\n15629.847\n806.048\n1584.940\n\n\nMulti Race\nexpenditures\n26\n669\n38619\n2622.0\n2059.75\n4456.731\n7332.135\n1437.950\n2961.514\n\n\nNative Hawaiian\nexpenditures\n3\n37479\n50141\n40727.0\n6331.00\n42782.333\n6576.462\n3796.922\n16336.838\n\n\nOther\nexpenditures\n2\n2018\n4615\n3316.5\n1298.50\n3316.500\n1836.356\n1298.500\n16499.007\n\n\nWhite not Hispanic\nexpenditures\n401\n340\n68890\n15718.0\n39157.00\n24697.549\n20604.376\n1028.933\n2022.793"
  },
  {
    "objectID": "lessons/03_Data_visualization/03_Data_visualization.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "href": "lessons/03_Data_visualization/03_Data_visualization.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "title": "Lesson 3: Data visualization",
    "section": "Case study: discrimination in developmental disability support (1.7.1)",
    "text": "Case study: discrimination in developmental disability support (1.7.1)\n\nPrevious research\n\nResearchers examined DDS expenditures for developmentally disabled residents by ethnicity\nFound that the mean annual expenditures on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?"
  },
  {
    "objectID": "lessons/04_Probability/04_Probability.html",
    "href": "lessons/04_Probability/04_Probability.html",
    "title": "Day 4: Probability Part 1",
    "section": "",
    "text": "Download slides\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "",
    "text": "Add tabbed sections to your html file using tabset.\n\nFirst tabSecond tabRead up on tabsets\n\n\n\nYou can make subsections appear as different tabs in your html file.\nThis is the first tab.\nIt was created by adding ::: panel-tabset right above the subsection ### First tab (see the code file).\nLook up to the right of where it says “First tab,” and you will see a second tab with the creative name “Second tab.”\nIf you are viewing the html output of this file, you can click on the different tabs to see what’s in them.\nTo stop new tabs from being created, close off the original ::: panel-tabset command with ::: at the end.\n\nIn the source code file, you will see the ::: at the end of the ### Read up on tabsets tab.\n\n\n\n\n\nWelcome to the second tab!\n\n\n\n\n\n\n\n\n\nYou can read up more about creating tabs at\n\nhttps://quarto.org/docs/interactive/layout.html#tabset-panel\n\n\n\nIf you are reading the source code file, the next line contains :::, which closes the tabsets."
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#moritzs-tip-of-the-day",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#moritzs-tip-of-the-day",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "",
    "text": "Add tabbed sections to your html file using tabset.\n\nFirst tabSecond tabRead up on tabsets\n\n\n\nYou can make subsections appear as different tabs in your html file.\nThis is the first tab.\nIt was created by adding ::: panel-tabset right above the subsection ### First tab (see the code file).\nLook up to the right of where it says “First tab,” and you will see a second tab with the creative name “Second tab.”\nIf you are viewing the html output of this file, you can click on the different tabs to see what’s in them.\nTo stop new tabs from being created, close off the original ::: panel-tabset command with ::: at the end.\n\nIn the source code file, you will see the ::: at the end of the ### Read up on tabsets tab.\n\n\n\n\n\nWelcome to the second tab!\n\n\n\n\n\n\n\n\n\nYou can read up more about creating tabs at\n\nhttps://quarto.org/docs/interactive/layout.html#tabset-panel\n\n\n\nIf you are reading the source code file, the next line contains :::, which closes the tabsets."
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#where-are-we",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#where-are-we",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#where-are-we-1",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#where-are-we-1",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Where are we?",
    "text": "Where are we?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n???\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#goals-for-today",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#goals-for-today",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Goals for today",
    "text": "Goals for today\n\n2-sample t-test (Section 5.3)\n\nStatistical inference for difference in means from 2 independent samples\n\nWhat are \\(H_0\\) and \\(H_a\\)?\nWhat is the SE for \\(\\bar{x}_1 - \\bar{x}_2\\)?\nHypothesis test\nConfidence Interval\nRun test in R - using long vs. wide data\nSatterthwaite’s df\nPooled SD\n\n\n\n\nPower and sample size (4.3.4, 5.4, plus notes)\n\nCritical values & rejection region\nType I & II errors\nPower\nHow to calculate sample size needed for a study?"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#examples-of-designs-with-two-independent-samples",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#examples-of-designs-with-two-independent-samples",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Examples of designs with two independent samples",
    "text": "Examples of designs with two independent samples\n\nAny study where participants are randomized to a control and treatment group\nStudy where create two groups based on whether they were exposed or not to some condition (can be observational)\nBook: “Does treatment using embryonic stem cells (ESCs) help improve heart function following a heart attack?”\nBook: “Is there evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who do not smoke?”\nThe key is that the data from the two groups are independent of each other."
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#steps-in-a-hypothesis-test",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#steps-in-a-hypothesis-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#does-caffeine-increase-finger-tapsmin-on-average",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#does-caffeine-increase-finger-tapsmin-on-average",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Does caffeine increase finger taps/min (on average)?",
    "text": "Does caffeine increase finger taps/min (on average)?\nStudy Design:\n\n\n20 male college students students were trained to tap their fingers at a rapid rate.\nEach then drank 2 cups of coffee (double-blind)\n\nControl group: decaf\nCaffeine group: ~ 200 mg caffeine\n\nAfter 2 hours, students were tested.\nTaps/minute recorded\n\n\n\nHand, David J.; Daly, Fergus; McConway, K.; Lunn, D. and Ostrowski, E. (1993). A handbook of small data sets. London, U.K.: Chapman and Hall.\n\n\n\nLoad the data from the csv file CaffeineTaps.csv\nThe code below is for when the data file is in a folder called data that is in your R project folder (your working directory)\n\n\n\nCaffTaps &lt;- read_csv(here::here(\"data\", \"CaffeineTaps.csv\"))\n\nRows: 20 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Group\ndbl (1): Taps\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(CaffTaps)\n\nRows: 20\nColumns: 2\n$ Taps  &lt;dbl&gt; 246, 248, 250, 252, 248, 250, 246, 248, 245, 250, 242, 245, 244,…\n$ Group &lt;chr&gt; \"Caffeine\", \"Caffeine\", \"Caffeine\", \"Caffeine\", \"Caffeine\", \"Caf…"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#eda-explore-the-finger-taps-data",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#eda-explore-the-finger-taps-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "EDA: Explore the finger taps data",
    "text": "EDA: Explore the finger taps data\n\n\nDotplot of taps/minute stratified by group\n\nggplot(CaffTaps, aes(x=Taps)) +\n  geom_dotplot() +\n  facet_wrap(vars(Group), ncol=1)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nSummary statistics stratified by group\n\n# get_summary_stats() from rstatix package\nsumstats &lt;- CaffTaps %&gt;% \n  group_by(Group) %&gt;% \n  get_summary_stats(type = \"mean_sd\") \nsumstats %&gt;% gt()\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\ndiff(sumstats$mean)\n\n[1] -3.5"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-2-null-alternative-hypotheses",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-2-null-alternative-hypotheses",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nQuestion: Is there evidence to support that drinking caffeine increases the number of finger taps/min?\n\n\n\nNull and alternative hypotheses in words\nInclude as much context as possible\n\n\n\\(H_0\\): The population difference in mean finger taps/min between the caffeine and control groups is …\n\\(H_A\\): The population difference in mean finger taps/min between the caffeine and control groups is …\n\n\nNull and alternative hypotheses in symbols\n\\[\\begin{align}\nH_0:& \\mu_{caff} - \\mu_{ctrl} = \\\\\nH_A:& \\mu_{caff} - \\mu_{ctrl} \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-3-test-statistic-part-1",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-3-test-statistic-part-1",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 1)",
    "text": "Step 3: Test statistic (part 1)\nRecall that in general the test statistic has the form:\n\\[\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\] Thus, for a two sample independent means test, we have:\n\\[\\text{test statistic} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{SE_{\\bar{x}_1 - \\bar{x}_2}}\\]\n\nWhat is the formula for \\(SE_{\\bar{x}_1 - \\bar{x}_2}\\)?\nWhat is the probability distribution of the test statistic?\nWhat assumptions need to be satisfied?"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#what-distribution-does-barx_1---barx_2-have",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#what-distribution-does-barx_1---barx_2-have",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What distribution does \\(\\bar{X}_1 - \\bar{X}_2\\) have?",
    "text": "What distribution does \\(\\bar{X}_1 - \\bar{X}_2\\) have?\n\n\nLet \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) be the means of random samples from two independent groups, with parameters shown in table:\n\n\n\n\n\nGroup 1\nGroup 2\n\n\n\n\nsample size\n\\(n_1\\)\n\\(n_2\\)\n\n\npop mean\n\\(\\mu_1\\)\n\\(\\mu_2\\)\n\n\npop sd\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n\n\nSome theoretical statistics:\n\nIf \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are independent normal r.v.’s, then \\(\\bar{X}_1 - \\bar{X}_2\\) is also normal\nWhat is the mean of \\(\\bar{X}_1 - \\bar{X}_2\\)?\n\n\\[E[\\bar{X}_1 - \\bar{X}_2] = E[\\bar{X}_1] - E[\\bar{X}_2] = \\mu_1-\\mu_2\\]\n\nWhat is the standard deviation of \\(\\bar{X}_1 - \\bar{X}_2\\)?\n\n\\[\\begin{align}\nVar(\\bar{X}_1 - \\bar{X}_2) &= Var(\\bar{X}_1) + Var(\\bar{X}_2) = \\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2} \\\\\nSD(\\bar{X}_1 - \\bar{X}_2) &= \\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}\n\\end{align}\\]"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-3-test-statistic-part-2",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-3-test-statistic-part-2",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 2)",
    "text": "Step 3: Test statistic (part 2)\n\n\n\n\\[\nt_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\n\n\\(\\bar{x}_1, \\bar{x}_2\\) are the sample means\n\\(\\mu_0=0\\) is the mean value specified in \\(H_0\\)\n\\(s_1, s_2\\) are the sample SD’s\n\\(n_1, n_2\\) are the sample sizes\n\n\n\n\nStatistical theory tells us that \\(t_{\\bar{x}_1 - \\bar{x}_2}\\) follows a student’s t-distribution with\n\n\\(df \\approx\\) smaller of \\(n_1-1\\) and \\(n_2-1\\)\nthis is a conservative estimate (smaller than actual \\(df\\) )\n\n\nAssumptions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, the observations from the two groups were not paired in any meaningful way.\n\nApproximately normal samples or big n’s\n\nThe distributions of the samples should be approximately normal\nor both their sample sizes should be at least 30."
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-3-test-statistic-part-3",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-3-test-statistic-part-3",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 3)",
    "text": "Step 3: Test statistic (part 3)\n\n\n\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\n\n\\[\n\\text{test statistic} = t_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\n\n\n\n\n\n\nBased on the value of the test statistic, do you think we are going to reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-3b-assumptions-satisfied",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-3b-assumptions-satisfied",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step “3b”: Assumptions satisfied?",
    "text": "Step “3b”: Assumptions satisfied?\n\n\nAssumptions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, the observations from the two groups were not paired in any meaningful way.\n\nApproximately normal samples or big n’s\n\nThe distributions of the samples should be approximately normal\nor both their sample sizes should be at least 30.\n\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`."
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-4-p-value",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-4-p-value",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-5-conclusion-to-hypothesis-test",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& \\mu_{caff} - \\mu_{ctrl} = 0\\\\\nH_A:& \\mu_{caff} - \\mu_{ctrl} &gt; 0\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.00397\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is sufficient evidence that the (population) difference in mean finger taps/min with vs. without caffeine is greater than 0 ( \\(p\\)-value = 0.004).\n\nMore realistic manuscript conclusion:\n\nThe mean finger taps/min were 244.8 (SD = 2.4) and 248.3 (SD = 2.2) for the control and caffeine groups, and the increase of 3.5 taps/min was statistically discrenible ( \\(p\\)-value = 0.004)."
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "95% CI for the mean difference in cholesterol levels",
    "text": "95% CI for the mean difference in cholesterol levels\n\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\n\n\n\nCI for \\(\\mu_{caff} - \\mu_{ctrl}\\):\n\\[\\bar{x}_{caff} - \\bar{x}_{ctrl} \\pm t^* \\cdot \\sqrt{\\frac{s_{caff}^2}{n_{caff}}+\\frac{s_{ctrl}^2}{n_{ctrl}}}\\]\n\n\n\n\n  \n\nInterpretation:\nWe are 95% confident that the (population) difference in mean finger taps/min between the caffeine and control groups is between 1.167 mg/dL and 5.833 mg/dL.\n\nBased on the CI, is there evidence that drinking caffeine made a difference in finger taps/min? Why or why not?"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#r-2-sample-t-test-with-long-data",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#r-2-sample-t-test-with-long-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test (with long data)",
    "text": "R: 2-sample t-test (with long data)\n\nThe CaffTaps data are in a long format, meaning that\n\nall of the outcome values are in one column and\nanother column indicates which group the values are from\n\nThis is a common format for data from multiple samples, especially if the sample sizes are different.\n\n\n\n(Taps_2ttest &lt;- t.test(formula = Taps ~ Group, \n                       alternative = \"greater\", \n                       data = CaffTaps))\n\n\n    Welch Two Sample t-test\n\ndata:  Taps by Group\nt = 3.3942, df = 17.89, p-value = 0.001628\nalternative hypothesis: true difference in means between group Caffeine and group NoCaffeine is greater than 0\n95 percent confidence interval:\n 1.711272      Inf\nsample estimates:\n  mean in group Caffeine mean in group NoCaffeine \n                   248.3                    244.8"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#tidy-the-t.test-output",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#tidy-the-t.test-output",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "tidy the t.test output",
    "text": "tidy the t.test output\n\n# use tidy command from broom package for briefer output that's a tibble\ntidy(Taps_2ttest) %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater\n  \n  \n  \n\n\n\n\n\nPull the p-value:\n\n\ntidy(Taps_2ttest)$p.value  # we can pull specific values from the tidy output\n\n[1] 0.001627703"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#r-2-sample-t-test-with-wide-data",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#r-2-sample-t-test-with-wide-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test (with wide data)",
    "text": "R: 2-sample t-test (with wide data)\n\n\n# make CaffTaps data wide: pivot_wider needs an ID column so that it \n# knows how to \"match\" values from the Caffeine and NoCaffeine groups\nCaffTaps_wide &lt;- CaffTaps %&gt;% \n  mutate(id = rep(1:10, 2)) %&gt;% #  \"fake\" IDs for pivot_wider step\n  pivot_wider(names_from = \"Group\",\n              values_from = \"Taps\")\n\nglimpse(CaffTaps_wide)\n\nRows: 10\nColumns: 3\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ Caffeine   &lt;dbl&gt; 246, 248, 250, 252, 248, 250, 246, 248, 245, 250\n$ NoCaffeine &lt;dbl&gt; 242, 245, 244, 248, 247, 248, 242, 244, 246, 242\n\nt.test(x = CaffTaps_wide$Caffeine, y = CaffTaps_wide$NoCaffeine, alternative = \"greater\") %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#why-are-the-dfs-in-the-r-output-different",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#why-are-the-dfs-in-the-r-output-different",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Why are the df’s in the R output different?",
    "text": "Why are the df’s in the R output different?\nFrom many slides ago:\n\nStatistical theory tells us that \\(t_{\\bar{x}_1 - \\bar{x}_2}\\) follows a student’s t-distribution with\n\n\\(df \\approx\\) smaller of \\(n_1-1\\) and \\(n_2-1\\)\nthis is a conservative estimate (smaller than actual \\(df\\) )\n\n\nThe actual degrees of freedom are calculated using Satterthwaite’s method:\n\\[\\nu = \\frac{[ (s_1^2/n_1) + (s_2^2/n_2) ]^2}\n{(s_1^2/n_1)^2/(n_1 - 1) + (s_2^2/n_2)^2/(n_2-1) }\n= \\frac{ [ SE_1^2 + SE_2^2 ]^2}{ SE_1^4/df_1 + SE_2^4/df_2 }\\]\n\nVerify the p-value in the R output using \\(\\nu\\) = 17.89012:\n\npt(3.3942, df = 17.89012, lower.tail = FALSE)\n\n[1] 0.001627588"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#pooled-standard-deviation-estimate",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#pooled-standard-deviation-estimate",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Pooled standard deviation estimate",
    "text": "Pooled standard deviation estimate\n\nSometimes we have reasons to believe that the population SD’s from the two groups are equal, such as when randomizing participants to two groups\n\n\n\n\nIn this case we can use a pooled SD:\n\n\\[s_{pooled}^2 = \\frac{s_1^2 (n_1-1) + s_2^2 (n_2-1)}{n_1 + n_2 - 2}\\]\n\n\n\\(n_1\\), \\(n_2\\) are the sample sizes, and\n\\(s_1\\), \\(s_2\\) are the sample standard deviations\nof the two groups\n\n\n\n\nWe use the pooled SD instead of \\(s_1^2\\) and \\(s_2^2\\) when calculating the standard error\n\n\\[SE = \\sqrt{\\frac{s_{pooled}^2}{n_1} + \\frac{s_{pooled}^2}{n_2}}= s_{pooled}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\]\n\n\nTest statistic with pooled SD:\n\\[t_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 -0}{s_{pooled}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\n\nCI with pooled SD:\n\\[(\\bar{x}_1 - \\bar{x}_2) \\pm t^{\\star} \\cdot s_{pooled} \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\]\n\n\n\nThe \\(t\\) distribution degrees of freedom are now:\n\n\\[df = (n_1 - 1) + (n_2 - 1) = n_1 + n_2 - 2.\\]"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#r-2-sample-t-test-with-pooled-sd",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#r-2-sample-t-test-with-pooled-sd",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test with pooled SD",
    "text": "R: 2-sample t-test with pooled SD\n\n# t-test with pooled SD\nt.test(formula = Taps ~ Group, \n       alternative = \"greater\", \n       var.equal = TRUE,  # pooled SD \n       data = CaffTaps) %&gt;% \n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001616497\n18\n1.711867\nInf\nTwo Sample t-test\ngreater\n  \n  \n  \n\n\n\n# t-test without pooled SD\nt.test(formula = Taps ~ Group, \n       alternative = \"greater\", \n       var.equal = FALSE,  # default, NOT pooled SD \n       data = CaffTaps) %&gt;% \n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater\n  \n  \n  \n\n\n\n\nSimilar output in this case - why??"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#whats-next",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#whats-next",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What’s next?",
    "text": "What’s next?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n???\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n???"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#critical-values",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#critical-values",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Critical values",
    "text": "Critical values\n\n\nCritical values are the cutoff values that determine whether a test statistic is statistically significant or not.\nIf a test statistic is greater in absolute value than the critical value, we reject \\(H_0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nCritical values are determined by\n\nthe significance level \\(\\alpha\\),\nwhether a test is 1- or 2-sided, &\nthe probability distribution being used to calculate the p-value (such as normal or t-distribution).\n\nThe critical values in the figure should look very familiar!\n\nWhere have we used these before?\n\n\n\n\n\n\nHow can we calculate the critical values using R?"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#rejection-region",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#rejection-region",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Rejection region",
    "text": "Rejection region\n\nIf the absolute value of the test statistic is greater than the critical value, we reject \\(H_0\\)\n\nIn this case the test statistic is in the rejection region.\nOtherwise it’s in the nonrejection region.\n\n\n\n\n\n\n\nStats & Geospatial Analysis\n\n\n\n\nWhat do rejection regions look like for 1-sided tests?"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#justice-system-analogy",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#justice-system-analogy",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Justice system analogy",
    "text": "Justice system analogy\n\n\n\n\nType I and Type II Errors - Making Mistakes in the Justice System"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#type-i-ii-errors",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#type-i-ii-errors",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Type I & II Errors",
    "text": "Type I & II Errors\n\n\n\n\n\n\n\n\n\n\\(\\alpha\\) = probability of making a Type I error\n\nThis is the significance level (usually 0.05)\nSet before study starts\n\n\\(\\beta\\) = probability of making a Type II error\nIdeally we want\n\nsmall Type I & II errors and\nbig power\n\n\n\n\n\n\n\n\n\n\nApplet for visualizing Type I & II errors and power: https://rpsychologist.com/d3/NHST/"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#relationship-between-type-i-ii-errors",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#relationship-between-type-i-ii-errors",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Relationship between Type I & II errors",
    "text": "Relationship between Type I & II errors\n\nType I vs. Type II error\n\nDecreasing P(Type I error) leads to\n\nincreasing P(Type II error)\n\nWe typically keep P(Type I error) = \\(\\alpha\\) set to 0.05\n\n\nFrom the applet at https://rpsychologist.com/d3/NHST/"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#relationship-between-type-ii-errors-and-power",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#relationship-between-type-ii-errors-and-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Relationship between Type II errors and power",
    "text": "Relationship between Type II errors and power\n\nPower = P(correctly rejecting the null hypothesis)\n\n\n\n\n\nPower is also called the\n\ntrue positive rate,\nprobability of detection, or\nthe sensitivity of a test\n\n\n\n\n\n\n\n\n\n\n\nPower vs. Type II error\n\nPower = 1 - P(Type II error) = 1 - \\(\\beta\\)\nThus as \\(\\beta\\) = P(Type II error) decreases, the power increases\nP(Type II error) decreases as the mean of the alternative population shits further away from the mean of the null population (effect size gets bigger).\nTypically want at least 80% power; 90% power is good"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#example-calculating-power",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#example-calculating-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Example calculating power",
    "text": "Example calculating power\n\n\nSuppose the mean of the null population is 0 ( \\(H_0: \\mu=0\\) ) with standard error 1\nFind the power of a 2-sided test if the actual \\(\\mu=3\\), assuming the SE doesn’t change.\n\n\n\n\n\n\n\n\n\n\nPower = \\(P(\\)Reject \\(H_0\\) when alternative pop is \\(N(3,1))\\)\nWhen \\(\\alpha\\) = 0.05, we reject \\(H_0\\) when the test statistic z is at least 1.96\nThus for \\(X\\sim N(3,1)\\) we need to calculate \\(P(X \\le -1.96) + P(X \\ge 1.96)\\):\n\n\n\n\n# left tail + right tail:\npnorm(-1.96, mean=3, sd=1, lower.tail=TRUE) + pnorm(1.96, mean=3, sd=1, lower.tail=FALSE)\n\n[1] 0.8508304\n\n\n\n\nThe left tail probability pnorm(-1.96, mean=3, sd=1, lower.tail=TRUE) is essentially 0 in this case.\n\n\n\nNote that this power calculation specified the value of the SE instead of the standard deviation and sample size \\(n\\) individually."
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#sample-size-calculation-for-testing-one-mean",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#sample-size-calculation-for-testing-one-mean",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Sample size calculation for testing one mean",
    "text": "Sample size calculation for testing one mean\n\n\nRecall in our body temperature example that \\(\\mu_0=98.6\\) °F and \\(\\bar{x}= 98.25\\) °F.\n\nThe p-value from the hypothesis test was highly significant (very small).\nWhat would the sample size \\(n\\) need to be for 80% power?\n\nCalculate \\(n\\),\n\ngiven \\(\\alpha\\), power ( \\(1-\\beta\\) ), “true” alternative mean \\(\\mu\\), and null \\(\\mu_0\\),\nassuming the test statistic is normal (instead of t-distribution):\n\n\n\n\n\\[n=\\left(s\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{\\mu-\\mu_0}\\right)^2\\]\n\n\nmu &lt;- 98.25\nmu0 &lt;- 98.6\nsd &lt;- 0.73\nalpha &lt;- 0.05\nbeta &lt;- 0.20\nn &lt;- (sd*(qnorm(1-alpha/2) + qnorm(1-beta)) / (mu-mu0))^2\nn\n\n[1] 34.14423\n\nceiling(n)  # always round UP to the next highest integer \n\n[1] 35\n\n\n\n\nWe would only need a sample size of 35 for 80% power!\nHowever, this is an under-estimate since we used the normal instead of t-distribution.\n\n\nSee http://powerandsamplesize.com/Calculators/Test-1-Mean/1-Sample-Equality."
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#power-calculation-for-testing-one-mean",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#power-calculation-for-testing-one-mean",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Power calculation for testing one mean",
    "text": "Power calculation for testing one mean\n\nConversely, we can calculate how much power we had in our body temperature one-sample test, given the sample size of 130.\n\nCalculate power,\n\ngiven \\(\\alpha\\), \\(n\\), “true” alternative mean \\(\\mu\\), and null \\(\\mu_0\\),\nassuming the test statistic is normal (instead of t-distribution)\n\n\n\\[1-\\beta=\n        \\Phi\\left(z-z_{1-\\alpha/2}\\right)+\\Phi\\left(-z-z_{1-\\alpha/2}\\right)\n        \\quad ,\\quad \\text{where } z=\\frac{\\mu-\\mu_0}{s/\\sqrt{n}}\\]\n\\(\\Phi\\) is the probability for a standard normal distribution\n\nmu &lt;- 98.25; mu0 &lt;- 98.6; sd &lt;- 0.73; alpha &lt;- 0.05; n &lt;- 130\n(z &lt;- (mu-mu0) / (sd/sqrt(n)) )\n\n[1] -5.466595\n\n(Power &lt;- pnorm(z-qnorm(1-alpha/2)) + pnorm(-z-qnorm(1-alpha/2)))\n\n[1] 0.9997731\n\n\nIf the population mean is 98.2 instead of 98.6, we have a 99.98% chance of correctly rejecting \\(H_0\\) when the sample size is 130."
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#r-package-pwr-for-power-analyses",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#r-package-pwr-for-power-analyses",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R package pwr for power analyses",
    "text": "R package pwr for power analyses\n\n\nUse pwr.t.test for both one- and two-sample t-tests.\n\nSpecify all parameters except for the one being solved for.\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"),\nalternative = c(\"two.sided\", \"less\", \"greater\"))\nd is Cohen’s d effect size: small = 0.2, medium = 0.5, large = 0.8\n\n\nOne-sample test (or paired t-test):\n\n\n\n\\[d = \\frac{\\mu-\\mu_0}{s}\\]\n\n\n\n\nTwo-sample test (independent):\n\n\n\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\]\n\n\n\n\n\n\\(\\bar{x}_1 - \\bar{x}_2\\) is the difference in means between the two groups that one would want to be able to detect as being significant,\n\\(s_{pooled}\\) is the pooled SD between the two groups - often assume have same sd in each group\nR package pwr for basic statistical tests\n\nhttps://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#pwr-sample-size-for-one-mean-test",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#pwr-sample-size-for-one-mean-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: sample size for one mean test",
    "text": "pwr: sample size for one mean test\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\mu-\\mu_0}{s}\\)\n\nSpecify all parameters except for the sample size:\n\n\n\nlibrary(pwr)\nt.n &lt;- pwr.t.test(\n  d = (98.6-98.25)/0.73, \n  sig.level = 0.05, \n  power = 0.80, \n  type = \"one.sample\")\n\nt.n\n\n\n     One-sample t test power calculation \n\n              n = 36.11196\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\nplot(t.n)"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#pwr-power-for-one-mean-test",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#pwr-power-for-one-mean-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: power for one mean test",
    "text": "pwr: power for one mean test\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\mu-\\mu_0}{s}\\)\n\nSpecify all parameters except for the power:\n\n\n\nt.power &lt;- pwr.t.test(\n  d = (98.6-98.25)/0.73, \n  sig.level = 0.05, \n  # power = 0.80, \n  n = 130,\n  type = \"one.sample\")\n\nt.power\n\n\n     One-sample t test power calculation \n\n              n = 130\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.9997354\n    alternative = two.sided\n\n\n\n\nplot(t.power)"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#pwr-two-sample-t-test-sample-size",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#pwr-two-sample-t-test-sample-size",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: Two-sample t-test: sample size",
    "text": "pwr: Two-sample t-test: sample size\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\)\n\nExample: Suppose the data collected for the caffeine taps study were pilot day for a larger study. Investigators want to know what sample size they would need to detect a 2 point difference between the two groups. Assume the SD in both groups is 2.3.\nSpecify all parameters except for the sample size:\n\n\n\n\n\nt2.n &lt;- pwr.t.test(\n  d = 2/2.3, \n  sig.level = 0.05, \n  power = 0.80, \n  type = \"two.sample\") \n\nt2.n\n\n\n     Two-sample t test power calculation \n\n              n = 21.76365\n              d = 0.8695652\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nplot(t2.n)"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#pwr-two-sample-t-test-power",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#pwr-two-sample-t-test-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: Two-sample t-test: power",
    "text": "pwr: Two-sample t-test: power\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\)\n\nExample: Suppose the data collected for the caffeine taps study were pilot day for a larger study. Investigators want to know what sample size they would need to detect a 2 point difference between the two groups. Assume the SD in both groups is 2.3.\nSpecify all parameters except for the power:\n\n\n\n\n\nt2.power &lt;- pwr.t.test(\n  d = 2/2.3, \n  sig.level = 0.05, \n  # power = 0.80, \n  n = 22,\n  type = \"two.sample\") \n\nt2.power\n\n\n     Two-sample t test power calculation \n\n              n = 22\n              d = 0.8695652\n      sig.level = 0.05\n          power = 0.8044288\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nplot(t2.power)"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#what-information-do-we-need-for-a-power-or-sample-size-calculation",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#what-information-do-we-need-for-a-power-or-sample-size-calculation",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What information do we need for a power (or sample size) calculation?",
    "text": "What information do we need for a power (or sample size) calculation?\n\n\nThere are 4 pieces of information:\n\nLevel of significance \\(\\alpha\\)\n\nUsually fixed to 0.05\n\nPower\n\nIdeally at least 0.80\n\nSample size\nEffect size (expected change)\n\nGiven any 3 pieces of information, we can solve for the 4th.\n\n\npwr.t.test(\n  d = (98.6-98.25)/0.73,\n  sig.level = 0.05, \n  # power = 0.80, \n  n=130,\n  type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 130\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.9997354\n    alternative = two.sided"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#more-software-for-power-and-sample-size-calculations-pass",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#more-software-for-power-and-sample-size-calculations-pass",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "More software for power and sample size calculations: PASS",
    "text": "More software for power and sample size calculations: PASS\n\nPASS is a very powerful (& expensive) software that does power and sample size calculations for many advanced statistical modeling techniques.\n\nEven if you don’t have access to PASS, their documentation is very good and free online.\nDocumentation includes formulas and references.\nPASS documentation for powering means\n\nOne mean, paired means, two independent means\n\n\nOne-sample t-test documentation: https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/One-Sample_T-Tests.pdf"
  },
  {
    "objectID": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#octri-berd-power-sample-size-presentations",
    "href": "lessons/11_Hyp_testing/11_Hyp_testing_02.html#octri-berd-power-sample-size-presentations",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "OCTRI-BERD power & sample size presentations",
    "text": "OCTRI-BERD power & sample size presentations\n\n\nPower and Sample Size 101\n\nPresented by Meike Niederhausen; April 13, 2023\nSlides: http://bit.ly/PSS101-BERD-April2023\nRecording\n\nPower and Sample Size for Clinical Trials: An Introduction\n\nPresented by Yiyi Chen; Feb 18, 2021\nSlides: http://bit.ly/PSS-ClinicalTrials\nRecording\n\nPlanning a Study with Power and Sample Size Considerations in Mind\n\nPresented by David Yanez; May 29, 2019\nSlides\nRecording\n\nPower and Sample Size Simulations in R\n\nPresented by Robin Baudier; Sept 21, 2023\nSlides\nRecording"
  },
  {
    "objectID": "lessons/07_Normal_Poisson/07_Normal_Poisson.html",
    "href": "lessons/07_Normal_Poisson/07_Normal_Poisson.html",
    "title": "Day 7: Normal and Poisson distributions",
    "section": "",
    "text": "Download slides\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "",
    "text": "With code folding we can hide or show the code in the html output by clicking on the Code buttons in the html file.\nNote the &lt;/&gt; Code button on the top right of the html output.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee more information at https://quarto.org/docs/output-formats/html-code.html#folding-code"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#moritzs-tip-of-the-day-code-folding",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#moritzs-tip-of-the-day-code-folding",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "",
    "text": "With code folding we can hide or show the code in the html output by clicking on the Code buttons in the html file.\nNote the &lt;/&gt; Code button on the top right of the html output.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee more information at https://quarto.org/docs/output-formats/html-code.html#folding-code"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#where-are-we",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#where-are-we",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Where are we?",
    "text": "Where are we?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n???\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n???"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#goals-for-today-sections-8.1-8.2",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#goals-for-today-sections-8.1-8.2",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Goals for today (Sections 8.1-8.2)",
    "text": "Goals for today (Sections 8.1-8.2)\n\nStatistical inference for a single proportion or the difference of two (independent) proportions\n\nSampling distribution for a proportion or difference in proportions\nWhat are \\(H_0\\) and \\(H_a\\)?\nWhat are the SE’s for \\(\\hat{p}\\) and \\(\\hat{p}_1-\\hat{p}_2\\)?\nHypothesis test\nConfidence Interval\nHow are the SE’s different for a hypothesis test & CI?\nHow to run proportions tests in R\nPower & sample size for proportions tests (extra material)"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#motivating-example",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#motivating-example",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Motivating example",
    "text": "Motivating example\n\nOne proportion\n\nA 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n\nWhat is the CI for the proportion?\nThe study also reported that 36% of noncollege young males had participated in sports betting. Is the proportion for male college students different from 0.36?\n\n\nTwo proportions\n\nThere were 214 men in the sample of noncollege young males (36% participated in sports betting in the previous year).\nCompare the difference in proportions between the college and noncollege young males.\n\nCI & Hypothesis test\n\n\n\n\nBarnes GM, Welte JW, Hoffman JH, Tidwell MC. Comparisons of gambling and alcohol use among college students and noncollege young people in the United States. J Am Coll Health. 2010 Mar-Apr;58(5):443-52. doi: 10.1080/07448480903540499. PMID: 20304756; PMCID: PMC4104810."
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#steps-in-a-hypothesis-test",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#steps-in-a-hypothesis-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#step-2-null-alternative-hypotheses",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#step-2-null-alternative-hypotheses",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nNull and alternative hypotheses in words and in symbols.\n\n\nOne sample test\n\n\\(H_0\\): The population proportion of young male college students that participated in sports betting in the previous year is 0.36.\n\\(H_A\\): The population proportion of young male college students that participated in sports betting in the previous year is not 0.36.\n\n\\[\\begin{align}\nH_0:& p = 0.36\\\\\nH_A:& p \\neq 0.36\\\\\n\\end{align}\\]\n\n\n\nTwo samples test\n\n\\(H_0\\): The difference in population proportions of young male college and noncollege students that participated in sports betting in the previous year is 0.\n\\(H_A\\): The difference in population proportions of young male college and noncollege students that participated in sports betting in the previous year is not 0.\n\n\\[\\begin{align}\nH_0:& p_{coll} - p_{noncoll} = 0\\\\\nH_A:& p_{coll} - p_{noncoll} \\neq 0\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#sampling-distribution-of-hatp",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#sampling-distribution-of-hatp",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sampling distribution of \\(\\hat{p}\\)",
    "text": "Sampling distribution of \\(\\hat{p}\\)\n\n\\(\\hat{p}=\\frac{X}{n}\\) where \\(X\\) is the number of “successes” and \\(n\\) is the sample size.\n\\(X \\sim Bin(n,p)\\), where \\(p\\) is the population proportion.\nFor \\(n\\) “big enough”, the normal distribution can be used to approximate a binomial distribution:\n\n\\[Bin(n,p) \\rightarrow N\\Big(\\mu = np, \\sigma = \\sqrt{np(1-p)} \\Big)\\]\n\nSince \\(\\hat{p}=\\frac{X}{n}\\) is a linear transformation of \\(X\\), we have for large n:\n\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\\]\n\nHow we apply this result to CI’s and test statistics is different!!!"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#step-3-test-statistic",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#step-3-test-statistic",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic",
    "text": "Step 3: Test statistic\n\nSampling distribution of \\(\\hat{p}\\) if we assume \\(H_0: p=p_0\\) is true:\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\n\\sim N\\Big(\n\\mu_{\\hat{p}}=p_0, \\sigma_{\\hat{p}}=\\sqrt{\\frac{p_0\\cdot(1-p_0)}{n}}\n\\Big)\\]\nTest statistic for a one sample proportion test:\n\\[\n\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\n= z_{\\hat{p}} = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0\\cdot(1-p_0)}{n}}}\n\\]\n\n\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n What is the test statistic when testing \\(H_0: p=0.36\\) vs.  \\(H_A: p \\neq 0.36\\)?\n\n\\[\\begin{align}\nz_{\\hat{p}} &= \\frac{94/269 - 0.36}{\\sqrt{\\frac{0.36\\cdot(1-0.36)}{269}}} \\\\\n& -0.3607455\n\\end{align}\\]"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#step-3b-conditions-satisfied",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#step-3b-conditions-satisfied",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step “3b”: Conditions satisfied?",
    "text": "Step “3b”: Conditions satisfied?\nConditions:\n\nIndependent observations?\n\nThe observations were collected independently.\n\nThe number of expected successes and expected failures is at least 10.\n\n\\(n_1 p_0 \\ge 10, \\ \\ n_1(1-p_0)\\ge 10\\)\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n Testing \\(H_0: p=0.36\\) vs. \\(H_A: p \\neq 0.36\\).\n Are the conditions satisfied?"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#step-4-p-value",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#step-4-p-value",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:\n\\[\\begin{align}\n2 &\\cdot P(\\hat{p}&lt;0.35) \\\\\n&= 2 \\cdot P\\Big(Z_{\\hat{p}} &lt; \\frac{94/269 - 0.36}{\\sqrt{\\frac{0.36\\cdot(1-0.36)}{269}}}\\Big)\\\\\n&=2 \\cdot P(Z_{\\hat{p}} &lt; -0.3607455)\\\\\n&= 0.7182897\n\\end{align}\\]\n\n2*pnorm(-0.3607455)\n\n[1] 0.7182897"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#step-5-conclusion-to-hypothesis-test",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& p = 0.36\\\\\nH_A:& p \\neq 0.36\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.7182897\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is insufficient evidence that the (population) proportion of young male college students that participated in sports betting in the previous year is different than 0.36 ( \\(p\\)-value = 0.72).\n\nMore realistic manuscript conclusion:\n\nIn a sample of 269 male college students, 35% had participated in sports betting in the previous year, which is not different from 36% ( \\(p\\)-value = 0.72)."
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#ci-for-population-proportion",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#ci-for-population-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "95% CI for population proportion",
    "text": "95% CI for population proportion\n\n\n\nWhat to use for SE in CI formula?\n\n\\[\\hat{p} \\pm z^* \\cdot SE_{\\hat{p}}\\]\n\n\n\n\nSampling distribution of \\(\\hat{p}\\):\n\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\\]\n\n\n\n\nProblem: We don’t know what \\(p\\) is - it’s what we’re estimating with the CI.\nSolution: approximate \\(p\\) with \\(\\hat{p}\\):\n\n\\[SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n\n\n\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\nFind the 95% CI for the population proportion.\n\n\\[\\begin{align}\n94/269 &\\pm 1.96 \\cdot SE_{\\hat{p}}\\\\\nSE_{\\hat{p}} &= \\sqrt{\\frac{(94/269)(1-94/269)}{269}}\\\\\n(0.293 &, 0.407)\n\\end{align}\\]\n\n\nInterpretation:\nWe are 95% confident that the (population) proportion of young male college students that participated in sports betting in the previous year is in (0.29, 0.41)."
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#conditions-for-one-proportion-test-vs.-ci",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#conditions-for-one-proportion-test-vs.-ci",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Conditions for one proportion: test vs. CI",
    "text": "Conditions for one proportion: test vs. CI\n\n\n\nHypothesis test conditions\n\nIndependent observations\n\nThe observations were collected independently.\n\n\n\n\nThe number of expected successes and expected failures is at least 10.\n\n\\[n_1 p_0 \\ge 10, \\ \\ n_1(1-p_0)\\ge 10\\]\n\n\n\nConfidence interval conditions\n\nIndependent observations\n\nThe observations were collected independently.\n\n\n\n\nThe number of successes and failures is at least 10:\n\n\\[n_1\\hat{p}_1 \\ge 10, \\ \\ n_1(1-\\hat{p}_1)\\ge 10\\]"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#sampling-distribution-of-hatp_1-hatp_2",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#sampling-distribution-of-hatp_1-hatp_2",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sampling distribution of \\(\\hat{p}_1-\\hat{p}_2\\)",
    "text": "Sampling distribution of \\(\\hat{p}_1-\\hat{p}_2\\)\n\n\n\\(\\hat{p}_1=\\frac{X_1}{n_1}\\) and \\(\\hat{p}_2=\\frac{X_2}{n_2}\\),\n\n\\(X_1\\) & \\(X_2\\) are the number of “successes”\n\\(n_1\\) & \\(n_2\\) are the sample sizes of the 1st & 2nd samples\n\n\n\n\nEach \\(\\hat{p}\\) can be approximated by a normal distribution, for “big enough” \\(n\\)\nSince the difference of independent normal random variables is also normal, it follows that for “big enough” \\(n_1\\) and \\(n_2\\)\n\n\\[\\hat{p}_1 - \\hat{p}_2 \\sim N \\Big(\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, ~~\n\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\n\\Big)\\]\nwhere \\(p_1\\) & \\(p_2\\) are the population proportions, respectively.\n\nHow we apply this result to CI’s and test statistics is different!!!"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#step-3-test-statistic-12",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#step-3-test-statistic-12",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic (1/2)",
    "text": "Step 3: Test statistic (1/2)\n\nSampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\): \\[\\hat{p}_1 - \\hat{p}_2 \\sim N \\Big(\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, ~~\n\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\n\\Big)\\]\nSince we assume \\(H_0: p_1 - p_2 = 0\\) is true, we “pool” the proportions of the two samples to calculate the SE:\n\\[\\text{pooled proportion} = \\hat{p}_{pool} = \\dfrac{\\text{total number of successes} }{ \\text{total number of cases}} = \\frac{x_1+x_2}{n_1+n_2}\\]\nTest statistic:\n\\[\n\\text{test statistic} = z_{\\hat{p}_1 - \\hat{p}_2} = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{\\sqrt{\\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_2}}}\n\\]"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#step-3-test-statistic-22",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#step-3-test-statistic-22",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic (2/2)",
    "text": "Step 3: Test statistic (2/2)\n\n\\[\n\\text{test statistic} = z_{\\hat{p}_1 - \\hat{p}_2} = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{\\sqrt{\\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_2}}}\n\\]\n\\[\\text{pooled proportion} = \\hat{p}_{pool} = \\dfrac{\\text{total number of successes} }{ \\text{total number of cases}} = \\frac{x_1+x_2}{n_1+n_2}\\]\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had.\nWhat is the test statistic when testing \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs.  \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\)?\n\\[\\begin{align}\nz_{\\hat{p}_1 - \\hat{p}_2} &= \\frac{94/269 - 77/214-0}{\\sqrt{0.354\\cdot(1-0.354)(\\frac{1}{269}+\\frac{1}{214})}}\\\\\n&=-0.2367497\n\\end{align}\\]"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#step-3b-conditions-satisfied-1",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#step-3b-conditions-satisfied-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step “3b”: Conditions satisfied?",
    "text": "Step “3b”: Conditions satisfied?\n\nConditions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\nThe number of expected successes and expected failures is at least 10 for each group - using the pooled proportion:\n\n\\(n_1\\hat{p}_{pool} \\ge 10, \\ \\ n_1(1-\\hat{p}_{pool}) \\ge 10\\)\n\\(n_2\\hat{p}_{pool} \\ge 10, \\ \\ n_2(1-\\hat{p}_{pool}) \\ge 10\\)\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had.\nTesting \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs.  \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\)? .\n Are the conditions satisfied?"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#step-4-p-value-1",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#step-4-p-value-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\n\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:\n\\[\\begin{align}\n2 &\\cdot P(\\hat{p}_1 - \\hat{p}_2&lt;0.35-0.36) \\\\\n= 2 &\\cdot P\\Big(Z_{\\hat{p}_1 - \\hat{p}_2} &lt; \\\\\n&\\frac{94/269 - 77/214-0}{\\sqrt{0.354\\cdot(1-0.354)(\\frac{1}{269}+\\frac{1}{214})}}\\Big)\\\\\n=2 &\\cdot P(Z_{\\hat{p}} &lt; -0.2367497) \\\\\n= & 0.812851\n\\end{align}\\]\n\n\n\n\n2*pnorm(-0.2367497)\n\n[1] 0.812851"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#step-5-conclusion-to-hypothesis-test-1",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#step-5-conclusion-to-hypothesis-test-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\n\\[\\begin{align}\nH_0:& p_{coll} - p_{noncoll} = 0\\\\\nH_A:& p_{coll} - p_{noncoll} \\neq 0\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.812851\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is insufficient evidence that the difference in (population) proportions of young male college and noncollege students that participated in sports betting in the previous year are different ( \\(p\\)-value = 0.81).\n\nMore realistic manuscript conclusion:\n\n35% of young male college students (n=269) and 36% of noncollege young males (n=214) participated in sports betting in the previous year ( \\(p\\)-value = 0.81)."
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#ci-for-population-difference-in-proportions",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#ci-for-population-difference-in-proportions",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "95% CI for population difference in proportions",
    "text": "95% CI for population difference in proportions\n\n\n\nWhat to use for SE in CI formula?\n\n\\[\\hat{p}_1 - \\hat{p}_2 \\pm z^* \\cdot SE_{\\hat{p}_1 - \\hat{p}_2}\\]\n\n\n\n\nSE in sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\)\n\n\\[\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}} \\]\n\n\n\n\nProblem: We don’t know what \\(p\\) is - it’s what we’re estimating with the CI.\nSolution: approximate \\(p_1\\), \\(p_2\\) with \\(\\hat{p}_1\\), \\(\\hat{p}_2\\):\n\n\\[SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\n\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]\n\n\n\n\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had. Find the 95% CI for the difference in population proportions.\n\n\n\n\\[\\frac{94}{269} - \\frac{77}{214} \\pm 1.96 \\cdot SE_{\\hat{p}_1 - \\hat{p}_2}\\]\n\\[\\begin{align}\n& SE_{\\hat{p}_1 - \\hat{p}_2}=\\\\\n& \\sqrt{\n\\frac{94/269 \\cdot (1-94/269)}{269} +\n\\frac{77/214 \\cdot (1-77/214)}{214}}\n\\end{align}\\]\n\n\n\n\nInterpretation:\nWe are 95% confident that the difference in (population) proportions of young male college and noncollege students that participated in sports betting in the previous year is in (-0.127, 0.106)."
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#conditions-for-difference-in-proportions-test-vs.-ci",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#conditions-for-difference-in-proportions-test-vs.-ci",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Conditions for difference in proportions: test vs. CI",
    "text": "Conditions for difference in proportions: test vs. CI\n\n\n\nHypothesis test conditions\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\n\n\n\nThe number of expected successes and expected failures is at least 10 for each group - using the pooled proportion:\n\n\\(n_1\\hat{p}_{pool} \\ge 10, \\ \\ n_1(1-\\hat{p}_{pool}) \\ge 10\\)\n\\(n_2\\hat{p}_{pool} \\ge 10, \\ \\ n_2(1-\\hat{p}_{pool}) \\ge 10\\)\n\n\n\n\n\nConfidence interval conditions\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\n\n\n\nThe number of successes and failures is at least 10 for each group.\n\n\\(n_1\\hat{p}_1 \\ge 10, \\ \\ n_1(1-\\hat{p}_1) \\ge 10\\)\n\\(n_2\\hat{p}_2 \\ge 10, \\ \\ n_2(1-\\hat{p}_2) \\ge 10\\)"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#summary-stats-input-for-1-sample-proportion-test",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#summary-stats-input-for-1-sample-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Summary stats input for 1-sample proportion test",
    "text": "Summary stats input for 1-sample proportion test\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\nTest \\(H_0: p=0.36\\) vs. \\(H_A: p \\neq 0.36\\)?\n\n.35*269 # number of \"successes\"; round this value\n\n[1] 94.15\n\nprop.test(x = 94, n = 269,              # x = # successes & n = sample size\n          p = 0.36,                     # null value p0\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)              # no continuity correction\n\n\n    1-sample proportions test without continuity correction\n\ndata:  94 out of 269, null probability 0.36\nX-squared = 0.13014, df = 1, p-value = 0.7183\nalternative hypothesis: true p is not equal to 0.36\n95 percent confidence interval:\n 0.2949476 0.4081767\nsample estimates:\n        p \n0.3494424 \n\n\nCan tidy() test output:\n\nprop.test(x = 94, n = 269, p = 0.36, alternative = \"two.sided\", correct = FALSE) %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.1301373\n0.7182897\n1\n0.2949476\n0.4081767\n1-sample proportions test without continuity correction\ntwo.sided"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#dataset-input-for-1-sample-proportion-test-12",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#dataset-input-for-1-sample-proportion-test-12",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 1-sample proportion test (1/2)",
    "text": "Dataset input for 1-sample proportion test (1/2)\n\n\nSince we don’t have a dataset, we first need to create a dataset based on the results:\n\n“out of 269 male college students, 35% had participated in sports betting in the previous year”\n\n\n.35*269 # number of \"successes\"; round this value\n\n[1] 94.15\n\nSportsBet1 &lt;- tibble(\n  Coll = c(rep(\"Bet\", 94),\n           rep(\"NotBet\",269-94))\n  )\n\n\n\nglimpse(SportsBet1)\n\nRows: 269\nColumns: 1\n$ Coll &lt;chr&gt; \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"B…\n\nSportsBet1 %&gt;% tabyl(Coll)\n\n   Coll   n   percent\n    Bet  94 0.3494424\n NotBet 175 0.6505576\n\n\n\nR code for proportions test requires input as a base R table:\n\ntable(SportsBet1$Coll)\n\n\n   Bet NotBet \n    94    175"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#dataset-input-for-1-sample-proportion-test-22",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#dataset-input-for-1-sample-proportion-test-22",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 1-sample proportion test (2/2)",
    "text": "Dataset input for 1-sample proportion test (2/2)\n\n\nWhen using a dataset, prop.test requires the input x to be a table\nNote that we do not also specify n since the table already includes all needed information.\n\n\nprop.test(x = table(SportsBet1$Coll),   # table() of data\n          p = 0.36,                     # null value p0\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)              # no continuity correction\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(SportsBet1$Coll), null probability 0.36\nX-squared = 0.13014, df = 1, p-value = 0.7183\nalternative hypothesis: true p is not equal to 0.36\n95 percent confidence interval:\n 0.2949476 0.4081767\nsample estimates:\n        p \n0.3494424 \n\n\nCompare output with summary stats method:\n\nprop.test(x = 94, n = 269,              # x = # successes & n = sample size\n          p = 0.36,                     # null value p0\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)  %&gt;%         # no continuity correction\n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.1301373\n0.7182897\n1\n0.2949476\n0.4081767\n1-sample proportions test without continuity correction\ntwo.sided"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#continuity-correction-1-prop-z-test-with-vs.-without-cc",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#continuity-correction-1-prop-z-test-with-vs.-without-cc",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Continuity correction: 1-prop z-test with vs. without CC",
    "text": "Continuity correction: 1-prop z-test with vs. without CC\n\n\nRecall that when we approximated the\nbinomial distribution with a normal distribution to calculate a probability,\nthat we included a continuity correction (CC)\nto account for approximating a discrete distribution with a continuous distribution.\n\n\nprop.test(x = 94, n = 269, p = 0.36, alternative = \"two.sided\", \n          correct = FALSE) %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.1301373\n0.7182897\n1\n0.2949476\n0.4081767\n1-sample proportions test without continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\n\n\nprop.test(x = 94, n = 269, p = 0.36, alternative = \"two.sided\", \n          correct = TRUE) %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.08834805\n0.7662879\n1\n0.2931841\n0.4100774\n1-sample proportions test with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nDifferences are small when sample sizes are large."
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#summary-stats-input-for-2-samples-proportion-test",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#summary-stats-input-for-2-samples-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Summary stats input for 2-samples proportion test",
    "text": "Summary stats input for 2-samples proportion test\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had. Test \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs. \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\).\n\n\n\n# round the number of successes:\n.35*269 # number of \"successes\" in college students\n\n[1] 94.15\n\n.36*214 # number of \"successes\" in noncollege students\n\n[1] 77.04\n\nNmbrBet &lt;- c(94, 77)                    # vector for # of successes in each group\nTotalNmbr &lt;- c(269, 214)                # vector for sample size in each group\n\nprop.test(x = NmbrBet,                  # x is # of successes in each group \n          n = TotalNmbr,                # n is sample size in each group\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)              # no continuity correction\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  NmbrBet out of TotalNmbr\nX-squared = 0.05605, df = 1, p-value = 0.8129\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.09628540  0.07554399\nsample estimates:\n   prop 1    prop 2 \n0.3494424 0.3598131"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#dataset-input-for-2-samples-proportion-test-12",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#dataset-input-for-2-samples-proportion-test-12",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 2-samples proportion test (1/2)",
    "text": "Dataset input for 2-samples proportion test (1/2)\n\n\n\nSince we don’t have a dataset, we first need to create a dataset based on the results:\n\n“out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had”\n\n\n\n# round the number of successes:\n.35*269 # college students\n\n[1] 94.15\n\n.36*214 # noncollege students\n\n[1] 77.04\n\nSportsBet2 &lt;- tibble(\n  Group = c(rep(\"College\", 269), \n         rep(\"NonCollege\", 214)),\n  Bet = c(rep(\"yes\", 94), \n          rep(\"no\", 269-94),\n          rep(\"yes\", 77), \n          rep(\"no\", 214-77))\n)\n\n\n\nglimpse(SportsBet2)\n\nRows: 483\nColumns: 2\n$ Group &lt;chr&gt; \"College\", \"College\", \"College\", \"College\", \"College\", \"College\"…\n$ Bet   &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"…\n\nSportsBet2 %&gt;% tabyl(Group, Bet)\n\n      Group  no yes\n    College 175  94\n NonCollege 137  77\n\n\n\nR code for proportions test requires input as a base R table:\n\ntable(SportsBet2$Group, \n      SportsBet2$Bet)\n\n            \n              no yes\n  College    175  94\n  NonCollege 137  77"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#dataset-input-for-2-samples-proportion-test-22",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#dataset-input-for-2-samples-proportion-test-22",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 2-samples proportion test (2/2)",
    "text": "Dataset input for 2-samples proportion test (2/2)\n\n\nWhen using a dataset, prop.test requires the input x to be a table\nNote that we do not also specify n since the table already includes all needed information.\n\n\nprop.test(x = table(SportsBet2$Group, SportsBet2$Bet),\n       alternative = \"two.sided\",\n       correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  table(SportsBet2$Group, SportsBet2$Bet)\nX-squared = 0.05605, df = 1, p-value = 0.8129\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.07554399  0.09628540\nsample estimates:\n   prop 1    prop 2 \n0.6505576 0.6401869 \n\n\nCompare output with summary stats method:\n\nprop.test(x = NmbrBet,                  # x is # of successes in each group \n          n = TotalNmbr,                # n is sample size in each group\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE) %&gt;%          # no continuity correction\n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.3598131\n0.05605044\n0.8128509\n1\n-0.0962854\n0.07554399\n2-sample test for equality of proportions without continuity correction\ntwo.sided"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#continuity-correction-2-prop-z-test-with-vs.-without-cc",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#continuity-correction-2-prop-z-test-with-vs.-without-cc",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Continuity correction: 2-prop z-test with vs. without CC",
    "text": "Continuity correction: 2-prop z-test with vs. without CC\n\n\nRecall that when we approximated the\nbinomial distribution with a normal distribution to calculate a probability,\nthat we included a continuity correction (CC)\nto account for approximating a discrete distribution with a continuous distribution.\n\n\nprop.test(x = NmbrBet, n = TotalNmbr, alternative = \"two.sided\", \n          correct = FALSE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.3598131\n0.05605044\n0.8128509\n1\n-0.0962854\n0.07554399\n2-sample test for equality of proportions without continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\n\n\nprop.test(x = NmbrBet, n = TotalNmbr, alternative = \"two.sided\", \n          correct = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.3598131\n0.01987511\n0.8878864\n1\n-0.1004806\n0.07973918\n2-sample test for equality of proportions with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nDifferences are small when sample sizes are large."
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#sample-size-calculation-for-testing-one-proportion",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#sample-size-calculation-for-testing-one-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sample size calculation for testing one proportion",
    "text": "Sample size calculation for testing one proportion\n\n\nRecall in our sports betting example that the null \\(p_0=0.36\\) and the observed proportion was \\(\\hat{p}=0.35\\).\n\nThe p-value from the hypothesis test was not significant.\nHow big would the sample size \\(n\\) need to be in order for the p-value to be significant?\n\nCalculate \\(n\\)\n\ngiven \\(\\alpha\\), power ( \\(1-\\beta\\) ), “true” alternative proportion \\(p\\), and null \\(p_0\\):\n\n\n\n\n\\[n=p(1-p)\\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{p-p_0}\\right)^2\\]\n\n\np &lt;- 0.35\np0 &lt;- 0.36\nalpha &lt;- 0.05\nbeta &lt;- 0.20  #power=1-beta; want &gt;=80% power\nn &lt;- p*(1-p)*((qnorm(1-alpha/2) + qnorm(1-beta)) /\n                (p-p0))^2\nn\n\n[1] 17856.2\n\nceiling(n) \n\n[1] 17857\n\n\n\n\nWe would need a sample size of at least 17,857!"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#power-calculation-for-testing-one-proportion",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#power-calculation-for-testing-one-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Power calculation for testing one proportion",
    "text": "Power calculation for testing one proportion\n\nConversely, we can calculate how much power we had in our example given the sample size of 269.\n\nCalculate power,\n\ngiven \\(\\alpha\\), \\(n\\), “true” alternative proportion \\(p\\), and null \\(p_0\\)\n\n\n\\[1-\\beta=\n            \\Phi\\left(z-z_{1-\\alpha/2}\\right)+\\Phi\\left(-z-z_{1-\\alpha/2}\\right)\n            \\quad ,\\quad \\text{where } z=\\frac{p-p_0}{\\sqrt{\\frac{p(1-p)}{n}}}\\]\n\\(\\Phi\\) is the probability for a standard normal distribution\n\np &lt;- 0.35; p0 &lt;- 0.36; alpha &lt;- 0.05; n &lt;- 269\n(z &lt;- (p-p0)/sqrt(p*(1-p)/n))\n\n[1] -0.343863\n\n(Power &lt;- pnorm(z - qnorm(1-alpha/2)) +  pnorm(-z - qnorm(1-alpha/2)))\n\n[1] 0.06365242\n\n\nIf the population proportion is 0.35 instead of 0.36, we only have a 6.4% chance of correctly rejecting \\(H_0\\) when the sample size is 269."
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#r-package-pwr-for-power-analyses",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#r-package-pwr-for-power-analyses",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "R package pwr for power analyses",
    "text": "R package pwr for power analyses\n\n\nSpecify all parameters except for the one being solved for.\nOne proportion\n\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\nTwo proportions (same sample sizes)\n\npwr.2p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\nTwo proportions (different sample sizes)\n\npwr.2p2n.test(h = NULL, n1 = NULL, n2 = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\", \"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size, and calculated using an arcsine transformation:\n\\[h = \\text{ES.h(p1, p2)} = 2\\arcsin(\\sqrt{p_1})-2\\arcsin(\\sqrt{p_2})\\]\n\n\nSee PASS documentation for\n\ntesting 1 proportion using effect size vs. other ways of powering a test of 1 proportion\ntesting 2 proportions using effect size vs. other ways of powering a test of 2 proportions."
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#pwr-sample-size-for-one-proportion-test",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#pwr-sample-size-for-one-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: sample size for one proportion test",
    "text": "pwr: sample size for one proportion test\n\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2)\n\np1 and p2 are the two proportions being tested\none of them is the null proportion \\(p_0\\), and the other is the alternative proportion\n\n\nSpecify all parameters except for the sample size:\n\n\n\n\n\nlibrary(pwr)\n\np.n &lt;- pwr.p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  power = 0.80, \n  alternative = \"two.sided\")\np.n\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 17971.09\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\n\nplot(p.n)"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#pwr-power-for-one-proportion-test",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#pwr-power-for-one-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: power for one proportion test",
    "text": "pwr: power for one proportion test\n\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2)\n\np1 and p2 are the two proportions being tested\none of them is the null proportion \\(p_0\\), and the other is the alternative proportion\n\n\nSpecify all parameters except for the power:\n\n\n\n\n\nlibrary(pwr)\n\np.power &lt;- pwr.p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  # power = 0.80, \n  n = 269,\n  alternative = \"two.sided\")\np.power\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 269\n      sig.level = 0.05\n          power = 0.06356445\n    alternative = two.sided\n\n\n\n\n\nplot(p.power)"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#pwr-sample-size-for-two-proportions-test",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#pwr-sample-size-for-two-proportions-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: sample size for two proportions test",
    "text": "pwr: sample size for two proportions test\n\n\nTwo proportions (same sample sizes)\n\npwr.2p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2); p1 and p2 are the two proportions being tested\n\nSpecify all parameters except for the sample size:\n\n\n\n\n\np2.n &lt;- pwr.2p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  power = 0.80, \n  alternative = \"two.sided\")\np2.n\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 35942.19\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes\n\n\n\n\n\nNote: \\(n\\) in output is the number per sample!\n\n\nplot(p2.n)"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#pwr-power-for-two-proportions-test",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#pwr-power-for-two-proportions-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: power for two proportions test",
    "text": "pwr: power for two proportions test\n\n\nTwo proportions (different sample sizes)\n\npwr.2p2n.test(h = NULL, n1 = NULL, n2 = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\", \"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2); p1 and p2 are the two proportions being tested\n\nSpecify all parameters except for the power:\n\n\n\n\n\np2.n2 &lt;- pwr.2p2n.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  n1 = 214,\n  n2 = 269,\n  sig.level = 0.05, \n  # power = 0.80, \n  alternative = \"two.sided\")\np2.n2\n\n\n     difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n             n1 = 214\n             n2 = 269\n      sig.level = 0.05\n          power = 0.05598413\n    alternative = two.sided\n\nNOTE: different sample sizes\n\n\n\n\n\nNote: \\(n\\) in output is the number per sample!\n\n\nplot(p2.n2)"
  },
  {
    "objectID": "lessons/12_Prop_inference/12_Prop_inference.html#where-are-we-1",
    "href": "lessons/12_Prop_inference/12_Prop_inference.html#where-are-we-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Where are we?",
    "text": "Where are we?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\\(\\sqrt{\\frac{p(1-p)}{n}}\\)\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n\\(\\sqrt{\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\\)"
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Untitled",
    "section": "",
    "text": "CalendarCredit to\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeptember\n\n\n\n\n\nMON\n\n\nTUE\n\n\nWED\n\n\nTHU\n\n\nFRI\n\n\n\n\n\n\n\n\n\n\n30\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOctober\n\n\n\n\n\nMON\n\n\nTUE\n\n\nWED\n\n\nTHU\n\n\nFRI\n\n\n\n\n\n\n\n\n\n\n30\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\n9\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\n\n\n\n\n\n\n\n\n15\n\n\n\n\n\n\n\n\n16\n\n\n\n\n\n\n\n\n17\n\n\n\n\n\n\n\n\n18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n21\n\n\n\n\n\n\n\n\n22\n\n\n\n\n\n\n\n\n23\n\n\n\n\n\n\n\n\n24\n\n\n\n\n\n\n\n\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVirtual Class\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n28\n\n\n\n\n\n\n\n\n29\n\n\n\n\n\n\n\n\n30\n\n\n\n\n\n\n\n\n31\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 4 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovember\n\n\n\n\n\nMON\n\n\nTUE\n\n\nWED\n\n\nTHU\n\n\nFRI\n\n\n\n\n\n\n\n\n\n\n28\n\n\n\n\n\n\n\n\n29\n\n\n\n\n\n\n\n\n30\n\n\n\n\n\n\n\n\n31\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 4 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n\n\n6\n\n\n\n\n\n\n\n\n7\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\n\n\nMidterm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\n\n\n\n\n\n\n\n\n12\n\n\n\n\n\n\n\n\n13\n\n\n\n\n\n\n\n\n14\n\n\n\n\n\n\n\n\n15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18\n\n\n\n\n\n\n\n\n19\n\n\n\n\n\n\n\n\n20\n\n\n\n\n\n\n\n\n21\n\n\n\n\n\n\n\n\n22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 6 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n25\n\n\n\n\n\n\n\n\n26\n\n\n\n\n\n\n\n\n27\n\n\n\n\n\n\n\n\n28\n\n\n\n\n\n\n\n\n29\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo Class\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecember\n\n\n\n\n\nMON\n\n\nTUE\n\n\nWED\n\n\nTHU\n\n\nFRI\n\n\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n\n\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 7 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n11\n\n\n\n\n\n\n\n\n12\n\n\n\n\n\n\n\n\n13\n\n\n\n\n\n\n\n\n\n\nFinal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecial thank you to Andrew Bray, who taught the Quarto workshop I attended. This schedule page was mostly taken from the Schedule on his STAT 20 course page. You can find the .qmd file for Andrew’s schedule page on his Github."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EPI 525: Biostatistics 1",
    "section": "",
    "text": "EPI 525: Biostatistics 1\n\nFall 2024\nWelcome to EPI 525! This course covers a broad range of basic statistical methods used in the health sciences. We will start with a review of descriptive statistics and introductory probability, then explore probability and sampling distributions, central limit theorem, and confidence intervals. These topics will be followed by basic hypothesis testing framework, along with the appropriate power and sample size considerations. We will also cover large-sample hypothesis tests for means, proportions and variances, and some exact tests. Students will be introduced to one-way analysis of variance (ANOVA), correlation and simple linear regression in preparation for BSTA 512/612. Most homework assignments are to be completed using R.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n OneDrive Folder\n\n\n Echo360 page\n\n\n\n\n\n\n\n\n\nInstructor\n Dr. Nicky Wakim\n Vanport 622A\n wakim@ohsu.edu\n\n\nOffice Hours\nOH with Nicky\n W 3 - 4pm\nOH with TA\n T 5:30 - 7pm\n\n\nCourse details\n Mondays, Wednesdays\n Sept 30 - Dec ?\n 1:00 PM - 2:50 PM\n In-person, VPT 515\n\n\nContacting me\nE-mail or Slack is the best way to get in contact with me. I will try to respond to all course-related e-mails within 24 hours Monday-Friday.\n\n\n\n\n\n\n\n\n View the source on GitHub"
  }
]